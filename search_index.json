[
["index.html", "経済動学: 線形編", " 経済動学: 線形編 佐藤 健治 2017-05-20 改訂履歴 date description 2017-05-20 Schur-QZ の章を追加 2017-03-31 3章にRコードを追加 2017-03-09 Blanchard-Kahn の章に図を追加 2017-03-07 記録開始 "],
["preface.html", "序文 構成 謝辞", " 序文 このノートは, 神戸大学大学院経済学研究科で2016年度, 2017年度の第1クォーターに実施した講義に基づいている. マクロ経済学, マクロ計量経済学, および数理経済学を専攻しようとする院生を読者として想定しているが, 数学的な議論が苦にならなければ学部上級の学生にとっても有益なものと思われる. 線形代数学, 動的システム理論, 力学系の初歩的な概念からはじめて, できるかぎり自己完結的なものとなるように心がけたので, マクロ経済学に特化した「経済数学」の参考書という使いかたもできると思う. また, 経済学に関する既知の事実をほとんど利用しないので, 経済モデルの分析に関心を持ち始めた理学・工学系の学生にとっても有用だろう. この講義では, 線形合理的期待モデルの分析を中心的な課題としている. 次の3つを中心的な課題としている. 線形合理的期待モデルの理論分析 その計算機シミュレーション マルコフスイッチング合理的期待モデルの紹介 マクロ数量分析の標準的なツールである Dynare を自分で使ったり, 名前を聞いたことのある学生も多いと思う. 一方で, その裏側でどのような計算が行われているかを知らないという学生も多いのではないかと思う. 理論に無関心でもおおよそ問題が起こらないのは, ひとえに Dynare が優れたインターフェイスを備えたアプリケーションであるということに外ならないのだが, そのブラックボックスを開けて理論を理解しようというのがこの講義の第1の課題である. 第2の課題は, 読者にコンピュータシミュレーションの基礎的な手法を身につけてもらうことである. 線形合理的期待モデルは近年のマクロ経済の数量分析に大きな役割を果たしているから, 各自の研究プロジェクトに大いに役に立つものと信じている. 数学もプログラミングも自分の手を動かし, 頭を悩ませなければ習熟できない. 理論とその実装を行き来しながら総合的な問題解決能力を養ってほしい. この講義では, 主に R言語を用いる.1 最後に, 近年盛んに研究されているマルコフスイッチング合理的期待モデルという非線形モデルを紹介する. 線形モデルのように, (ある種の) 安定性・不安定性に関する必要十分条件が得られるという特徴があり, 理論的に扱いやすい. 金融政策ルールに関するテイラーの条件を緩和できることが知られており, 理論・実証の両面から盛んに研究が進んでいる分野である. 構成 本書は15回 (週2回, 各回90分 × 8週) のクォーター制の講義に合わせて作られている. 原則的には各回1章を順に読み進めていくように構成している. 随所に理解を試す「練習問題」を設けたので, すべての問題に取り組んでほしい. 最初の数章は動的システムの分析に必要な, 学部初年次に習うであろう定義と結果をまとめているだけなので, 多くの読者にとって既知の事柄の羅列になっているはずだ. したがって, 練習問題を解いてみて躓くことがなければ読み飛ばすことができる. ただし, 各章は段階的に難しくなるように配置されているので, 少しでも不安があるようなら一読をお薦めする. また, この分野をすでに知っている読者でなければ関心のある章だけを読むという使い方はできないと思う. 第1章: はじめに どのようなモデルを解きたいか プログラミング環境の構築 第2章: 複素数 複素数の復習 なぜ複素数が必要なのか 第3章: 行列論の復習 行列の復習 行列積と複素数積の類似性 第4章: 行列の固有値 行列の固有値の復習 線形システム複素固有値の関係を明らかにする 第5章: 固有空間 第6章: ジョルダン標準形と線形システムの安定性 ジョルダン標準形 安定性 第7章: \\(\\det A \\neq 0\\) Blanchard and Kahn (1980) 第8章: ワイエルシュトラス標準形とデスクリプタシステム \\(\\det A = 0\\) 第9章: 一般の \\(A\\) Stock and Watson? 第10章: 数値解法 Schur 分解 QZ 分解 第11章: Klein の方法 Klein (2000) 第12章: Sims の方法 第13章: 確率システム 第14章: Lubik-Schorfheide 第15章: マルコフスイッチングシステム 謝辞 コロンビア大学の安東宇さんから有益なコメントをいくつかいただいた. 参考文献 "],
["intro.html", "第1章 はじめに 1.1 経済動学 1.2 対象とするモデル 1.3 プログラミング環境", " 第1章 はじめに 1.1 経済動学 「経済動学」（economic dynamics）では経済の状態を表す変数（経済指標）の時間を通じた変化， あるいは変化の不在，を研究する。どのような問題を扱おうとしているのか整理しておこう。 学部レベルのミクロ経済学を履修した読者は次のような最適化問題には馴染みがあると思う。 \\[ \\begin{aligned} &amp;\\max_{c\\in \\mathbb{R}^N_+} u(c) \\\\ &amp;\\text{subject to}\\quad p \\cdot c \\le I \\end{aligned} \\] 価格 \\(p = (p_1, \\dots, p_N)\\) と所得 \\(I\\) のもとで，効用 \\(u(c)\\) を最大にする消費量の組み合わせ \\(c = (c_1, \\dots, c_N)\\) を選べという問題である。この消費者モデルに対する標準的な仮定は，経済には\\(N\\) 種の異なる財があり，そのうちの第 \\(n\\) 財, \\(n = 1, \\dots, N\\), には価格 \\(p_n\\) が設定されている。 消費者はそれを \\(c_n\\) 単位購入する。すべての財を購入するには，\\(p\\cdot c = \\sum_{n = 1}^N p_n x_n\\) を支払う必要があるが，所得 \\(I\\) を超える支出はできない。 各財を並べた順番 \\(n = 1, \\dots, N\\) は完全に任意である。 \\(c_n\\) と \\(c_{n + 1}\\) がそのように並んでいることには何の理由もない。したがって，例えば， \\(c_1 &lt; c_2 &lt; c_3 &lt; \\cdots\\) という単調性（monotonicity）が得られたとしても， たまたまそのようになったという以上の解釈はできない。 モデルを変えずに，解釈を変更してみよう。経済には本質的には1つの財しかないとする。 例えば\\(c_1\\) は2001年の消費，\\(c_2\\) を2002年の消費という様に， 各 \\(c_n\\) をある特定の時点における経済活動を表す変数と解釈する。このような解釈の変更は， 解くべきモデルを変えることなく結果の解釈を変える。すなわち，先程の単調性は， 年々消費が増えていくということを意味している。説明上，\\(N\\) を有限としたが，\\(N = \\infty\\) のケースを考えることが多い。\\(N\\) が有限のとき， そのモデルは有限ホライズン（finite horizon）のモデルであるといい， そうでないとき無限ホライズン（infinite horizon）であるという。 各変数が時間によってラベル付けされたモデルを解き， その解が満たす経時的な性質を調べるのが経済動学の主要な課題である。 次のような命題に関心がある。 時間を通じて一定であるか，あるいは，十分時間が経てば収束状態に落ち着くか。 それとも，発散や恒常的な振動経路が観測されるか。 収束経路は単調に増加（あるいは減少）するか，それとも振動的か。 恒常的な振動が観測さるケースでは，振動は規則的（周期的， periodic）かあるいは不規則的（カオス，chaos）か。 経済状態は過去の経路に依存して決まるか，それとも経路依存性はないか。 経済状態は一意的に定まるのか，あるいは経済主体の「気まぐれ」が経路を変えてしまうことがあるのか。 etc. モデルの解を特徴付ける方程式を動学方程式（dynamic equation）, あるいは動的システム（dynamic system）という。 経済学のモデルでは，消費者の効用最大化・企業の利潤最大化・種々の制約条件からなる最適化問題が満たす均衡条件として， 時点が隣り合う変数が満たす方程式を得られることが多い[^上のリストで挙げた「経路依存性」がないケースである。]。 \\[ F(x_t, x_{t + 1}) = 0, \\quad t = 1, 2, \\dots \\] ただし，\\(x_t\\) は関心のある経済変数を並べたベクトルである。時間（time）を表すインデックスを \\(t\\) に変更した。経済政策などの外的な要因（ショック，shock）で経済環境が変化するケースを扱う場合には， \\[ F(x_t, x_{t+1}, z_t) = 0 \\] といった動学方程式が得られることになる。これらのシステムは陰関数の解として変数が定まるという 意味で陰的システム（implicit system）である。 経済の経時的な変化を知るためには，\\(x_1, x_2, \\dots\\) という時系列が必要であるから， 理想的には逐次的な計算公式 \\[ x_{n + 1} = G_t (x_t, z_t) \\] が必要である。あるいは，\\(G\\) の時間依存性がなければ， \\[ x_{n+1} = G(x_t, z_t) \\] とできるかもしれない。上で例示したような動的システムは解くことができない。 1.2 対象とするモデル この講義では，次のような動的システムを考察する。 \\[\\begin{equation} A\\mathbb{E}_{t}x_{t+1}=Bx_{t}+Cz_{t} \\tag{1.1} \\end{equation}\\] \\(x_t\\) はモデルを解いて決まる変数を並べたベクトル（内生変数，endogenous variable）， \\(z_t\\) はモデルの外で決まる変数（外生変数，exogenous variable）。\\(A\\), \\(B\\), \\(C\\) は適切なサイズの行列である。 \\(\\mathbb E_t\\) は条件付き期待値である。 行列積と足し算だけからなる上記のようなシステムを線形システム（linear system）という。上の線形方程式を \\[\\begin{equation} Bx_{t} = A\\mathbb{E}_{t}x_{t+1} - Cz_{t} \\tag{1.2} \\end{equation}\\] と書き換えると，今期の経済変数は将来に対する期待によって定まっていると読むことができる。 式(1.1) や 式(1.2) のようなシステムを 線形合理的期待モデル (linear rational expectations model) という。 本書ではモデルの導出に深く踏み込まないが，価格や消費といった変数は将来に対する予想を反映して (forward-looking) 決まるということだけ意識しておけば十分だろう。 予想を反映して価格が決まるという関係は次のような例を通して理解できる。 例 1.1 企業の株価はその企業が将来的に生み出す価値を利子率 (あるいは割引率) を考慮して割引いた値と一致するように決まる，というのが経済理論の基本公式である。 なお， 「割引」(discount) というのは収益を手にするまでに待たなければいけない時間分だけ減価調整することである。 このような関係が成立する理由は比較的容易に理解できる。 ある企業の期待収益の割引現在価値がその企業の株価総額よりも一時的に高いとしよう。 当該株式を購入すれば 長い時間を通じて購入価格よりも高い収益を得ることができるのだから， そのような株式には買い手が集まり，市場価格は上昇する。逆に， 期待収益が株価総額を一時的に下回っているとしよう。 株主は株式を手放すインセンティブを持つが，そのような市場価格では買い手はつかない。 売値を下げてでも売りたいと考える株主がいなくなるまで株価は下落するだろう。 予想した価値と市場価格の不一致が解消された結果として達成されるのが先の価格公式というわけである。 利鞘を稼ぐチャンス（裁定機会，arbitrage opportunity）が存在すれば， 市場を通じて価格が速やかに調整され，その結果として裁定機会は失われる。 経済理論は裁定機会が消滅した後の経済環境（均衡）に焦点を当てて分析を行うことが多いが， それは上のような思想に基づいている。 通常, 経済モデルは非線形システム（線形でないシステムをすべて非線形システムという） によって記述されることが多いのだが, 非確率的な平衡点 \\(x^* = x_t = x_{t+1}\\), \\(z_t = 0\\) （\\(t &gt; 0\\)）の周りに分析を限定すれば，線形システムによってよく近似されることが知られている2。 線形システムの分析は係数行列 (上記の \\(A\\)，\\(B\\)，\\(C\\)) の分析に落とし込むことができるため， 理論的にも数値的にも大変扱いやすい。したがって， これから我々が学ぶ分析手法は平衡状態にある経済に対して小さなショックが加わったときに経済変数がどのような経時的変化を示すかを分析するための第一歩である。 実際には線形近似のみから数量的なインプリケーションを導くことは困難であるから， 有用な分析を行うためには高次の近似手法を学ぶ必要がある。しかし， 線形理論を理解することなく非線形理論を理解することはできない。一歩一歩着実に進んでいこう。 1.2.1 決定論モデル 実は確率的な要因がなくなったとしても分析の基本的な方針は変わらない。すなわち， 非確率的 (決定論的) なシステム \\[\\begin{equation} Ax_{t+1} = Bx_t + Cz_t \\tag{1.3} \\end{equation}\\] を分析する手法を確立すれば，(1.1) の分析・シミュレーションができる。 まずは (1.3) の分析について述べたのち， 確率的な要因を導入するというステップで理論分析を進める。 さらに，非決定論的な分析を数段階に分けて解説する。 \\(A\\) が正則のケース 解析的手法 数値的手法 \\(A\\) が非正則のケース 解析的手法 数値的手法 \\(A\\) が正則のケースでは (1.3) は \\[\\begin{equation} x_{t+1} = A^{-1}Bx_t + A^{-1}Cz_t \\tag{1.4} \\end{equation}\\] と同値であるから（\\(A^{-1}\\) は \\(A\\) の逆行列）， 標準的な線形システム (状態空間方程式) と形式的には同じものである。 続いて，\\(A\\) が非正則のケースを扱う。上記の (1.3) は， 制御理論の分野でデスクリプタシステム（descriptor system）， あるいは陰的システム（implicit system）として知られている対象である。 この分野で研究を進めようという人は， 同じ概念が異なる分野で異なる名前で利用されていることを知っておく方がよいだろう。 ちなみに，デスクリプタシステムは「非因果的」（non-causal） なシステムを表現するために利用される。すなわち，未来の情報が現在に影響をおよぼすようなシステムである。 どこかで聞いたことのある話ではないだろうか？ 経済学における “forward-looking” は制御理論では非因果性と呼ばれている。 1.2.2 制御理論との違い 経済学と制御理論の扱う対象にはわずかながら違いがある。 制御理論では，制御変数を導入しなくても勝手に動作している対象を扱う一方で，経済理論では制御変数（例えば消費）は分析対象のモデルを構成する要素の一部である。したがって，経済モデルは \\(A\\)が正則であったとしても forward-looking（非因果的）な現象を表すように作られている。 力学系理論や制御理論で式(1.3)のようなシステムを扱うときには， 通常 \\(x\\) と同じ数だけの初期条件 (initial codition) を与える。 そのようなケースでは初期条件から出発してシステムの解を逐次的に求めることができるから， 解を求める上で特段の難しさはない。一方，経済学における forward-looking の表現は， システム方程式(1.3)とは独立している。 すなわち，変数（ベクトル） \\(x\\) の一部の要素に初期条件が与えられて， 残りの要素には初期条件が与えられないという形で forward-looking を扱う。 初期条件を持つ成分を先決成分 (predetermined component) とか先決変数 (predetermined variable)と呼ぶ。 初期条件を持たない成分を非先決成分 (non-predetermined component) とか非先決変数 (non-predetermined variable) と呼ぶ3。 例えば, 株式保有量を\\(a\\)，株価を\\(p\\)，株価に対する外的な影響を\\(u\\)として， ベクトル \\((a, p)\\) が次の動学方程式を満たすというモデルを作ったとしよう。 \\[ \\begin{bmatrix} a_{t+1} \\\\ p_{t+1} \\end{bmatrix} = B \\begin{bmatrix} a_{t} \\\\ p_{t} \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ u_{t} \\end{bmatrix} \\] このとき \\(a_0\\) は初期値として与えられているが，\\(p_0\\) は \\(a_0\\) および \\(u\\) に対する予想に基いて決まるというのが典型的なマクロ経済学の問題である. 1.2.3 安定性と決定性 さて，一部の変数に初期条件が与えられていない問題をどのように解けばよいだろうか。 もちろん，\\(a_1, a_2, \\dots\\) や \\(p_0, p_1, \\dots\\) に対して何の制約も置かなければこのような問題を解くことはできない。 Blanchard and Kahn (1980) が提案した条件は次のようなものである。 経済主体は幾何級数よりも早いスピードで発散するような予想に基いて行動することはない。 すなわち，幾何級数より早く発散するような初期値を除いた結果，初期値を1点に定めることができれば， その点は一意の均衡である。しかし，動学方程式を満たしつつ， 安定性（非幾何的発散）を満足するような経路は一般には無数に存在する。一意的であるようなケースを 決定的 (deterinate) と呼び，決定的でないケースを 不決定的 (indeterminate) と呼ぶ。 1.2.4 合理的期待モデル 続いてシステム (1.1) の分析を行う. ショック項 \\(z\\) を確率過程となるが, 決定論の分析を理解していれば, 条件付き期待値の性質を少し覚えるだけで理解できるようになる. 決定論モデルと同じ安定性の概念を利用して決定性を特徴づけることができるので, 実は決定論システムと本質的な違いはない. 本書では, やや発展的な話題を紹介する. 1.2.5 マルコフスイッチングモデル 次に, 係数行列がマルコフ連鎖に従って変化するモデルに拡張する. このような拡張をすると, 伝統的な合理的期待モデルが利用してきた安定性の概念を利用することができなくなる. 二乗平均安定性 (Mean Square Stability)という安定性概念を導入し, 決定性を特徴付ける. 1.3 プログラミング環境 1.3.1 RStudio 前述のような線形モデルは (広く普及している C, Fortran のルーチンのおかげで) 大抵のプログラミング言語で解くことができる. 本書で R を使用するのは, 想定する読者層がもっとも接近しやすい言語であろうと考えたからである. 計量経済学の学習は Stata を使い, マクロ経済学では Matlab, 数理経済学の授業では Python というようなことにもなりがちだが, プログラミングの初級者は1つの言語に深く習熟するようにした方がよい. 将来的に他の言語に移るとしても, 広く浅く学んだ人よりも深く狭く学んだ人の方が, 速く学ぶ. R は計量経済学のツールとしてもとても優秀で, しかも多くのユーザーコミュニティがある. データ収集, 図示, モデリング, ドキュメント作成のすべての工程をRとRStudio で実行することができる. RStudio に触れる機会を増やすために, すべてのレポートを RStudio で作成するように気持ちを切り替えてみよう. RMarkdown という書式で書き, それをコンパイルする. knitr や pandoc がスムーズに仕事をこなしてくれる. PDF に変換するには texlive をインストールする. サイズは大きいが簡単だ. 数式を入力するには LaTeX の構文を覚える必要があるが, 大学院にいる以上いずれ必要になる技術だから, 今やっておいて損はない. RMarkdown は LaTeX そのものと比べると概ね自然に書くことができるし, 何よりも R のコードや計算結果, グラフを文章内に埋め込むのが非常に簡単だ. ところで, 本書も RStudio で書いている. いくつかの RMarkdown ファイル, 設定ファイルを bookdown というパッケージがうまく処理してくれる. R に十分習熟し, プロファイリングの努力も虚しく計算速度に限界を感じることがあるかもしれない. そうなった C++ を少し学べばよい. 重い処理だけを扱う C++ のコードを用意し, RCpp パケージを使って呼び出すことができる. そのような願望が現れるころには計算機の仕組みをある程度理解しているだろうから, C++ も速く学べると思う. 本書のコードに関する解説は, 読者が RStudio で作業をしているものと想定している. Rプログラミングの経験が浅い読者は, 筆者と同様の環境を構築しておくほうが混乱がないだろう. 本節の残りの部分で RStudio の使い方に関する簡単な説明と, 環境構築を行う. 1.3.2 準備 R 本体も RStudio もバイナリファイルをダウンロードして簡単にインストールすることができるので, インストールに関する詳細は省略する. このコースの学習用に RStudio プロジェクトファイルを用意しておこう. RStudio を起動し, メニューから「File &gt; New Project…」をクリックして新しいプロジェクトを作成する. 作成方法に関していくつかの選択肢を提示されるので, 特に問題がなければ「New Directory &gt; Empty Project」を選ぶ. 必要な入力項目を入力する. 例えば, プロジェクト名をlinear-economic-dynamics としておこう. 指定したディレクトリの配下に, linear-economic-dynamics という名前の新しいディレクトリ (フォルダ) , さらにその中に linear-economic-dynamics.Rproj というファイルが作成される. 作業再開時には, linear-economic-dynamics.Rproj をダブルクリックして, RStudio が開く. 先程作ったディレクトリが作業ディレクトリになる. プロジェクト作成直後はすでに作業ディレクトリを移っているのでそのまま進めて構わない. 次の用語を説明する. 説明できない場合は検索して調べる. ディレクトリ, フォルダ, directory, folder 作業ディレクトリ, working directory / current directory 相対パス, relative path 絶対パス, absolute path コンソール, console スクリプトファイル, script file コマンドの実行はコンソールに入力＋Enter 押下でもよいし, コマンドをスクリプトファイルに書いて実行してもよい. コンソール Console と書かれたエリアを探してほしい. 設定をいじっていなければ,4 R version 3.3.2 (2016-10-31) -- &quot;Sincere Pumpkin Patch&quot; Copyright (C) 2016 The R Foundation for Statistical Computing Platform: x86_64-apple-darwin13.4.0 (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type &#39;license()&#39; or &#39;licence()&#39; for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type &#39;contributors()&#39; for more information and &#39;citation()&#39; on how to cite R or R packages in publications. Type &#39;demo()&#39; for some demos, &#39;help()&#39; for on-line help, or &#39;help.start()&#39; for an HTML browser interface to help. Type &#39;q()&#39; to quit R. &gt; という表示が見えるはずだ. 最後の記号 「&gt;」は「プロンプト」と呼ばれる. ここにコマンドを入力せよという意味である. 本書では, コンソールに入力して実行して結果を確認するという文脈であってもプロンプトを表示しない. 各自入力して実行してほしいコマンドは次のようなグレーのボックスで表示する. 10 * 3 ## [1] 30 [1] 30 は実行結果であり, 10 * 3 の結果が 30 であることを意味している. 先頭についている ## は特に意味はない. [1] は今のところ無視してもよい. コマンドの直後に配置された白背景のボックスに直前のコマンドの実行結果を掲載する. プロンプトを省略することで, 複数行にわたるコマンドであってもコピー＆ペーストで実行できるという利点がある. 次のコードをまるごとコピーして, コンソールにペーストし, Enter を押下すればPlotsペインに結果が出力されるはずだ. # 01-cobbdouglas-plot.R A = 1.2 alpha = 0.3 cobbdouglas = function(k) { return(A * k ^ alpha) } k = seq(0, 10, length.out=200) y = cobbdouglas(k) plot(k, y, type=&#39;l&#39;) 図 1.1: ‘01-cobbdouglas-plot.R’ の実行結果 # に続く内容は, プログラマが読むためだけのコメントである. 本書の約束事として, コードチャンクの1行目のコメントには, スクリプトとして保存する場合のファイル名や, 本文中で参照するための短いキャプションを書くことにする. スクリプトファイル スクリプトファイルを保存するディレクトリを作っておこう. Files ペインを探して内容を見ると linear-economic-dynamics.Rproj ファイルを見つけられる思う. そこがプロジェクトディレクトリの最上位階層 (ルートディレクトリ) である. もし当該ファイルが見当たらなければ, ディレクトリ階層を移動して探してほしい. 元いた場所から離れることができたのだから戻ることもできるはずだ. 帰れなくなったら RStudio を終了して, 再び .Rproj をダブルクリックして起動しなおせばよい. プロジェクトの .Rpoj ファイルが見つかったら「New Folder」というボタンを押して, 「R」という名前の新しいディレクトリを作る. スクリプト (script)とは1つ以上のコマンドをまとめたファイルのことである. R はスクリプトファイルを読んで, 上から順番に実行することができる. コンソール上での作業は複数行に渡るコードを入力するには不向きなので, 別途ファイルに保存しておくことで生 RStudio で新しいスクリプトファイルを作るにはいくつかの方法がある. RStudio 左上にあるるボタン (白い四角の上に足し算記号のマーク) から「R Script」を選びクリックする. メニューから「File &gt; New File &gt; R Script」と進む. キーボードショートカット Ctrl+Shift+N / Cmd+Shift+N を押下する 最後のキーボードショートカットを覚えるのが一番よいだろう. Cmd+Shit+N を押下すると, Untitled1 という名前でソースペインが開く. ここに先程の 01_cobbdouglas_plot.R のコードをコピー＆ペーストしてみよう. Ctrl+S / Cmd+S によって保存しようとすると, 保存場所と名前を決めるように促されるので, 先程作成しておいた R というフォルダに, 01_cobbdouglas_plot.R という名前で配置する. プロジェクトディレクトリは次のような構成になっているはずだ.5 linear-economic-dynamics ├── R │   └── 01_cobbdouglas_plot.R └── linear-economic-dynamics.Rproj 作業ディレクトリ (linear-economic-dynamics) にある, R というサブディレクトリの中にある 01_cobbdouglas_plot.R というスクリプトファイルを実行するには以下のようにする. source(&#39;R/01_cobbdouglas_plot.R&#39;) 作業ディレクトリを起点としたファイルの位置を相対パス (relative path)という. スクリプト内にファイルパスを書く場合は, やむを得ない事情で移動できない場合を除いて必ず相対パスで指定するようにする. さきほどのスクリプトファイルはもっと下位のサブディレクトリに配置することもできる. 例えば, 章ごと節ごとにディレクトリを作って source(&#39;R/part1/chapter03/section04/subsection01/01.R&#39;) ということも可能ではある. しかし, 階層が深くなりすぎると管理が難しくなるので, できるだけフラットにしておくようにしよう. この本では, R/chapternumber_descriptive_name.R という形式のファイル名をつけ, すべて R ディレクトリの直下に配置する. 1.3.3 パッケージ tidyverse 本書では tidyverse パッケージを利用する. tidyverse はRのデータフレーム操作を円滑にするためのパッケージ群である. 必要なもの以上に関数を読み込むことを望まない場合には, ggplot2, dplyr, tidyr を必要に応じて読み込むとよい. 試しにプロンプトに と打ち込んでみよう. エラーが出る場合には次のコマンドでインストールしてから, 再度実行する. install.packages(&quot;tidyverse&quot;) Loading tidyverse: ggplot2 Loading tidyverse: tibble Loading tidyverse: tidyr Loading tidyverse: readr Loading tidyverse: purrr Loading tidyverse: dplyr Conflicts with tidy packages --------------------------- filter(): dplyr, stats lag(): dplyr, stats といったメッセージが出ると思う. tidyverse はパッケージ群なので, 幾つかの個別のパッケージを読み込むのが仕事である. Conflicts が出て心配になるかもしれないが, 特に問題はない. filter(), lag() という関数がデフォルトで読み込まれているにもかかわらず, 同じ名前を持つ dplyr を読み込んだことでfilter(), lag() という関数の実体が変更されてしまった. もし, デフォルトの filter() を使いたければ, stats::filter() と呼び出せばよい. stats:: や dplyr:: といったプレフィックスは, 関数やオブジェクトがどのパッケージで定義されたものかを明示したいときに使う. 本書では, base::data.frame() の代わりに tibble::tibble() を利用する. tibble パッケージは tidyverse に含まれている. 違いを簡単に見ておこう. x = c(1, 2, 3) y = c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;) z = c(TRUE, TRUE, FALSE) (df = data.frame(x = x, y = y, z = z)) ## x y z ## 1 1 a TRUE ## 2 2 b TRUE ## 3 3 c FALSE (tbl = tibble(x = x, y = y, z = z)) ## # A tibble: 3 × 3 ## x y z ## &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 1 a TRUE ## 2 2 b TRUE ## 3 3 c FALSE base のデータフレームと比べて, tibble は次の点で扱いやすい. データのサイズを表示してくれる コラムの型情報を表示する (&lt;dbl&gt;, &lt;chr&gt;, &lt;lgl&gt;) 文字列をファクター型にしない ファクター型は, 文字列を内部的に整数値として保存している. これを文字列と思って扱うと問題が起こる. 例えば, &quot;a&quot; &lt; &quot;b&quot; ## [1] TRUE と同じ振る舞いを期待して, 次のような評価をすると問題が起こる: df$y[1] &lt; df$y[2] ## Warning in Ops.factor(df$y[1], df$y[2]): &#39;&lt;&#39; not meaningful for factors ## [1] NA QZ 同様に QZ パッケージもインストールしておこう. 後々利用することになる. install.packages(&quot;qz&quot;) devtools パッケージ開発環境. devtools::github_install() を使って, github レポジトリからパッケージをインストールできるようになる. Step 1: コンパイラをインストールする. Windows: Rtools (https://cran.r-project.org/bin/windows/Rtools/) をインストール Mac: Xcode をインストール Step 2: CRAN からインストール. install.packages(&quot;devtools&quot;) 参考文献 "],
["complexnumbers.html", "第2章 複素数 2.1 はじめに 2.2 複素数 2.3 複素平面（imaginary plane） 2.4 R コード 2.5 極形式 2.6 共役複素数 2.7 補遺：ラムゼーモデル", " 第2章 複素数 2.1 はじめに マクロ経済学の導入的なトピックとして1セクターの最適成長理論を学び，その後，多セクターモデルを学ばないというケースもあるかもしれない。1セクターモデルの典型的な成長経路は単調であり，補遺で詳しく説明するように，このようなケースで複素数を考える必要はない。しかし，応用上用いられるモデルの多くは複数のセクターと多数の内生変数によって構成されるため，1セクターモデルのように単純な成長経路を描かない可能性がある。例えば，ある1点の周りを回転しながらその点に接近するような経路を描くような動学経路を表現するには，実数を考えるだけでは難しい。経済モデルの動学を理解するには複素数の取扱いが必須なのである6。 この節は，経済学の学習の中で複素数に出会ったことのない人を対象とした入門的なトピックを扱っている。上記のような話を具体的なモデルの中で説明できる読者は読み飛ばしても構わない。あるいは，マクロ経済学が未修であって行列の標準化の理論に明るい読者は補遺まで読み飛ばしてもよい。 2.2 複素数 実数 (real number) の集合を \\(\\mathbb R\\) と書く。\\(\\mathbb R\\) に含まれない「数」を1つ追加し，四則演算を自由にできるようにしたものが複素数 (complex number) の集合 \\(\\mathbb C\\) になる。 追加する数は虚数単位 (imaginary unit)と呼ばれる。これは \\[ i^2 + 1 = 0 \\] を成り立たせる \\(i\\) のことである。実際にはそのような \\(i\\) は複数ある (かもしれない) ので，そのうちの1つに \\(i\\) という名前をつける。実数の範囲にはこのような数は存在しないから，\\(i\\) の追加によって \\(\\mathbb R\\) より大きい集合を考えることになる.7 定義 2.1 複素数 (complex number) とは，実数 \\(a\\)，\\(b\\) を用いて \\[ a + bi \\] と書ける数のことをいう。\\(a\\) を実部 (real part)，\\(b\\) を虚部 (imaginary part) という。 いつも \\(a + bi\\) のように書くと読みにくいので，\\(z \\in \\mathbb C\\) に対して, \\[ \\mathrm{Re}~z = a，\\qquad \\mathrm{Im}~z = b \\] のような書き方をする. 複素数同士の四則演算を次のように定義する. 定義 2.2 任意の \\(a_1，a_2，b_1，b_2 \\in \\mathbb R\\) に対して, \\[ \\begin{aligned} (a_1 + b_1 i) + (a_2 + b_2 i) &amp;= (a_1 + a_2) + (b_1 + b_2) i \\\\ (a_1 + b_1 i) - (a_2 + b_2 i) &amp;= (a_1 - a_2) + (b_1 - b_2) i \\\\ (a_1 + b_1 i) (a_2 + b_2 i) &amp;= (a_1 a_2 - b_1 b_2) + (a_1 b_2 + a_2 b_1 ) i. \\end{aligned} \\] \\(a_2 \\neq 0\\) または \\(b_2 \\neq 0\\) であれば， \\[ \\begin{aligned} \\frac{a_1 + b_1 i}{a_2 + b_2 i} = \\left( \\frac{a_1 a_2 + b_1 b_2}{a_2^2 + b_2^2} \\right) + \\left( \\frac{- a_1 b_2 + a_2 b_1}{a_2^2 + b_2^2} \\right) i. \\end{aligned} \\] 2.3 複素平面（imaginary plane） 1つの複素数は \\((a, b)\\) という実数のペアと同一視できるので，平面上の1点として複素数を表現できる。 図2.1 参照。 図 2.1: 複素平面 複素平面は動学の分析に重要な役割を果たすので自ら図を描いて手になじませておくとよい。 2.4 R コード R では複素数を簡単に使うことができる。例えば次のように書くと，z という名前の変数に \\(5 + i\\) という複素数を代入するという意味になる。コンソールで実行してみてほしい. z = 5 + 1i z ## [1] 5+1i 上のコードを次のように書き換えると正しく動くだろうか？何が起こるかを予想する。その後，Rコンソールで実行して予想した結果と比べる. 1 と i の間にはスペースを入れる. 1 を省略して 5 + i と書く. 実部 複素数を代入した変数をもっていれば，関数 Re() によってその実部を取得できる。 Re(z) ## [1] 5 虚部 Im() によって虚部を取得できる. Im(z) ## [1] 1 四則演算 四則演算も通常の数と同じようにできる。 加算 w = 4 - 3i z + w ## [1] 9-2i 減算 z - w ## [1] 1+4i 乗算 z * w ## [1] 23-11i 除算 z / w ## [1] 0.68+0.76i 実数との演算 実数と複素数の演算も自然に行うことができる。結果は常に複素数になる。 10 + z ## [1] 15+1i 注意 R はゼロで割ってもエラーにならない。 z / 0 ## [1] Inf+Infi z / (0 + 0i) ## [1] Inf+Infi 次の結果は複素数になるが z * 0 ## [1] 0+0i 次の結果は真値を返す。 z * 0 == 0 ## [1] TRUE 複素平面上の図示 ggplot2 による作図の基本 base R の plot() 関数は複素数に対応しているが，残念ながら ggplot2 はそのようにはできていない。とは言え ggplot2 の方が強力なので，ggplot2 のイディオムを習得してほしい。 まず実部と虚部を分けてデータフレームに保存しておく。base R の data.frame() ではなく tidyverse パッケージ (tibble パッケージ) の tibble() を用いる。本書では tibble データフレーム（tbl_df）もデータフレームと呼ぶ。 points = tibble(z = c(z, w, z / w)) %&gt;% mutate(Re = Re(z), Im = Im(z)) points ## # A tibble: 3 × 3 ## z Re Im ## &lt;cplx&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.00+1.00i 5.00 1.00 ## 2 4.00-3.00i 4.00 -3.00 ## 3 0.68+0.76i 0.68 0.76 今作ったデータフレームの re 列を横軸に，im 列を縦軸にプロットすればよい。 ggplot(points) + geom_point(aes(x = Re, y = Im, color = factor(z))) + coord_fixed() + labs(color = &quot;z&quot;) + xlim(-5, 5) + ylim(-5, 5) # この行は装飾 べき乗 上の方法をもう少し進めてみよう。同じ複素数を繰り返し掛けるとどうなるだろうか。 \\[ z_p = 0.5 + 0.5i \\] として，\\(z_p^n\\) を計算する。ただし，\\(z_p^n\\) は \\(z_p\\) のべき乗で，実数と同様 \\(z_p^0 = 1\\)，\\(z_p^n = z_p z_p^{n-1}\\)，\\(n &gt; 1\\) と定義する8。 zp = 0.5 + 0.5i zn = tibble(Re = Re(1), Im = Im(1), n = 0) for (i in 1:10) { zn = zn %&gt;% add_row(Re = Re(zp ^ i), Im = Im(zp ^ i), n = i) } ggplot(zn) + geom_point(aes(x = Re, y = Im, color = factor(n))) + labs(color = &quot;Power&quot;) + coord_fixed() + xlim(-1.02, 1.02) + ylim(-1.02, 1.02) \\(1\\) に \\(0.5 + 0.5i\\) を繰り返し掛けると次第に原点に近づいていくことが分かる。これは，実部・虚部ともに \\(0.5\\) という比較的小さな数字だから成り立ちそうな気がする。実数の場合，\\(\\lim_{n\\to \\infty} a^n = 0\\) となるのは，\\(-1 &lt; a &lt; 1\\) の場合ということを思いだしてほしい。複素数の場合には，実部・虚部のそれぞれが\\(1\\) より小さい絶対値を持てば原点に収束するだろうか？ 例えば次の例を考えてみよう。 \\[ z_q = 0.9 + 0.9i \\] はたして \\(z_q^n\\) は原点に収束するだろうか？ zq = 0.9 + 0.9i zm = tibble(re = Re(1), im = Im(1), n = 0) for (i in 1:10) { zm = zm %&gt;% add_row(re = Re(zq ^ i), im = Im(zq ^ i), n = i) } ggplot(zm) + geom_point(aes(x=re, y=im, color=factor(n))) + coord_fixed() + xlim(-12, 12) + ylim(-12, 12) 上の図を見る限り，回転しながらどんどん原点から離れていっているようだ。原点への収束，原点からの乖離はどのような条件で特徴付けられるだろうか。 2.5 極形式 複素数のべき乗を計算するということは \\(1\\) に同じ複素数を繰り返し掛けることに外ならない。その結果として，上で描いた2つの図のように回転と拡大・縮小という現象を確認することができた。 複素数 \\(z = a + bi\\) を平面上の点 \\((a, b)\\) とみなせるというのが，複素平面を描いた際に念頭に置いていた事実であった。さらに平面上の点 \\((a, b)\\) は原点からの距離と 矢線ベクトル \\((1, 0)\\) を基準とした回転角で表せるということを思い出してほしい。つまり，ある \\(0 \\le \\theta &lt; 360^\\circ\\) が存在して次の等式が成立する. \\[ \\begin{aligned} a = \\sqrt{a^2 + b^2} \\cos \\theta\\\\ b = \\sqrt{a^2 + b^2} \\sin \\theta\\\\ \\end{aligned} \\] \\(r = \\sqrt{a^2 + b^2}\\ge 0\\) を \\(z = a + bi\\) の絶対値 (absolute value)という。\\(z\\) の絶対値を \\(|z|\\) で表す。\\(\\theta\\) を偏角 (argument) という (\\(\\mathrm{arg}(z)\\) で表すこともある)。すべての複素数が \\[ z = r(\\cos\\theta + i\\sin\\theta) \\] という表現を持つ。この表現を複素数の極形式 (polar form)という。図2.2 参照。 図 2.2: 極形式 絶対値が\\(1\\) で偏角の異なる2つの複素数 \\(\\cos\\theta_1 + i\\sin\\theta_1\\) と \\(\\cos\\theta_2 + i\\sin\\theta_2\\) を掛け合わせると \\[\\begin{align} &amp;(\\cos\\theta_1 + i\\sin\\theta_1)(\\cos\\theta_2 + i\\sin\\theta_2) \\notag \\\\ &amp;\\qquad= (\\cos\\theta_1 \\cos\\theta_2 - \\sin\\theta_1\\sin\\theta_2) + i(\\sin\\theta_1\\cos\\theta_2 + \\cos\\theta_1\\sin\\theta_2) \\notag \\\\ &amp;\\qquad= \\cos(\\theta_1 + \\theta_2) + i\\sin(\\theta_1 + \\theta_2) \\tag{2.1} \\end{align}\\] を得る.9 絶対値が\\(1\\)でない場合にも，\\(r_1, r_2 &gt; 0\\) として, \\[ \\begin{aligned} r_1(\\cos\\theta_1 + i\\sin\\theta_1)\\cdot r_2(\\cos\\theta_2 + i\\sin\\theta_2) = r_1 r_2 \\left[ \\cos(\\theta_1 + \\theta_2) + i\\sin(\\theta_1 + \\theta_2) \\right] \\end{aligned} \\] つまり，複素数 \\(r_2(\\cos\\theta_2 + i\\sin\\theta_2)\\) を掛けるという操作は，絶対値を \\(r_2\\) 倍に伸縮し，偏角を \\(+\\theta_2\\) だけ回転させる作用がある。 オイラーの公式 式(2.1) は複素数の積が偏角の和に相当することを述べている。この等式を眺めて指数関数との関連性に気がつく人もいるかもしれない。実際， \\[\\begin{align} e^{i\\theta} = \\cos\\theta + i\\sin\\theta \\tag{2.2} \\end{align}\\] と定義すれば，指数法則 \\[ e^{i\\theta_1} e^{i\\theta_2} = e^{i(\\theta_1 + \\theta_2)} \\] によって，式(2.1) を「導出」できる。本来は，\\(e^{i\\theta}\\) が意味するところをきちんと定義して，等式 (2.2) を証明する必要があるだろうが，ここでは記号として「オイラーの公式」を紹介した。関心のある読者は適当な複素関数論の教科書を読めばよい。 すべての複素数が \\[\\begin{equation} z = re^{i\\theta}, \\quad r \\ge 0,\\ 0^\\circ \\le \\theta &lt; 360^\\circ \\end{equation}\\] という表現を持つということを知っていればよい。なお, この表現は次のようにして, \\(\\theta &lt; 0^\\circ\\), \\(\\theta \\ge 360^\\circ\\) に拡張することができる: 任意の \\(\\theta\\) に対して, \\(\\theta = \\theta_0 + n \\times 360^\\circ\\) なる整数 \\(n = 0, \\pm 1, \\pm 2, \\dots\\) がたった1つだけ存在する。\\(e^{i360^\\circ} = 1\\) に注意すれば, \\[\\begin{equation} e^{i\\theta} = e^{i\\theta_0}\\left(e^{i360^\\circ}\\right)^n = e^{i\\theta_0} \\end{equation}\\] を得る。 べき乗 以上の準備の下で, \\[ \\begin{aligned} \\lim_{n\\to\\infty} z_p^n = (0.5 + 0.5i)^n \\to 0, \\quad \\text{and}\\quad \\lim_{n\\to\\infty} z_q^n = (0.9 + 0.9i)^n \\not\\to 0 \\end{aligned} \\] について説明することができる。 極形式による表現 \\[ \\begin{aligned} z_p = r_p e^{i\\theta_p}\\\\ z_q = r_q e^{i\\theta_q} \\end{aligned} \\] によれば, \\[ \\begin{aligned} r_p = \\sqrt{0.5^2 + 0.5^2} \\simeq 0.7071068 &lt; 1\\\\ r_q = \\sqrt{0.9^2 + 0.9^2} \\simeq 1.2727922 &gt; 1. \\end{aligned} \\] したがって, \\[ \\begin{aligned} |z_p^n| &amp;= |r_p^n e^{i n\\theta_p}| = r_p^n \\to 0\\\\ |z_q^n| &amp;= |r_q^n e^{i n\\theta_q}| = r_q^n \\to \\infty \\end{aligned} \\] を得る。この観察をまとめておこう。 \\(|z| &lt; 1\\) ならば \\(\\lim_{n\\to\\infty} |z^n| = 0\\), \\(|z| &gt; 1\\) ならば \\(\\lim_{n\\to\\infty} |z^n| = \\infty\\), \\(|z| = 1\\) ならば 任意の \\(n\\) について \\(|z^n| = 1\\). べき乗の収束性（安定性）は考えている複素数が複素平面上で原点を中心とする単位円（unit circle）の内側（単位円盤 unit disk の上）にあるかどうかで決まる。図2.3において，\\(z_1\\)のべきは発散し，\\(z_2\\)のべきは原点に収束する。 図 2.3: 単位円 2.6 共役複素数 複素数 \\(z = a + bi\\) に対して, \\(\\bar z = a - bi\\) を共役複素数 (complex conjugate)あるいは複素共役という。\\(z\\) は \\(\\bar z\\) の共役複素数であるから, \\[\\begin{equation*} \\bar{\\bar z} = z \\end{equation*}\\] が成り立つ。（2.4） 図 2.4: 共役複素数 共役複素数には次の性質がある。 任意の複素数 \\(z\\) に対して, \\[ z\\bar z = |z|^2 \\] を示せ。 R では, Conj() で複素共役を, abs() で複素数の絶対値を計算できる. z ## [1] 5+1i Conj(z) ## [1] 5-1i abs(z) ## [1] 5.09902 先程の練習問題の性質は z * Conj(z) と abs(z) ^ 2 の差が十分ゼロに近いことで確認できる。 z * Conj(z) - abs(z) ^ 2 ## [1] 3.552714e-15+0i e-15 というのは，10^{-15} を意味しているので，大変小さい数字であることが分かるだろう。あるいは，次の様にすればよい. all.equal(z * Conj(z), abs(z)^ 2 + 0i) ## [1] TRUE 計算機上の小数 (浮動小数点数) は実数を有限近似したものに過ぎないので等号で評価することはできない。有限の長さを持つように見えるありきたりな有理数でさえ，等号による評価はあてにならない。例えば次のような例がある. sum = 0 for (i in 1:10) { sum = sum + 0.1 } sum == 1 ## [1] FALSE 0.1 を 10回足しても 1 にはならない。小数の比較に == を用いてはいけない。 多項式方程式の解 後ほど明らかになるように，線形システムは固有多項式と呼ばれる実係数多項式を「因数分解」する問題を通じて分析される。あるいは， 本質的には同じことだが，実係数多項式方程式の解（根 root）を調べる10。いずれにせよ実係数多項式方程式が重要な役割を果たす。 多項式方程式は次のような性質を持つ。 定理 2.1 (代数学の基本定理) \\(a_n, \\dots, a_0 \\in \\mathbb C\\) とする。\\(z\\) の多項式 \\[ p(z) = a_n z^n + a_{n-1} z^{n-1} + \\cdots + a_1 z + a_0 \\] は，複素数の範囲で（重根を重複度の回数数えれば）ちょうど \\(n\\) 個の根（root）をもつ。 適当な代数学・複素関数論の本を参照のこと。複素係数の多項式方程式の解は必ず複素数に含まれる。 この性質をもって \\(\\mathbb C\\) は代数的閉体であると言われる11。 実数係数多項式方程式が複素根をもつ時，その共役複素数もまた根である。 定理 2.2 \\(a_n, \\dots, a_0 \\in \\mathbb R\\) とする。\\(z\\) の多項式 \\[ p(z) = a_n z^n + a_{n-1} z^{n-1} + \\cdots + a_1 z + a_0 \\] が\\(z_0 \\in \\mathbb C\\) で \\(p(z_0) = 0\\) を満たせば，\\(p(\\bar z_0) = 0\\) が成り立つ。 証明. 任意の \\(a \\in \\mathbb R\\) に対して\\(\\bar a = a\\) であることと，\\(\\overline{z^n} = (\\bar z)^n\\) に注意する。\\(\\overline{p(z)} = p(\\bar z)\\) が成り立つので，\\(p(z) = 0\\) ならば \\(p(\\bar z) = 0\\) が言える。 2.7 補遺：ラムゼーモデル 章の冒頭に1セクターモデルの話を書いたので，マクロ経済学を未修の読者のために補足をしておこう。 大学院初級のマクロ経済学を履修したことがあって，「複素数は出てこなかったし，これからも出会うことはない」と考えていた読者にもぜひ読んでほしい。 多くのマクロ経済モデルは最適成長モデル（optimal growth model）あるいはラムゼーモデル（Ramsey model）と呼ばれる基本モデルをベースに組み立てられている。 最適成長モデルは次のような無限ホライズンの最大化問題として記述される。 \\[ \\begin{aligned} &amp; \\max_k \\sum_{t = 0}^\\infty \\beta^t u(k_t, k_{t + 1}) \\\\ &amp; \\begin{array}{ll} \\text{subject to} &amp; k_{t + 1} \\in \\Gamma(k_t), \\ t = 0, 1, \\dots \\\\ &amp; k_0 &gt; 0: \\text{given} \\end{array} \\end{aligned} \\] 登場する記号について少しずつ説明していこう。 標準的な成長理論の文脈では \\(k_t\\) は \\(t\\)-期の期初の資本（capital at the beginning of period \\(t\\)）, \\(k_{t + 1}\\) は\\(t\\)-期の期末の 資本（capital at the end of period \\(t + 1\\)），これはすなわち\\((t+1)\\)-期の期初資本，と解釈される。 次期に残すことのできる資本 \\(k_{t+1}\\) は経済で利用可能な生産技術 \\(\\Gamma\\) と今期利用可能な資本 \\(k_t\\) によって定まる。これが制約式\\(k_{t + 1} \\in \\Gamma(k_t)\\) の意味するところである。 標準的なマクロモデルでは，\\(\\Gamma\\) は生産技術を表す関数 (生産関数, production function) \\(f\\) を使って \\[ \\Gamma(x) = [0, f(x)] \\] と表される対応（correspondence）と考えることが多い12。 すなわち，\\(0 \\le k_{t + 1} \\le f(k_t)\\) が成り立たなければならないというのが制約条件の要請である。 生産関数 \\(f\\) は \\(f(0) = 0\\)，\\(f&#39;(x) &gt; 0\\)，\\(f&#39;&#39;(x) &lt; 0\\) などの性質を持つものと仮定される。1つ目の性質は， 生産には要素投入（factor input）が必須であることを表している。2つ目は，投入量が多ければ産出も多いことを意味している。 最後の性質は，限界生産性逓減（diminishing marginal productivity）と呼ばれる性質で， 要素投入１単位当たりの成果物が徐々に小さくなっていくことを意味している。 1日100枚皿を作れる職人を100人雇っても1日に10000枚の皿を作れるようにはならない。 コミュニケーションや利用できる土地などの制約によってどうしてもボトルネックが生じる。 1セクターモデルというのは，数学的には，\\(k_t\\)，\\(t = 0, 1, \\dots\\)， が（非負の）実数であるようなモデルのことである。経済学の言葉を使うと，財は1種類のみ 存在している。資本を \\(k_t\\) だけ持っている経済において，\\(f(k_t)\\) の産出が得られる。 次期に残す資本として \\(k_{t + 1}\\) を確保しようとする。 簡単ではあるが，これで投資（investment）とか貯蓄（saving） と呼ばれる経済活動の一番簡単な定式化になっている。 さて，残った \\(f(k_t) - k_{t+1}\\) はどこに行っただろう。実は，1セクターモデルでは 経済主体がこれを食べてしまうと考える。機械や建物（生産要素）としても， さらには食事（最終財）にも使える財を1種類だけ生産し，最適な貯蓄と消費のバランスを 見つけるのが 1 セクターモデルの目標である。いかにも奇妙ではあるが， 長期的な経済成長に関する比較的良好な見通しを得ることができる。 \\(t\\)-期の消費（consumptions）を \\(c_t = f(k_t) - k_{t + 1}\\) と表そう。さらに，経済主体は消費のみから効用を 得ると仮定する。すなわち，資本が沢山あったとしてもそれ自体は効用を生まない。 もちろん，これも単純化のための仮定である。各期の消費から得られる効用の水準を効用関数（utility function），\\(U(c_t\\)， で表せるとして， \\[ U(c_t) = U(f(k_t) - k_{t + 1}) = u(k_t, k_{t + 1}) \\] とできる。右辺のように書き直したものが \\(u\\) の正体である。\\(u\\) を既約型効用関数（reduced-form utility function）という13。 消費量は多ければ多いほうが幸せなので，\\(U\\) は単調増加である。ただし，1単位の 追加的な消費量に対して \\(U(c)\\) が増える程度，\\(U&#39;(c)\\)，は \\(c\\) の大きさに依存して変化するする。 典型的には \\(c\\) が大きければ大きいほど \\(U(c)\\) は増えにくくなるだろうから，\\(U&#39;&#39;(c) &lt; 0\\) と仮定するのが標準的である。これを限界効用逓減（diminishing marginal utility）という。 さて，我々が最大化したい対象は \\(U(c_t)\\) ではない。これを重み付き平均を取ったものである。 \\(0&lt; \\beta &lt; 1\\) を割引因子（discount factor）という14。 重み \\(\\beta^t\\) は \\(t\\) が大きくなるに連れてゼロに近づいていく。すなわち，将来の消費は 現在の消費と比べるとウェイトが低く重要ではないということを表現している。\\(\\beta\\) が1に近いほど， 減少のスピードがゆっくりになるので，将来を比較的大切に考える経済主体のモデルとなる。 最適化のための必要条件は \\[ u_2(k_{t-1}, k_t) + \\beta u_1 (k_t, k_{t+1}) = 0 \\] で与えられる。ただし，\\(u_1 = \\partial u/\\partial k_t\\), \\(u_2 = \\partial u/\\partial k_{t+1}\\)。 この動学方程式を不動点（steady state）\\(k^*\\) のまわりで線形化（linearize）すると，線形化方程式 \\[ u_{21}^* \\cdot (k_{t-1} - k^*) + (u_{22}^* + \\beta u_{11}^*) \\cdot (k_t - k^*) + \\beta u_{12}^* \\cdot (k_{t+1} - k^*) = 0 \\] が得られる。ここでは\\(u_{12}^* = \\frac{\\partial^2 u}{\\partial k_t \\partial k_{t+1}}(k^*, k^*)\\) などと置いた。 この線形化方程式は \\((k_t, k_{t+1}) = (k^*, k^*)\\) の近傍の動学を近似する動学方程式である。 \\(\\hat k_t := k_t - k^*\\) と置き，\\(u_{12}^* \\neq 0\\) を仮定すると15， \\[ \\hat k_{t+1} + \\left(\\frac{u_{22}^* + \\beta u_{11}^*}{\\beta u_{12}^*} \\right) \\hat k_t + \\beta^{-1} \\hat k_{t-1} = 0 \\] このような漸化式（recurrence relation）あるいは差分方程式（difference equation）を解くには， 特性方程式（characteristic equation） \\[\\begin{equation} \\lambda^2 + \\left(\\frac{u_{22}^* + \\beta u_{11}^*}{\\beta u_{12}^*} \\right) \\lambda + \\beta^{-1} = 0 \\tag{2.3} \\end{equation}\\] を解けばよいのであった。 次の定理は Levhari and Liviatan (1972) と Santos (1991) による結果の1セクター版である。 定理 2.3 方程式 (2.3) の根はともに実数である。 証明. 2次方程式の根は2つしかないから，\\(\\lambda_1\\), \\(\\lambda_2\\) は互いに共役（\\(\\bar \\lambda_1 = \\lambda_2\\)）であるか， いずれも実数（\\(\\lambda_1, \\lambda_2 \\in \\mathbb R\\)）でなければならない。しかし，簡単に確かめられるように方程式 (2.3) の根 \\(\\lambda \\neq 0\\) が1つ見つかったとき，\\(\\beta^{-1} \\lambda^{-1}\\) もまた方程式 (2.3) の根である。 仮に \\(\\lambda\\) が複素根であるとすれば，\\(\\bar \\lambda\\) も根であるから，\\(\\bar \\lambda = \\beta^{-1} \\lambda^{-1}\\) が成り立たなければ根が4つになってしまうので不合理である。\\(\\bar \\lambda = \\beta^{-1} \\lambda^{-1}\\) として計算を進めると方程式 (2.3) は次の方程式と同値であることが分かる（確認せよ）。 \\[ \\left(\\lambda + \\frac{u_{12}^*}{u_{22}^*} \\right) \\overline{\\left(\\lambda + \\frac{u_{12}^*}{u_{22}^*} \\right)} + \\frac{u_{11}^* u_{22}^* - (u_{12}^*)^2}{(u_{22})^2} = 0. \\] 第1項は複素数の絶対値と同じ形式であるから正であり，第2項は，\\(U&#39;&gt;0\\), \\(U&#39;&#39;&gt;0\\) および \\(f&#39;&#39;&lt;0\\)の仮定により正である。 したがって，この方程式は成立しえない。つまり，\\(\\lambda\\) は実数でなければならない。 以上が 1セクターのラムゼーモデルで複素数に出会わない理由である。 参考文献 "],
["matrix.html", "第3章 行列論の復習 3.1 行列の形 3.2 行列の演算 3.3 線形方程式 3.4 R コード", " 第3章 行列論の復習 本章と次章では学部初年次の入門的な線形代数学で学ぶ重要項目のうち関連するものを復習する。経済動学の学習に最低限必要な項目を取り上げているが，十分に網羅的になっている訳ではないので， 線形代数学の成書を1つ手元に置いておくのがよいと思う. 本章では行列の形態にまつわる名称や基本演算および初等変形を定義する。次章以降で明らかになるように，線形動的システムの係数行列に対して初等変形を施すことで， 同等な解を持ち分析が容易な動的システムを導出できる。安定性分析の基礎となる重要な 3.1 行列の形 システムの振る舞いをよりよく理解するためには抽象的な線形空間論まで踏み込む必要があるが，それは次回以降に譲って，ここでは表形式に数を並べたものとして行列 (matrix) を捉えよう。従って，行列とは次のような対象である。 \\[ A = \\begin{bmatrix} a_{1,1} &amp; a_{1,2} &amp; \\cdots &amp; a_{1,n-1} &amp; a_{1,n} \\\\ a_{2,1} &amp; a_{2,2} &amp; \\cdots &amp; a_{2,n-1} &amp; a_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ a_{m-1,1} &amp; a_{m-1,2} &amp; \\cdots &amp; a_{m-1,n-1} &amp; a_{m-1,n}\\\\ a_{m,1} &amp; a_{m,2} &amp; \\cdots &amp; a_{m-1,n} &amp; a_{m,n} \\end{bmatrix} \\] ただし，\\(a_{i,j}\\in\\mathbb{F}\\)，\\(i=1,\\dots,m\\)，\\(j=1,\\dots,n\\)。\\(\\mathbb{F}\\) は \\(\\mathbb{R}\\) または \\(\\mathbb{C}\\)。数 \\(a_{i,j}\\) を行列の成分または要素 (element，component，entry) と呼ぶ。混乱の恐れがない場合はコンマを外して \\(a_{ij}\\) と書くことが多い。成分を明らかにするための簡略表記として \\(A=[a_{ij}]\\) といった書き方をする場合がある。また，行列 \\(A\\) の \\((i,j)\\) 成分 \\(a_{ij}\\) を\\(A_{ij}\\) のように書くこともある. 各 \\[ \\begin{aligned} &amp;\\begin{bmatrix}a_{1,1} &amp; a_{1,2} &amp; \\cdots &amp; a_{1,n-1} &amp; a_{1,n}\\end{bmatrix}\\\\ &amp;\\begin{bmatrix}a_{2,1} &amp; a_{2,2} &amp; \\cdots &amp; a_{2,n-1} &amp; a_{2,n}\\end{bmatrix}\\\\ &amp;\\hspace{6em}\\vdots\\\\ &amp;\\begin{bmatrix}a_{m,1} &amp; a_{m,2} &amp; \\cdots &amp; a_{m,n-1} &amp; a_{m,n}\\end{bmatrix} \\end{aligned} \\] を行列の行 (row) という。一方，各 \\[ \\begin{bmatrix} a_{1,1}\\\\ a_{2,1}\\\\ \\vdots\\\\ a_{m-1,1}\\\\ a_{m,1} \\end{bmatrix}, \\ \\begin{bmatrix} a_{1,2}\\\\ a_{2,2}\\\\ \\vdots\\\\ a_{m-1,2}\\\\ a_{m,2} \\end{bmatrix}, \\ \\dots,\\ \\begin{bmatrix} a_{1,n}\\\\ a_{2,n}\\\\ \\vdots\\\\ a_{m-1,n}\\\\ a_{m,n} \\end{bmatrix} \\] を行列の列 (column) という。上の行列\\(A\\) は\\(m\\)個の行と\\(n\\)個の列を持つので，\\(m\\times n\\) 行列と呼ばれる。\\(\\mathbb{F}^{m\\times n}\\) を \\(\\mathbb{F}\\) の値を成分にもつ \\(m\\times n\\) 行列の全体の集合と定義する。 行列の形にまつわるいくつかの用語を確認しておこう。 正方行列 (square matrix) \\(m=n\\) のとき，すなわち \\(A\\in\\mathbb{F}^{n\\times n}\\) のとき，\\(A\\) は \\(n\\) 次の正方行列 (square matrix of order \\(n\\)) であるいう。 ゼロ行列 (zero matrix，null matrix) すべての成分がゼロである行列をゼロ行列という。サイズが \\(m\\times n\\) であるゼロ行列を \\(0_{m\\times n}\\) とか \\(O_{m\\times n}\\) と書く。多くの場合にそうであるように，混乱の恐れがない場合には \\(O\\) とか \\(0\\) と書く。 対角成分 (diagonal elements) 正方行列 \\(A\\) の成分 \\(\\{a_{ij}\\ |\\ i,j=1,\\dots,n\\}\\) のうち \\(i=j\\) なる部分 \\(\\{a_{11},\\dots,a_{nn}\\}\\) を対角成分 (diagonal element)という。対角成分の1つ上の成分 \\(\\{a_{12},a_{23},\\dots,a_{n-1,n}\\}\\) を優対角成分 (superdiagonal element)，対角成分の1つ下の成分 \\(\\{a_{21},a_{32},\\dots,a_{n,n-1}\\}\\) を劣対角成分 (subdiagonal element) という。対角成分の和をトレース (trace) といい，\\(\\mathrm{trace}A\\) と書く. 三角行列 (triangular matrix) 正方行列\\(A=[a_{ij}]\\) が \\(i&gt;j\\Rightarrow a_{ij}=0\\) を満たすとき，\\(A\\) を上三角行列 (upper triangular matrix) という。一方，\\(A=[a_{ij}]\\) が \\(i&lt;j\\Rightarrow a_{ij}=0\\) を満たすとき，\\(A\\) は下三角行列 (lower triangular matrix) であるという。上三角行列は対角成分より下にある成分がすべてゼロ，下三角行列は対角成分より上にある成分がすべてゼロである。 対角行列 (diagonal matrix) 対角成分を除いた成分がすべてゼロであるような正方行列を対角行列 (diagonal matrix)という。ときに，\\(\\mathrm{diag}\\{a_{1},\\dots,a_{n}\\}\\) のように書いて対角成分が左上から順に \\(a_{1},\\dots,a_{n}\\) である対角行列を表すことがある。これは Matlab などで利用されている記法で，R でも類似の方法（diag(c(1, 2, 3))）で対角行列を定義できる。 単位行列 (identity matrix) 対角成分がすべて\\(1\\) である対角行列を単位行列という。\\(n\\)次の単位行列を \\(I_{n}\\) と書く。誤解の恐れがない場合は単に \\(I\\) と書く。R では diag(n) で \\(n\\) 次単位行列を作る。 転置行列 (transpose matrix) \\(A=[a_{ij}]\\in\\mathbb{F}^{m\\times n}\\) の転置行列 \\(A^{\\top}\\) とは，\\((A^{\\top})_{ij}=a_{ji}\\), \\(i=1,\\dots,m\\)，\\(j=1,\\dots n\\) を満たす\\(m\\times n\\)行列のことをいう。R では t(A) で行列 A の転置行列を得る。 共役転置行列 (conjugate transpose) \\(A=[a_{ij}]\\in\\mathbb{F}^{m\\times n}\\) の共役転置行列 \\(A^{*}\\) とは，\\((A^{*})_{ij}=\\bar{a}_{ji}\\), \\(i=1,\\dots,m\\)，\\(j=1,\\dots n\\) を満たす\\(m\\times n\\)行列のことである。実行列の共役転置行列は転置行列である。 R では Conj(t(A)) 関数で行列 A の転置行列を得る。 対称行列 (symmetric matrix) \\(A\\in\\mathbb{R}^{n\\times n}\\) が対称行列 (symmetric matrix)であるとは，\\(A^{\\top}=A\\) が成り立つことをいう。 エルミート行列 (Hermitian matrix) \\(A\\in\\mathbb{C}^{n\\times n}\\) がエルミート行列であるとは，\\(A^{*}=A\\) が成り立つことをいう。 列ベクトル・行ベクトル (column vector，row vector) のちに見るように行列の集合には元どうしの加法とスカラー倍が定義され，それらは望ましい性質を満たす。ゼロ元・逆元と呼ばれる特別な元の存在も自明であるので，特定のサイズの行列全体の集合は，(のちに定義する) ベクトル空間の一例となっている。特に \\[ \\mathbb{F}^{n\\times1},\\quad\\mathbb{F}^{1\\times n} \\] は我々が頻繁に用いる有限次元ベクトル空間の表現となっている。通常これらのうちいずれかを \\(\\mathbb{F}^{n}\\) と記す. 一般の \\(A\\in\\mathbb{F}^{n\\times m}\\) を\\(n\\)次元列ベクトル (\\(\\mathbb{F}^{n\\times1})\\) を \\(m\\) 個並べたものと捉えたり，\\(m\\)次元列ベクトル (\\(\\mathbb{F}^{1\\times m})\\) を \\(n\\) 個並べたものと捉えたりすることがある. ブロック行列 (block matrix) 行列をいくつかの部分行列に分解した上で分析する方が都合の良い場合がある。例えば, \\[ A=\\left[ \\begin{array}{ccc|ccc} a_{1,1} &amp; \\cdots &amp; a_{1,n} &amp; a_{1,n+1} &amp; \\cdots &amp; a_{1,n+q}\\\\ \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{m,1} &amp; \\cdots &amp; a_{m,n} &amp; a_{m,n+1} &amp; \\cdots &amp; a_{m,n+q}\\\\ \\hline a_{m+1,1} &amp; \\cdots &amp; a_{m+1,n} &amp; a_{m+1,n+1} &amp; \\cdots &amp; a_{m+1,n+q}\\\\ \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{m+p,1} &amp; \\cdots &amp; a_{m+p,n} &amp; a_{m+p,n+1} &amp; \\cdots &amp; a_{m+p,n+q} \\end{array} \\right] =: \\left[ \\begin{array}{c|c} A_{11} &amp; A_{12}\\\\ \\hline A_{21} &amp; A_{22} \\end{array} \\right] \\] のように，大きな行列 \\(A\\in\\mathbb{F}^{(m+p)\\times(n+q)}\\) を4つの部分行列 \\(A_{11}\\in\\mathbb{F}^{m\\times n}\\), \\(A_{12}\\in\\mathbb{F}^{m\\times q}\\)，\\(A_{21}\\in\\mathbb{F}^{p\\times n}\\), \\(A_{22}\\in\\mathbb{F}^{p\\times q}\\) に分解するなどである.16 \\(A_{12}=0\\)，\\(A_{21}=0\\) であるとき，ブロック対角行列 (block diagonal matrix)であるといい，\\(A_{21}=0\\) であるときブロック上三角行列 (block upper triangular matrix)であるなどという。 3.2 行列の演算 スカラー倍 \\(A\\in\\mathbb{F}^{m\\times n}\\) に対してスカラー倍 (scalar multiplication) あるいは定数倍と呼ばれる操作が次のように定義される: \\(\\alpha\\in\\mathbb{F}\\) について, \\[ \\alpha A:=\\begin{bmatrix}\\alpha a_{1,1} &amp; \\alpha a_{1,2} &amp; \\cdots &amp; \\alpha a_{1,n-1} &amp; \\alpha a_{1,n}\\\\ \\alpha a_{2,1} &amp; \\alpha a_{2,2} &amp; \\cdots &amp; \\alpha a_{2,n-1} &amp; \\alpha a_{2,n}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots\\\\ \\alpha a_{m-1,1} &amp; \\alpha a_{m-1,2} &amp; \\cdots &amp; \\alpha a_{m-1,n-1} &amp; \\alpha a_{m-1,n}\\\\ \\alpha a_{m,1} &amp; \\alpha a_{m,2} &amp; \\cdots &amp; \\alpha a_{m-1,n} &amp; \\alpha a_{m,n} \\end{bmatrix}\\in\\mathbb{F}^{m\\times n}. \\] 和 同数の行と列を持つ2つの行列に対して次のようにして和が定義できる。\\(A=[a_{ij}]\\in\\mathbb{F}^{m\\times n}\\), \\(B=[b_{ij}]\\in\\mathbb{F}^{m\\times n}\\) に対して， \\[ A+B:=[a_{ij}+b_{ij}]\\in\\mathbb{F}^{m\\times n}. \\] すなわち，行列の和は成分ごとに和をとった行列である。定義から自明なことであるが，和は交換法則と結合法則を満たす。すなわち, 任意の \\(A,B,C\\in\\mathbb{F}^{m\\times n}\\) について \\[ \\begin{aligned} A+B &amp; =B+A\\\\ A+(B+C) &amp; =(A+B)+C \\end{aligned} \\] が成り立つ。ゼロ行列 \\(0\\in\\mathbb{F}^{m\\times n}\\) は任意の \\(A\\in\\mathbb{F}^{m\\times n}\\)に対して \\[ A+O_{m\\times n}=A \\] を満たす。 積 行列 \\(A=[a_{ij}]\\in\\mathbb{F}^{m\\times n}\\) と \\(B=[b_{ij}]\\in\\mathbb{F}^{n\\times p}\\) の積 \\(AB\\in\\mathbb{F}^{m\\times p}\\) を次のように定義する. \\[ AB:=\\left[\\sum_{k=1}^{n}a_{ik}b_{kj}\\right]. \\] この定義は線形写像の合成という観点から見ればごく自然なものであることが分かる。解説は第5章で行う。 正方行列 \\(A,B\\in\\mathbb{F}^{n\\times n}\\) に対して，\\(AB\\) と \\(BA\\) の両方が定義される。しかし，それらは一般には一致しない。例えば， \\[ \\begin{aligned} \\begin{bmatrix} 1 &amp; 1\\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0\\\\ 1 &amp; 1 \\end{bmatrix} &amp; \\neq \\begin{bmatrix} 1 &amp; 0\\\\ 1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1\\\\ 0 &amp; 1 \\end{bmatrix} \\end{aligned} \\] である。\\(AB=BA\\) が成り立つとき，\\(A\\) と \\(B\\) は可換であるという。単位行列とゼロ行列は任意の行列と可換である. 任意の \\(A\\in\\mathbb{F}^{n\\times n}\\) に対して \\[ AI_{n}=I_{n}A=A \\] と \\[ AO_{n\\times n}=O_{n\\times n}A=O_{n\\times n} \\] が成り立つ。 行列の積には次の性質がある。 \\[ \\begin{aligned} (AB)C &amp; =A(BC)\\\\ A(B+C) &amp; =AB+AC\\\\ (A+B)C &amp; =AC+BC. \\end{aligned} \\] スカラー\\(\\alpha\\)に対して， \\[ (\\alpha A)B=A(\\alpha B)=\\alpha(AB). \\] 逆行列 (inverse matrix) 正方行列 \\(A\\in\\mathbb{F}^{n\\times n}\\) に対して， \\[ AB=BA=I_{n} \\] なる \\(B\\in\\mathbb{F}^{n\\times n}\\) が存在するとき \\(A\\) は可逆 (invertible) あるいは正則 (regular) であるという。\\(B\\) を \\(A\\) の逆行列 (inverse matrix) といい \\(A^{-1}\\) と記す。 逆行列は存在すれば一意的に定まることを証明せよ. 逆行列は存在すれば正則であることを証明せよ。 \\(A,B\\in\\mathbb{F}^{n\\times n}\\) がともに正則であるとき，\\(AB\\) は可逆であり，\\((AB)^{-1}=B^{-1}A^{-1}\\) が成り立つことを示せ。 直交行列 (diagonal matrix) \\(A\\in\\mathbb{R}^{n\\times n}\\) が直交行列であるとは，\\(A^{\\top}=A^{-1}\\) が成り立つことをいう。 ユニタリ行列 (unitary matrix) \\(A\\in\\mathbb{C}^{n\\times n}\\) がユニタリ行列であるとは，\\(A^{*}=A^{-1}\\) が成り立つことをいう。 3.3 線形方程式 行列とベクトルの組 \\[ \\begin{aligned} A &amp; =[a_{ij}]\\in\\mathbb{F}^{m\\times n},\\\\ b &amp; =[b_{j}]\\in\\mathbb{F}^{m\\times1} \\end{aligned} \\] に対して， \\[ Ax=b \\] を満たす \\(x\\in\\mathbb{F}^{n\\times1}\\) を求める問題を線形方程式 (linear equation)という。\\(Ax\\) は行列\\(A\\)と列ベクトル \\(x\\) の積である。これは次の連立1次方程式の行列表現に外ならない。 \\[ \\begin{cases} a_{11}x_{1}+\\cdots+a_{1n}x_{n}=b_{1}\\\\ \\qquad\\vdots\\\\ a_{m1}x_{1}+\\cdots+a_{mn}x_{n}=b_{m} \\end{cases} \\] 3.3.1 線形連立方程式の変形 要点を理解するために簡単な例を用いよう。連立方程式 \\[\\begin{equation} \\begin{cases} x_{1}+x_{2}=1\\\\ x_{1}-x_{2}=2 \\end{cases} \\tag{3.1} \\end{equation}\\] を行列の形式で表現すると \\[\\begin{equation} \\begin{bmatrix} 1 &amp; 1\\\\ 1 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_{1}\\\\x_{2} \\end{bmatrix} = \\begin{bmatrix} 1\\\\2 \\end{bmatrix} \\tag{3.2} \\end{equation}\\] と表すことができる。式(3.1)と全く同じ連立方程式で，順序だけを入れ替えたもの \\[\\begin{equation} \\begin{cases} x_{1}-x_{2}=2\\\\ x_{1}+x_{2}=1 \\end{cases} \\tag{3.3} \\end{equation}\\] を行列表示すると， \\[\\begin{equation} \\begin{bmatrix}1 &amp; -1\\\\ 1 &amp; 1 \\end{bmatrix}\\begin{bmatrix}x_{1}\\\\ x_{2} \\end{bmatrix}=\\begin{bmatrix}2\\\\ 1 \\end{bmatrix} \\tag{3.4} \\end{equation}\\] である。式(3.2)と式(3.4)は，全く同じ \\((x_{1},x_{2})\\) が解であるという意味で同値な方程式であるが，係数行列 \\(A\\) と定数項 \\(b\\) が異なっている。この他にも，式(3.1)の第1式を定数倍した \\[\\begin{equation} \\begin{cases} 2x_{1}-2x_{2}=4\\\\ x_{1}+x_{2}=1 \\end{cases} \\tag{3.5} \\end{equation}\\] や，式(3.1) の第1式を第2式に足すことで得られる \\[\\begin{equation} \\begin{cases} x_{1}-x_{2}=2\\\\ x_{1}+x_{2}+(x_{1}-x_{2})=1+2 \\end{cases} \\tag{3.6} \\end{equation}\\] も同じ解をもつはずである。各自，式(3.5)，式(3.6) の行列表現を確認してほしい。 あるいは次の方程式 \\[ \\begin{cases} x_{2}+x_{1}=1\\\\ -x_{2}+x_{1}=2 \\end{cases} \\] も全く同じ方程式を変形したものなので，その行列表示 \\[\\begin{equation} \\begin{bmatrix}1 &amp; 1\\\\ -1 &amp; 1 \\end{bmatrix}\\begin{bmatrix}x_{2}\\\\ x_{1} \\end{bmatrix}=\\begin{bmatrix}1\\\\ 2 \\end{bmatrix}\\tag{3.7} \\end{equation}\\] も，\\(x_{1},x_{2}\\) の順序は入れ替わるが同じ解を導く。従って，\\((x_{1},x_{2})\\) でなく \\((x_{2},x_{1})\\) の順で解を得たことさえ了解していれば，式(3.7)と式(3.4) は本質的に同じ線形方程式である. 見かけ上異なる複数の行列が連立方程式の解という観点から見れば全て同じものになるという事実は応用上大変重要である。連立方程式や動学方程式を分析する上では, もっとも有利な形式に変形してから分析すれば十分なのである。特に，あらかじめ都合のよい形式に変形されているものとして理論分析を行うこともあるので, 応用者は自らそのような形式に変形し，さらに復元できなければいけない。 連立方程式の求解に関していえば，同値な変形を繰り返して \\[ \\begin{cases} x_{1}=*\\\\ x_{2}=* \\end{cases} \\] 形式を導けばよい。行列表示すると \\[ \\begin{bmatrix} 1 &amp; 0\\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x_{1}\\\\ x_{2} \\end{bmatrix}=\\begin{bmatrix}*\\\\ * \\end{bmatrix} \\] を導けばよい。 3.3.2 行基本変形 連立方程式の変形を行列の言葉で表現してみよう。 行の交換 行列の行に対するもっとも基本的な操作は行の交換であろう。連立方程式は順序付けされてない方程式の組であり，これに無理やり順序付けたものが先ほどの行列表示に他ならないので, 行順序の変更に対して解は不変である。 \\(2\\times2\\) 行列の行順序の変更は行列 \\(\\left[\\begin{smallmatrix}0 &amp; 1\\\\ 1 &amp; 0 \\end{smallmatrix}\\right]\\)を左から掛ける操作に対応する。 \\[ \\begin{aligned} \\begin{bmatrix}0 &amp; 1\\\\ 1 &amp; 0 \\end{bmatrix}\\begin{bmatrix}1 &amp; 1\\\\ 1 &amp; -1 \\end{bmatrix}\\begin{bmatrix}x_{1}\\\\ x_{2} \\end{bmatrix} &amp; =\\begin{bmatrix}1 &amp; -1\\\\ 1 &amp; 1 \\end{bmatrix}\\begin{bmatrix}x_{1}\\\\ x_{2} \\end{bmatrix}\\\\ \\begin{bmatrix}0 &amp; 1\\\\ 1 &amp; 0 \\end{bmatrix}\\begin{bmatrix}1\\\\ 2 \\end{bmatrix} &amp; =\\begin{bmatrix}2\\\\ 1 \\end{bmatrix}. \\end{aligned} \\] 次の関係が成り立つことに注目してほしい. \\[ \\begin{bmatrix} 0 &amp; 1\\\\ 1 &amp; 0 \\end{bmatrix}^{-1} = \\begin{bmatrix} 0 &amp; 1\\\\ 1 &amp; 0 \\end{bmatrix}. \\] 従って，式(3.2) と式(3.4)は正則行列 \\(\\left[\\begin{smallmatrix}0 &amp; 1\\\\ 1 &amp; 0 \\end{smallmatrix}\\right]\\) を通して互いに変形し合う。 より一般の \\(m\\times n\\) 行列についてどのようになるか考えてみよ。 A\\(\\in\\mathbb{F}^{m\\times n}\\) を任意の行列とする。行列の第\\(i\\)行と第\\(j\\)行を入れ替えた結果が行列積 \\(C_{ij}^{m}A\\) で表せるような行列 \\(C_{ij}^{m}\\in\\mathbb{F}^{m\\times m}\\) が存在する。 \\(C_{ij}^{m}\\) の要素をかきだしなさい。 行全体の非ゼロ定数倍 1つの行にゼロでない定数 \\(u\\) をかける操作も行列積を用いて表現できる。ここでも簡単な場合だけ見ておこう。 \\[ \\begin{bmatrix}1 &amp; 0\\\\ 0 &amp; u \\end{bmatrix}\\begin{bmatrix}1 &amp; 1\\\\ 1 &amp; -1 \\end{bmatrix}=\\begin{bmatrix}1 &amp; 1\\\\ u &amp; -u \\end{bmatrix}. \\] 次の関係が成り立つことに注目してほしい. \\[ \\begin{bmatrix}1 &amp; 0\\\\ 0 &amp; u \\end{bmatrix}^{-1}=\\begin{bmatrix}1 &amp; 0\\\\ 0 &amp; 1/u \\end{bmatrix}. \\] \\(A\\in\\mathbb{F}^{m\\times n}\\) を任意の行列とする。行列の第\\(i\\)行に一斉に非ゼロ定数 \\(u\\) をかけた結果が行列積 \\(D_{i}^{m}(u)A\\) で表せるような行列 \\(D_{i}^{m}(u)\\in\\mathbb{F}^{m\\times m}\\) が存在する。\\(D_{i}^{m}\\) の要素をかきだしなさい。 行の定数倍を別の行に加える 1つの行に(ゼロであってもよい)定数 \\(a\\) をかける操作も行列積を用いて表現できる。簡単な場合だけ見ておこう。 \\[ \\begin{bmatrix}1 &amp; a\\\\ 0 &amp; 1 \\end{bmatrix}\\begin{bmatrix}1 &amp; 1\\\\ 1 &amp; -1 \\end{bmatrix}=\\begin{bmatrix}1+a &amp; 1-a\\\\ 1 &amp; -1 \\end{bmatrix}. \\] 次の関係が成り立つことに注目してほしい。 \\[ \\begin{bmatrix}1 &amp; a\\\\ 0 &amp; 1 \\end{bmatrix}^{-1}=\\begin{bmatrix}1 &amp; -a\\\\ 0 &amp; 1 \\end{bmatrix}. \\] 一般の場合は練習問題とする。 \\(A\\in\\mathbb{F}^{m\\times n}\\) を任意の行列とする。行列の第\\(i\\)行の \\(a\\)倍を第\\(j\\)行に加えた結果が行列積 \\(E_{ij}^{m}(a)A\\) で表せるような行列 \\(E_{ij}^{m}(a)\\in\\mathbb{F}^{m\\times m}\\) が存在する。\\(E_{ij}^{m}(a)\\) の要素をかきだしなさい。 3.3.3 列基本変形 行の変形と同様に，列の変形も定義できる。実は上で得られた行列を「右から」かける操作が列変形に対応している。各自確認しておいてほしい。 \\(A\\in\\mathbb{F}^{m\\times n}\\) を任意の行列とする。行列の第\\(i\\)列と第\\(j\\)列を入れ替えた行列は \\(AC_{ij}^{m}\\) と一致する。 \\(A\\in\\mathbb{F}^{m\\times n}\\) を任意の行列とする。行列の第\\(i\\)列に一斉に非ゼロ定数 \\(u\\) をかけた結果は \\(AD_{i}^{m}(u)\\) と一致する。 \\(A\\in\\mathbb{F}^{m\\times n}\\) を任意の行列とする。行列の第\\(i\\)列の \\(a\\)倍を第\\(j\\)列に加えた結果は \\(AE_{ij}^{m}(a)\\) と一致する。 列の変形は式(3.7)で見た「解の並び替え」を一般化したものである。線形方程式 \\[ Ax=b \\] は \\[ AC_{ij}^{m}\\left(C_{ij}^{m}\\right)^{-1}x=b \\] と同値であるから \\(y=\\left(C_{ij}^{m}\\right)^{-1}x\\) と置き換えれば \\[ AC_{ij}^{m}y=b \\] と同値である。この変換については座標変換と関連付けて今後より詳しく学ぶことになる。 3.3.4 初等変形 行列に行基本変形と列基本変形を繰り返して得られる操作を初等変形という。\\(A\\in\\mathbb{F}^{m\\times n}\\)，\\(i=1,\\dots,n_{r}\\)，\\(j=1,\\dots,n_{c}\\) について \\(P_{i}\\) をいずれかの行基本変形，\\(Q_{j}\\) をいずれかの列基本変形とすると \\[ P=P_{n_{r}}\\cdots P_{1},\\quad Q=Q_{1}\\cdots Q_{n_{c}} \\] を用いて初等変形 (の結果) を \\[ PAQ \\] と表すことができる。 \\(P,Q\\) が正則行列であることを示せ. 命題 3.1 任意の行列 \\(A\\in\\mathbb{F}^{m\\times n}\\) に対して，適当に初等変形 \\(P\\)，\\(Q\\) を選べば \\[ PAQ=\\left[\\begin{array}{c|c} I_{d\\times d} &amp; O_{d\\times(n-d)}\\\\ \\hline O_{(m-d)\\times d} &amp; O_{(m-d)\\times(n-d)} \\end{array}\\right] \\] とできる。ここで，\\(d\\) は\\(P,Q\\)の選び方によらず\\(A\\)のみから決まる定数である。 ランク 上の \\(d\\) を行列 \\(A\\) のランク (rank)といい，\\(\\mathrm{rank}A\\) と書く。 一般に \\(\\mathrm{rank}A\\le\\min\\{m,n\\}\\) である。この不等式が等号で成り立つとき，\\(A\\) はフルランク (full rank) であるという。特に，\\(\\mathrm{rank}A=m\\) のとき行フルランク (full column rank)，\\(\\mathrm{rank}A=n\\) のとき列フルランク (full row rank)という。 命題 3.2 行列 \\(A\\in\\mathbb{F}^{n\\times n}\\) が正則であることと，フルランクであることは同値である。 証明. \\(A\\) がフルランクであると仮定する。すなわち \\[ PAQ=I \\] なる初等変形 \\(P\\)，\\(Q\\) がある。左から \\(P^{-1}\\)，右から \\(Q^{-1}\\) をかけてやれば \\[\\begin{equation} A = P^{-1}Q^{-1} \\tag{3.8} \\end{equation}\\] が得られる。右辺は正則なので逆行列が存在する。すなわち \\(A^{-1}=QP\\). \\(A\\) は正則であるがフルランクでないと仮定する。すなわち，\\(d&lt;n\\) に対して \\[ PAQ=\\left[\\begin{array}{c|c} I_{d\\times d} &amp; O_{d\\times(n-d)}\\\\ \\hline O_{(m-d)\\times d} &amp; O_{(m-d)\\times(n-d)} \\end{array}\\right] \\] となる。逆行列 \\(A^{-1}\\) が存在するので，\\(PAQ\\) に右から \\(Q^{-1}A^{-1}\\) をかければ \\[ PAQ(Q^{-1}A^{-1})=P \\] を得る。\\(PAQ\\) の形状により \\(P\\) の \\((d+1)\\) 行目以下はすべてゼロにならなければならないが，このような行列を基本変形の積として表現することはできない. 従って，この等式は不合理である。よって\\(A\\)はフルランクでなければならない. 式(3.8) を少し変形すると \\[ A=P^{-1}Q^{-1}=\\tilde{P}^{-1} \\] なる行基本変形のみ（あるいは列基本変形のみ）からなる初等変形 \\(\\tilde{P}\\) が存在することが分かる。\\(A^{-1}=\\tilde{P}\\) であるから，\\(A\\) から \\(I\\) に至る行基本変形を \\(I\\) に施せば逆行列を得ることができる. 3.4 R コード 線形システムの分析において行列計算は中心的な役割を果たすので，繰り返しタイプして十分に手に馴染ませて欲しい。 3.4.1 行列の定義 定義したい行列の要素と同じ要素を持つアトミックベクタを定義してから，行列に変換するのがもっとも簡単である。 a &lt;- 1:9 (A &lt;- matrix(a, nrow = 3)) ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 列方向に並んでいくことを確認してほしい。この振る舞いは，オプションで変更することができる。タイプする文字が増えてしまうが，次のように定義する方がコードから内容が直感的に分かりやすいかもしれない。 b &lt;- c( 1.11, 3.12, 4.13, -3.21, 2.22, 5.23, 0.31, 1.32, 8.33) (B &lt;- matrix(b, nrow = 3, byrow = TRUE)) ## [,1] [,2] [,3] ## [1,] 1.11 3.12 4.13 ## [2,] -3.21 2.22 5.23 ## [3,] 0.31 1.32 8.33 特別な行列 ゼロ行列 matrix(0, 3, 3) ## [,1] [,2] [,3] ## [1,] 0 0 0 ## [2,] 0 0 0 ## [3,] 0 0 0 対角行列 d &lt;- c(1.11, 3.22, -1.33) diag(d) ## [,1] [,2] [,3] ## [1,] 1.11 0.00 0.00 ## [2,] 0.00 3.22 0.00 ## [3,] 0.00 0.00 -1.33 単位行列 diag(1, 4) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 3.4.2 要素の取得 行列の要素を取得するには，次のような構文を使う。 B[1, 2] ## [1] 3.12 ただし，このような要素の取得はできる限りなくす努力をするべきだ。 3.4.3 行列の演算・関数 スカラー倍 0.5 * A ## [,1] [,2] [,3] ## [1,] 0.5 2.0 3.5 ## [2,] 1.0 2.5 4.0 ## [3,] 1.5 3.0 4.5 和・差 A + B ## [,1] [,2] [,3] ## [1,] 2.11 7.12 11.13 ## [2,] -1.21 7.22 13.23 ## [3,] 3.31 7.32 17.33 積 行列の積は A * B と計算できると期待するかもしれないが，この演算は成分ごとの積を計算する。行列積には特別な演算子 %*% を使わなければならない。 A %*% B ## [,1] [,2] [,3] ## [1,] -9.56 21.24 83.36 ## [2,] -11.35 27.90 101.05 ## [3,] -13.14 34.56 118.74 転置 転置行列は, t() で計算できる。 t(B) ## [,1] [,2] [,3] ## [1,] 1.11 -3.21 0.31 ## [2,] 3.12 2.22 1.32 ## [3,] 4.13 5.23 8.33 複素数を成分に持つ行列は，共役転置行列が必要だろう。次のように計算する。 C &lt;- A + B * 1i Conj(t(C)) ## [,1] [,2] [,3] ## [1,] 1-1.11i 2+3.21i 3-0.31i ## [2,] 4-3.12i 5-2.22i 6-1.32i ## [3,] 7-4.13i 8-5.23i 9-8.33i 逆行列 実用上利用するかどうかは別として，逆行列は次のように計算できる。 solve(B) ## [,1] [,2] [,3] ## [1,] 0.14306178 -0.25353377 0.08825168 ## [2,] 0.35010078 0.09833723 -0.23532052 ## [3,] -0.06080218 -0.00614762 0.15405343 多くの場合，\\(B^{-1} A\\) を計算する次の形式で使うことになる。 solve(B, A) ## [,1] [,2] [,3] ## [1,] -0.09925073 -0.1659117 -0.2325726 ## [2,] -0.15918632 0.4801661 1.1195186 ## [3,] 0.38906287 0.6503737 0.9116846 すなわち，やや冗長に書けば，solve(B) は次のコードと同一である。 solve(B, diag(3)) ## [,1] [,2] [,3] ## [1,] 0.14306178 -0.25353377 0.08825168 ## [2,] 0.35010078 0.09833723 -0.23532052 ## [3,] -0.06080218 -0.00614762 0.15405343 ランク ランクの計算は四則演算からなる基本変形をほどこしてゼロでない行を数えればよい。簡単な計算だと思うかもしれないが，特定の数字が0であるかどうかを判定することは計算機には難しい。次の行列のランクはもちろんゼロである。 \\[ M = \\begin{bmatrix} 0.1 + 0.1 + 0.1 - 0.3 &amp; 0 \\\\ 0 &amp; 0\\\\ \\end{bmatrix} \\] しかし，数値計算で判定することは容易でない。 M = matrix(c(0.1 + 0.1 + 0.1 - 0.3, 0, 0, 0), nrow = 2) qr(M)$rank ## [1] 1 \\(P:=Q\\) という表現は，\\(P\\) を \\(Q\\) で定義するという意味，\\(P =:Q\\) は \\(Q\\) を\\(P\\) で定義するという意味である.↩ "],
["eigen.html", "第4章 行列の固有値 4.1 行列式 4.2 固有値と固有ベクトル 4.3 対角化 4.4 線形動学方程式と固有値 4.5 ラムゼーモデルと固有値 4.6 R コード", " 第4章 行列の固有値 本章では，行列の固有値に関して復習する。 4.1 行列式 4.1.1 置換 有限個の自然数の集合 \\(\\{1,2,\\dots,n-1,n\\}\\) を並び替える方法には \\(n!\\) 通りの方法がある。この並び替え全体の集合を \\(S_n\\) で表す。例えば \\(\\sigma&#39;=(1,2,\\dots,n-1,n)\\) や \\(\\sigma&#39;&#39;=(n,n-1,\\dots,2,1)\\) などが\\(S_{n}\\) の元である。\\(S_{n}\\) の元を置換 (permutation)と呼ぶ。置換 \\(\\sigma\\in S_{n}\\) を \\(\\{1,2,\\dots,n\\}\\) からそれ自身への全単射と考えて，\\(\\sigma&#39;(1)=1\\)，\\(\\sigma&#39;&#39;(n-1)=2\\) などのように書くこともできる. この記法は置換の合成 (通常，「積」と呼ばれる) の自然な定義を導いてくれる。すなわち \\[ (\\sigma_{1}\\sigma_{2})(i)=\\sigma_{1}(\\sigma_{2}(i)),\\quad i=1,2,\\dots,n. \\] 恒等置換 (identity permutation)とは，\\(\\sigma_{id}(i)=i\\)，\\(i=1,\\dots,n\\) なる置換である。互換 (transposition)とは，2つの文字を入れ替える特別な置換である。すなわち \\(i\\neq j\\) に対して \\[ \\pi_{ij}(i)=j,\\quad\\pi_{ij}(j)=i,\\quad\\pi_{ij}(k)=k,\\quad k\\neq i,j. \\] すべての置換は互換の積として表すことができる。例えば，\\(\\sigma=(2,3,4,1)\\) とすれば \\[ (1,2,3,4)\\xrightarrow{\\pi_{1,2}}(2,1,3,4)\\xrightarrow{\\pi_{1,3}}(2,3,1,4)\\xrightarrow{\\pi_{1,4}}(2,3,4,1) \\] なので，\\(\\sigma=\\pi_{1,4}\\pi_{1,3}\\pi_{1,2}\\) となる。この分解の方法は一意的ではないが，分解を構成する互換の数は奇遇が不変となることが知られている。奇数個の互換の積に分解できる置換を奇置換 (odd permutation), 偶数個の互換の積に分解できる置換を偶置換 (even permutation) と呼ぶ。写像 \\(\\mathrm{sgn}:S_{n}\\to\\{-1,1\\}\\) を次のように定義できる. \\[ \\mathrm{sgn}(\\sigma)=\\begin{cases} -1 &amp; \\mbox{if }\\sigma\\mbox{ is odd}\\\\ +1 &amp; \\mbox{if }\\sigma\\mbox{ is even}. \\end{cases} \\] 4.1.2 行列式の定義 正方行列 \\(A=[a_{ij}]\\in\\mathbb{F}^{n\\times n}\\) に対して，行列式 (determinant) \\(\\det A\\) (\\(|A|\\)とも書く) を次のように定義する。 \\[ \\det A=\\sum_{\\sigma\\in S_{n}}\\mathrm{sgn}(\\sigma)a_{1\\sigma(1)}\\cdots a_{n\\sigma(n)}. \\] 命題 4.1 行列式は次の性質を持つ. \\(\\det A^{\\top}=\\det A\\). \\(\\det C_{ij}A=-\\det A\\) (行の交換で符号が変わる). \\(\\det D_{i}^{n}(a)A=a\\det A\\) (行の\\(a\\)倍で行列式が\\(a\\)倍になる。\\(a=0\\) でも成り立つ)。 \\(\\det\\left(E_{ij}^{n}(a)A\\right)=\\det A\\) (行の\\(a\\)倍を別の行に加えても行列式は変化しない). \\[ \\det\\begin{bmatrix} a_{11} &amp; \\cdots &amp; a_{1n}\\\\ a_{i1}+b_{i1} &amp; \\cdots &amp; a_{in}+b_{in}\\\\ a_{n1} &amp; \\cdots &amp; a_{nn} \\end{bmatrix}=\\det\\begin{bmatrix} a_{11} &amp; \\cdots &amp; a_{1n}\\\\ a_{i1} &amp; \\cdots &amp; a_{in}\\\\ a_{n1} &amp; \\cdots &amp; a_{nn} \\end{bmatrix}+\\det\\begin{bmatrix} a_{11} &amp; \\cdots &amp; a_{1n}\\\\ b_{i1} &amp; \\cdots &amp; b_{in}\\\\ a_{n1} &amp; \\cdots &amp; a_{nn} \\end{bmatrix}. \\] \\(\\det I=1\\). \\(\\det AB=\\det A\\det B\\). 命題 4.1 を証明せよ. \\(A \\in \\mathbb{F}^{n \\times n}\\) が正則であるための必要十分条件は \\(\\det A \\neq 0\\)。 証明. \\(A\\) が正則であるとしよう。このとき，命題 4.1 性質6，7 より， \\(1 = \\det I = \\det A \\det A^{-1}\\) が成り立つから \\(\\det A = 0\\) にはなりえない。 次に，\\(\\det A \\neq 0\\) が成り立つとして，\\(A\\) の正則性を導こう。対偶を示す17。 \\(A\\)が正則でければ \\(A\\)はフルランクでない（命題3.2）。 このとき正則行列 \\(P, Q\\) が存在して， \\[ PAQ=\\left[ \\begin{array}{c|c} I_{d\\times d} &amp; O_{d\\times(n-d)}\\\\ \\hline O_{(m-d)\\times d} &amp; O_{(m-d)\\times(n-d)} \\end{array} \\right] \\] とできる。右辺の行列式は定義によりゼロである。 命題 4.1 性質7 により，\\(\\det P \\det A \\det Q = 0\\)。 正則行列 \\(P, Q\\) に対しては \\(\\det P \\neq 0 \\neq \\det Q\\) が成り立つことが分かっているから， \\(\\det A = 0\\) が成り立たなければならない。 4.1.3 行列式の意味 2次正方行列 \\[ A = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\] の行列式は定義に従って容易に計算することができる。 \\[ \\det A = ad - bc \\] この量は，行列\\(A\\)の列を成す2つのベクトル \\[ A_1 = \\begin{bmatrix} a \\\\ c \\end{bmatrix}, \\quad A_2 = \\begin{bmatrix} b \\\\ d \\end{bmatrix} \\] が作る平行四辺形 (parallelogram) の（符号付き）面積と一致する（図4.1の色付き部分）。 図 4.1: 列ベクトルが成す平行四辺形 一般の次数でも同様の考え方が適用できる。例えば，3次正方行列の行列式は，列ベクトルが作る平行6面体（parallelepiped）の符号付き体積と一致する（図4.2）。 図 4.2: 列ベクトルが成す平行六面体 このことから分かることは，行列を構成する列に同じ向きを持つ（平行な）ベクトルが含まれていると行列式は必ずゼロになるということである。 図 4.3: 平行な列ベクトル 2本のベクトル\\(v_1 \\neq 0 \\neq v_2\\) が面積を持つ平行四辺形を作るための必要十分条件は， \\(v_1 = \\alpha v_2\\) となるような, \\(\\alpha\\) が存在しないことである（図4.3）。 ゼロと異なる3本のベクトル \\(v_1, v_2, v_3\\) が体積を持つ平行六面体を作るための必要十分条件は， \\(v_1 = \\alpha v_2 + \\beta v_3\\) かつ \\(\\alpha\\) と \\(\\beta\\) がないというものである。 いずれかのベクトルがゼロである可能性も考慮して，次のような概念を定義すると便利である。 定義 4.1 \\(n\\) 本のベクトル \\(v_1, \\dots, v_n\\) が1次独立（linearly independent）であるとは, \\(\\alpha_{1},\\dots,\\alpha_{n}\\) に対して \\(\\alpha_{1}v_{1}+ \\cdots + \\alpha_{n}v_{n}=0\\) が成り立てば，\\(\\alpha_{1}=\\cdots=\\alpha_{n}=0\\) が成り立つことをいう。 1次独立でないとき，ベクトルの組は 1次従属（linearly dependent）であるという。 行列を構成する列ベクトルが1次独立でなければ，列基本変形によりゼロのみからなる列を構成することができる。したがって，そのような行列の行列式はゼロでなければならない。逆に，行列が1次独立な列をもつならば（後に示すように），その行列はフルランクである。したがって，行列式はゼロでない。 これから先，正方行列の正則性は列ベクトルの1次独立性，行列式がゼロでないこと，行列のランクが次数と一致することなど同値な条件が形を変えて繰り返し現れることになる。これらの関係を理解しておくと動学理論を見通しよく習得できる。 4.2 固有値と固有ベクトル 複素数 \\(\\lambda\\) が正方行列 \\(A\\in\\mathbb{F}^{n\\times n}\\) の固有値 (eigenvalue) であるとは，ゼロでない列ベクトル \\(v\\in\\mathbb{C}^{n}\\) が存在して \\[ Av=\\lambda v \\] が成り立つことをいう。ベクトル \\(v\\) を固有値 \\(\\lambda\\) に対応する固有ベクトル (eigenvector) という。 一般に，行列を掛けるとベクトルの向きを回転させるのだが，固有ベクトルに対してだけは向きを変えないか， あるいは \\(\\lambda\\) が負であればベクトルの矢印を反転させる。 固有ベクトルという言葉は，1つの固有値に対して1つの固有ベクトルがあるというような印象を与えるかもしれない。 しかし，\\(v\\) が固有ベクトルであれば，\\(2v\\) も \\(-3v\\) も固有ベクトルであることが容易に確かめられる。 あるいは，1つの固有値に対して，1次独立な \\(v, w\\) という2つの固有ベクトルが見つかることもある。 このとき \\(v + w\\) も \\(-v + 3w\\) も同じ固有値に対する固有ベクトルである。 固有ベクトルを特定の性質を持つベクトルの集合（固有空間 eigenspace）から任意に1つ選んだものと認識しておくとよい。 方程式\\(Av=\\lambda v\\)の自明な変形により \\[ (\\lambda I-A)v=0 \\] が得られる。この方程式がゼロでないベクトルを解に持つための必要十分条件は \\[ \\phi_{A}(\\lambda)=\\det(\\lambda I-A)=0 \\] が成り立つことである。 上の事実を確認せよ。[ヒント：正則性と行列式の関係を使う。] \\(\\phi_{A}(\\lambda)\\) は \\(\\lambda\\) に関する \\(n\\) 次多項式であり, 固有多項式 (characteristic polynomial) という。\\(\\phi_{A}(\\lambda)=0\\) は重複度を込めて\\(n\\) 個の解を持つので，\\(A\\) は (重複度を込めて) \\(n\\) 個の固有値を持つ18。 固有多項式の係数を \\[ \\phi_{A}(\\lambda)=\\lambda^{n}+c_{1}\\lambda^{n-1}+\\cdots+c_{n} \\] とすれば， \\[ c_{1}=-\\operatorname{trace} A,\\qquad c_{n}=(-1)^{n}\\det A \\] が成り立つことを示せ。 なお，\\(\\operatorname{trace} A = \\sum_{i=1}^n A_{i,i}\\) は行列 \\(A\\) のトレースといい， 対角成分を足して得られる数である。 定理 4.1 実行列 \\(A\\in\\mathbb{R}^{n\\times n}\\)が複素固有値 \\(\\lambda\\) を持てば，\\(\\bar{\\lambda}\\) も\\(A\\) の固有値である。 定理4.1を証明せよ。 定理 4.2 異なる固有値 \\(\\lambda_{1}\\neq\\lambda_{2}\\) に対する固有ベクトル \\(v_{1}\\)，\\(v_{2}\\) は1次独立である。すなわち \\(\\alpha_{1},\\alpha_{2}\\in\\mathbb{C}\\) に対して， \\[ \\alpha_{1}v_{1}+\\alpha_{2}v_{2}=0\\Rightarrow\\alpha_{1}=\\alpha_{2}=0. \\] 証明. 定理の主張に反して1次従属であったとしよう。すなわち，同時にはゼロでない \\(\\alpha_{1}, \\alpha_{2}\\) があって\\(\\alpha_{1}v_{1}+\\alpha_{2}v_{2}=0\\)を満たすとする。 一般性を失うことなく，\\(\\alpha_1 \\neq 0\\) とできる。 このとき， \\[ \\alpha_{1}Av_{1}+\\alpha_{2}Av_{2}=0. \\] 固有値の定義より \\[ \\alpha_{1}\\lambda_{1}v_{1}+\\alpha_{2}\\lambda_{2}v_{2}=0. \\] \\(\\alpha_1 v_1 + \\alpha_2 v_2 = 0\\) より, \\[ \\alpha_1 (\\lambda_1 - \\lambda_2) v_1 = 0. \\] \\(\\alpha_{1}\\neq0\\)， \\(v_{1}\\neq0\\), \\(\\lambda_{1} \\neq \\lambda_{2}\\) であったから，これは不合理である。 従って，相異なる固有値に対応する固有ベクトルは1次独立でなければならない。 2つ以上の異なる固有値についても同様のことが証明できる。 定理 4.3 正方行列 \\(A \\in \\mathbb R^{n \\times n}\\) が相異なる固有値 \\(\\lambda_{1}, \\dots, \\lambda_{n}\\) をもつとする。それぞれの固有値 \\(\\lambda_i\\) に対応する固有ベクトル \\(v_i\\) としたとき， \\(\\{ v_1, \\dots, v_n \\}\\) は1次独立である。 すなわち， \\(\\alpha_{1},\\dots,\\alpha_{n}\\in\\mathbb{C}\\) に対して \\(\\alpha_{1}v_{1}+ \\cdots + \\alpha_{n}v_{n}=0\\) が成り立てば，\\(\\alpha_{1}=\\cdots=\\alpha_{n}=0\\) が成り立つ。 定理4.3を証明せよ。 4.3 対角化 4.3.1 計算方法の確認 対角化（diagonalization）というのは，行列の標準形（canonical form）の中でももっとも基本的なものであるから， きちんと理解して使えるようになってほしい。 理論的な解説は次章以降に譲って，ここでは計算方法を思い出してもらおう。 簡単のため，行列 \\(A \\in \\mathbb R^{n \\times n}\\) の固有値\\(\\lambda_1,\\dots, \\lambda_n \\in \\mathbb C\\) が互いに異なると仮定する。対応する固有ベクトルをそれぞれ \\(v_1,\\dots, v_n \\in \\mathbb C^n\\) とすれば， \\[ Av_1 = \\lambda_1 v_1, \\dots, Av_n = \\lambda_n v_n \\] が成り立つ。\\(v_1, \\dots, v_n\\) を並べた行列 \\[ V = \\begin{bmatrix} v_1 &amp; \\cdots &amp; v_n \\end{bmatrix} \\] を定義すれば， \\[ \\begin{aligned} AV &amp;= \\begin{bmatrix} Av_1 &amp; \\cdots &amp; Av_n \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} \\lambda_1 v_1 &amp; \\cdots &amp; \\lambda_n v_n \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} v_1 &amp; \\cdots &amp; v_n \\end{bmatrix} \\begin{bmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n \\end{bmatrix} \\\\ &amp;= VD \\end{aligned} \\] が成り立つ。ただし， \\[ D = \\begin{bmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n \\end{bmatrix} \\] とおいた。 もとの行列と対角化行列を行き来するために，固有ベクトルを並べた行列\\(V\\) が重要な役割を果たす。 実際，定理4.3 によって\\(V\\)が正則であることが分かる。\\(V^{-1}\\)が存在するので， \\[ A = VDV^{-1}, \\qquad D = V^{-1}AV \\] という公式を得る。 4.3.2 例題 行列 \\[ J = \\begin{bmatrix} 0 &amp; -1\\\\ 1 &amp; 0 \\end{bmatrix} \\] の固有値と固有ベクトルを計算してみよう。 \\[ \\det(\\lambda J-I) = \\left| \\begin{bmatrix} \\lambda &amp; 1\\\\ -1 &amp; \\lambda \\end{bmatrix} \\right| = \\lambda^2 + 1 \\] だから，固有値は \\(\\pm i\\) である。固有値 \\(i\\) に対する固有ベクトルは， \\[ \\begin{bmatrix} 0 &amp; -1\\\\ 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} x\\\\ y \\end{bmatrix} = i \\begin{bmatrix} x\\\\ y \\end{bmatrix} \\] の解であり， \\[ x = iy \\] によって特徴付けられる。例えば， \\[ v_i = \\begin{bmatrix}i\\\\1\\end{bmatrix} \\] が固有ベクトルである。固有値 \\(- i\\) に対する固有ベクトルは， \\[ \\begin{bmatrix} 0 &amp; -1\\\\ 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} x\\\\ y \\end{bmatrix} = - i \\begin{bmatrix} x\\\\y \\end{bmatrix} \\] の解であり， \\[ ix=y \\] によって特徴付けられる。例えば， \\[ v_{-i}=\\begin{bmatrix} 1\\\\ i \\end{bmatrix} \\] が固有ベクトルである。 固有ベクトル\\(v_{i}\\) と \\(v_{-i}\\)を並べた行列 \\[ V = \\begin{bmatrix} v_{j} &amp; v_{-j} \\end{bmatrix} = \\begin{bmatrix} i &amp; 1\\\\ 1 &amp; i \\end{bmatrix} \\] は正則であり（\\(\\det V = i^2 - 1 = -2\\)）， \\[ V^{-1} = \\frac{1}{i^2-1} \\begin{bmatrix} i &amp; -1\\\\ -1 &amp; i \\end{bmatrix} = \\begin{bmatrix} -i/2 &amp; 1/2\\\\ 1/2 &amp; -i/2 \\end{bmatrix}. \\] \\[ \\begin{aligned} V\\begin{bmatrix} i &amp; 0\\\\ 0 &amp; -i \\end{bmatrix}V^{-1} &amp; = \\begin{bmatrix} i &amp; 1\\\\ 1 &amp; i \\end{bmatrix}\\begin{bmatrix} i &amp; 0\\\\ 0 &amp; -i \\end{bmatrix}\\begin{bmatrix} -i/2 &amp; 1/2\\\\ 1/2 &amp; -i/2 \\end{bmatrix}\\\\ &amp; = \\begin{bmatrix} -1 &amp; -i\\\\ i &amp; 1 \\end{bmatrix}\\begin{bmatrix} -i/2 &amp; 1/2\\\\ 1/2 &amp; -i/2 \\end{bmatrix}\\\\ &amp; = \\begin{bmatrix} 0 &amp; -1\\\\ 1 &amp; 0 \\end{bmatrix}\\\\ &amp; = J. \\end{aligned} \\] あるいは \\[\\begin{equation} \\begin{bmatrix} i &amp; 0\\\\ 0 &amp; -i \\end{bmatrix} = V^{-1}JV \\tag{4.1} \\end{equation}\\] が成り立つ。固有ベクトルを並べた行列\\(V\\)によって \\(J\\) を対角行列に変形 (対角化) することができた。 あらゆる行列に対してこのような変形ができる訳ではないことには注意が必要である。 すべての固有値が相異なる場合や，対称行列などが対角化が可能である。 4.3.3 複素数の行列表現 行列 \\[ J = \\begin{bmatrix} 0 &amp; -1\\\\ 1 &amp; 0 \\end{bmatrix} \\] に対して \\[ J^{2}=-I \\] が成り立つことに注意しよう。\\(I\\) を実数の単位（すなわち，\\(1\\)）と対応させれば， \\(J\\) が虚数単位と同じ性質を持つことが想像できるだろう。複素数 \\(z = a + bi\\) を行列を用いて \\[ Z = aI+bJ = \\begin{bmatrix} a &amp; -b\\\\ b &amp; a \\end{bmatrix} \\] と表すことができる。これをさらに極形式 \\(z = re^{i \\theta} = r\\cos \\theta + i(r \\sin \\theta)\\)， \\(r \\ge 0, 0 \\le \\theta &lt; 2\\pi\\) の記号を用いて書き直せば， \\[ Z=r\\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta\\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix} \\] とできる。 \\[ Z^n \\begin{bmatrix} x_{1}\\\\ x_{2} \\end{bmatrix} = r^n \\begin{bmatrix} \\cos n\\theta &amp; -\\sin n\\theta\\\\ \\sin n\\theta &amp; \\cos n\\theta \\end{bmatrix} \\begin{bmatrix} x_{1}\\\\ x_{2} \\end{bmatrix} \\] をR上で計算し，\\(Z\\) がベクトルの伸縮と回転という複素数積と同様の性質を持っていることを確認しておこう。 r = 0.8; theta = pi / 6 # polar form Z = r * matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), nrow=2, byrow=TRUE) v0 = c(1, 0) # initial vector # iterative multiplication simulation_size = 50 # &gt;= 2 res = tibble(x1 = numeric(simulation_size), x2 = numeric(simulation_size)) res$x1[1] = v0[1]; res$x2[1] = v0[2] for (i in 2:simulation_size) { v1 = Z %*% v0 res$x1[i] = v1[1]; res$x2[i] = v1[2] v0 = v1 } # plot ggplot(res, aes(x1, x2)) + geom_point() 4.3.4 実対角化 複素数を実係数の行列で表現できるという考え方は，実係数の範囲で行列を標準化（ブロック対角化, 実ジョルダン標準化）する場合などに役に立つ。一般に固有値・固有ベクトルは複素数・複素ベクトルであるが，考えたいモデルの自然な表現が実数である場合には，実標準形の方が望ましいというケースもあるだろう。また，Klein (2000) が議論しているように，実標準形を用いる方が数値計算を高速に実行できるケースもある。 本書では複素数の範囲で標準化を考えるが，基本的な方針だけを述べておこう。 例えば，2次正方行列\\(A \\in \\mathbb R^2\\) が対角化の手続きによって， \\[ AP = P \\begin{bmatrix} a + bi &amp; 0 \\\\ 0 &amp; a - bi \\end{bmatrix} \\] と対角化できたとしよう。 定理4.1より共役複素数が必ずペアで現れることに注意せよ。 \\(P\\) は固有ベクトルを並べた行列である。式(4.1)より， \\[ \\begin{aligned} AP &amp;= P \\left( a I + b \\begin{bmatrix} i &amp; 0 \\\\ 0 &amp; - i \\end{bmatrix} \\right) \\\\ &amp;= P(aV^{-1}IV + b V^{-1} J V)\\\\ &amp;= PV^{-1} (aI + bJ) V \\end{aligned} \\] したがって， \\[ A(PV^{-1}) = (PV^{-1})(aI + bJ) \\] あるいは， \\[ A = (PV^{-1})(aI + bJ)(PV^{-1})^{-1} \\] とできる。これが\\(A\\)の実行列の範囲での標準化（real canonical form）の基本である。 2次より大きい行列の場合には， 現れる複素数の数だけ \\(aI + bJ\\) の形式のブロックが対角成分に並ぶ。 実対角化の結果は，例えば次のようなブロック対角行列になる。 \\[ \\begin{bmatrix} a_1 &amp; -b_1 &amp; &amp; &amp; &amp; \\\\ b_1 &amp; a_1 &amp; &amp; &amp; &amp; \\\\ &amp; &amp; a_2 &amp; -b_2 &amp; &amp; \\\\ &amp; &amp; b_2 &amp; a_2 &amp; &amp; \\\\ &amp; &amp; &amp; &amp;\\ddots &amp; \\\\ &amp; &amp; &amp; &amp; &amp; \\ddots \\end{bmatrix} \\] 4.4 線形動学方程式と固有値 再び，線形動学方程式 \\[ x_{t+1} = Ax_t \\] を考えよう。行列 \\(A\\) が \\(AV = VD\\) によって対角化されたとすると， $y_t = \\[ x_{t+1} = AVV^{-1}x_t = VDV^{-1} x_t. \\] \\(y = V^{-1}x\\) と定義すれば， \\[ y_{t+1} = Dy_t \\] とできる。\\(y_t = D^t y_0\\) は容易に計算することができる。実際， \\[ D = \\begin{bmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n \\end{bmatrix} \\] とすれば， \\[ D^t = \\begin{bmatrix} \\lambda_1^t &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n^t \\end{bmatrix}, \\qquad t = 0,1,\\dots \\] が成り立つ。\\(A^t = (VDV^{-1})^t= VD^tV^{-1}\\) により, 固有値を用いて\\(x_t\\) の振る舞いを分析できる。 固有値がすべて複素平面上の単位円の内側にあれば，原点は大域的に漸近安定である。 線形システムは安定であり，任意の初期値 \\(x_0 \\in \\mathbb R^n\\) に対して \\(\\lim_{t\\to\\infty} x_t \\to 0\\) が成り立つ。すべての固有値が単位円の外側にあれば， 任意の初期値\\(x_0 \\neq 0\\) に対して，\\(x_t\\) は発散する。すなわち，\\(\\| x_t \\| \\to \\infty\\) が成り立つ19。 このような挙動を指してシステムが反安定 antistableであると呼ぼう。 絶対値が1より小さい固有値を安定固有値（stable eigenvalue）, 1より大きい固有値を不安定固有値（unstable eigenvalue）と呼ぶ。 （ただし，この呼び方は離散時間システムを考えている限りにおいてのみ意味がある。 連続時間システムでは別の条件によって安定性が特徴付けられる。） 経済学においては，固有値のいくつかが安定であり， 残りが不安定であるというのが典型的である。このような原点を鞍点（saddle）， システムの鞍点周りの挙動を鞍点安定（saddle-point stability）と呼んでいる。 力学系の理論では，通常，不安定性の一種として扱われることが多いのだが， 経済学ではすべての初期条件が与えられていない問題を考えるという事情によって 鞍点安定なシステムにおける収束経路に重要な解釈を与えている。 すなわち，発散する経路ではなく原点に収束していく経路が経済主体の意思決定によって選ばれるのである。これを仮定することもあれば，モデルの解が満たす条件として得られるケースもある。 実際には反安定となるケースもあるにはあるのだが，鞍点の方が発生しやすい。 ラムゼーモデルを通して観察してみよう。 4.5 ラムゼーモデルと固有値 再びラムゼーモデルの不動点を考えよう。Levhari and Liviatan (1972) によれば，\\(\\lambda\\) が固有値であれば，\\(1/(\\beta \\lambda)\\) も固有値である。したがって， ラムゼーモデルの不動点の固有値は複素平面上で原点を中心とする半径\\(1/\\beta\\) の円を挟むように分布する。 1セクターモデルではいずれも実数になるから，図4.4 あるいは， 図4.5 といったケースが典型的である。 図 4.4: 鞍点（1セクター） 図 4.5: 不安定（1セクター） 一般のセクターに対しては定理2.3 は成り立たない。 複素固有値 \\(\\lambda\\) が存在すれば，定理4.1 によって\\(\\bar \\lambda\\) も固有値である。さらに，Levhari and Liviatan (1972) によれば \\(1/(\\beta \\lambda)\\) と \\(1/(\\beta \\bar \\lambda)\\)も固有値である。1つの複素固有値から3つの複素固有値が派生する。図4.6, 図4.7。 図 4.6: 鞍点（マルチセクター） 図 4.7: 不安定（マルチセクター） 不動点は鞍点であるか，あるいは反安定となる。図から想像できるように， \\(\\beta\\) が1に近づくにつれて，安定な固有値が生じて鞍点になりやすい。 実は\\(\\beta\\) が十分1に近いとき，ラムゼーモデルは大域的に安定な経路を持つことが知られている。 大域的安定性の研究はターンパイク（turnpike）理論と呼ばれる一大分野を形成している。(McKenzie 1986) 逆に，\\(\\beta\\)が小さくなると安定性が失われ，不安定挙動（周期軌道，カオス）が現れる。 例えば Deneckere and Pelikan (1986) を見よ。 4.6 R コード 行列式 実行列の行列式は，ずばり det() で計算できる。 det(A) ## [1] 0 det(B) ## [1] 81.00696 残念ながら複素行列の行列式は計算できない。 det(C) ## Error in determinant.matrix(x, logarithm = TRUE, ...): &#39;determinant&#39; not currently defined for complex matrices しかし，det() の計算をする必要があるケースは限られているだろう。 どうしても必要な場合は次のように計算すればよい。 prod(eigen(C, only.values = TRUE)$values) ## [1] -104.7096-110.527i これで行列式を計算できるのは，固有値の積が行列式と一致するという公式による。次章で説明する。 参考文献 "],
["linspace.html", "第5章 線形空間論の基礎 5.1 線形空間 5.2 線形部分空間 5.3 1次独立性 5.4 線形写像 5.5 行列 5.6 不変部分空間", " 第5章 線形空間論の基礎 これまでは縦横に数字を並べたものとして行列を定義した。行列積が数ベクトルに対して作用すると， 数ベクトルを変換するように働く。例えば，\\(A = [a_{i,j}] \\in \\mathbb R^{m \\times n}\\) が \\(x = [x_1, \\dots, x_n]^\\top \\in \\mathbb R^n\\) に作用すると \\[ Ax = \\begin{bmatrix} \\sum_{j = 1}^n a_{1, j} x_j \\\\ \\vdots \\\\ \\sum_{j = 1}^n a_{m, j} x_j \\end{bmatrix} \\in \\mathbb R^m \\] という \\(\\mathbb R^m\\) に属する数ベクトルに変換される。写像（mapping），あるいは関数（function）の記法を採用すれば次のように書くことができるだろう20。 \\[ \\begin{aligned} f :\\ &amp; \\mathbb R^n \\to \\mathbb R^m\\\\ &amp; x \\mapsto Ax \\end{aligned} \\] 行列積の定義により，写像 \\(f\\) は次の性質を持つ。任意の \\(x_1, x_2 \\in \\mathbb R^n\\) と \\(\\alpha \\in \\mathbb R\\) に対して \\[ \\begin{aligned} f(x_1 + x_2) &amp;= f(x_1) + f(x_2), \\\\ f(\\alpha x_1) &amp;= \\alpha f(x_1). \\end{aligned} \\] 上の関係を確認せよ。 この性質を線形性（lineariry）といい，経済動学理論を習得する上で最も重要な概念の1つである。 本節では，数ベクトル空間を抽象化した線形空間（vector space）を導入し，線形空間の間の線形写像（linear mapping）として行列を再導入する。 多くの読者にとって馴染み深い行列は，線形空間の間の線形写像を表形式で表現したものであり， その要素は定義域・終域にあたる線形空間の「基底」（basis）ごとに定まる。 したがって，基底を変更すると行列は変わる。 対角化の方法を前章で確認したが，これは対角化の手続きによって線形空間の基底が変更されたことによる。次章以降で紹介する標準化の理論は，行列の分析にとって最も望ましい基底を探すための手続きに外ならない。 やや逆説的に聞こえるかもしれないが，抽象的な理論を持ち出す理由はシステム表現の変更に際して「変化しない実体」を見失わないようにするためである。しっかり身につけてほしい。 5.1 線形空間 \\(X\\) を集合，\\(\\mathbb{F} = \\mathbb R\\) または \\(\\mathbb C\\) とする。 定義 5.1 (線形空間) \\(X\\) が \\(\\mathbb{F}\\) 上の線形空間 (ベクトル空間，linear space over \\(\\mathbb{F}\\)) であるとは，和（addition） \\[ X \\times X \\ni (x,y) \\mapsto x + y \\in X \\] とスカラー倍（scalar multiplication） \\[ X\\times\\mathbb{F}\\ni(x,\\alpha)\\mapsto\\alpha x\\in X \\] が定義されていて, 以下の諸条件を満たすことをいう。線形空間の元をベクトルと呼ぶ。 任意の \\(x,y,z\\in X\\) に対して，\\((x+y)+z=x+(y+z)\\) が成り立つ。 任意の \\(x,y\\in X\\) に対して，\\(x+y=y+x\\) が成り立つ。 任意の \\(x\\in X\\) に対して \\(\\theta+x=x\\) なる \\(\\theta\\in X\\) が存在する。これをゼロベクトルと呼び， \\(0\\) で表す。 任意の \\(x\\in X\\) に対して，\\(x+x&#39;=0\\) なる \\(x&#39;\\in X\\) が存在する。これを逆元と呼び \\(-x\\) で表す。 任意の \\(x\\in X\\) と \\(\\alpha,\\beta\\in\\mathbb{F}\\) に対して，\\((\\alpha+\\beta)x=\\alpha x+\\beta x\\) が成り立つ。 任意の \\(x\\in X\\) と \\(\\alpha,\\beta\\in\\mathbb{F}\\) に対して，\\(\\alpha(x+y)=\\alpha x+\\alpha y\\) が成り立つ。 任意の \\(x\\in X\\) と \\(\\alpha,\\beta\\in\\mathbb{F}\\) に対して，\\((\\alpha\\beta)x=\\alpha(\\beta x)\\) が成り立つ。 任意の \\(x\\in X\\) について，\\(1x=x\\)。 いくつか具体例を確認しておこう。 例 5.1 数ベクトル空間 \\(\\mathbb{F}^{n}\\) \\[ \\mathbb{F}^{n}=\\left\\{ x=\\begin{bmatrix}x_{1}\\\\ x_{2}\\\\ \\vdots\\\\ x_{n} \\end{bmatrix}\\ \\mid\\ x_{i}\\in\\mathbb{R},\\ i=1,2,\\dots,n\\right\\} \\] は成分ごとの和とスカラー倍によって線形空間となる。 例 5.2 \\(m\\times n\\) 行列全体の集合は線形空間をなす。節3.2 を参照。 例 5.3 高々 \\(n\\) 次の多項式全体の集合 \\(P^{n}[z]\\) も通常の和と定数倍に関して線形空間となる。 例 5.4 数列全体の空間 \\(\\{x_{n}\\}_{n=1}^{\\infty}\\subset\\mathbb{R}\\) も成分ごとの和とスカラー倍を導入すると\\(\\mathbb{R}\\) 上の線形空間となる。 例 5.5 有限期待値を持つ実確率変数の空間は\\(\\mathbb{R}\\) 上の線形空間となる.21 5.2 線形部分空間 定義 5.2 \\(X\\) を \\(\\mathbb{F}\\)上の線形空間，\\(Y\\) を \\(X\\) の部分集合とする。\\(Y\\) が\\(X\\) に定義された演算について線形空間になるとき， \\(Y\\) は\\(X\\)の線形部分空間であるという。 \\(X\\) 自身と \\(\\{0\\}\\) は必ず線形部分空間になる。 命題 5.1 \\(Y \\subset X\\) が \\(X\\) の線形部分空間であるための必要十分条件は，\\(Y\\) が和とスカラー倍について閉じていることである。 すなわち，ある部分集合 \\(Y\\) が線形部分空間であることを確認するには，任意の \\(y_{1},y_{2}\\in Y\\)，\\(\\alpha\\in\\mathbb{F}\\)について，\\(y_{1}+y_{2}\\in Y\\) と \\(\\alpha y_{1}\\in Y\\) が成り立つことを示しさえすればよい。 命題 5.1 を証明せよ。 定義 5.3 ベクトル\\(x_{1},\\dots,x_{n}\\in X\\) をすべて含む最小の線形部分空間を，\\(\\{x_{1},\\dots,x_{n}\\}\\) によって張られる線形部分空間（linear space spanned by …）といい，\\(\\mathrm{span}\\{x_{1},\\dots,x_{n}\\}\\) と書く。 \\[ \\mathrm{span}\\{x_{1},\\dots,x_{n}\\} = \\left\\{ \\alpha_{1}x_{1}+\\cdots+\\alpha_{n}x_{n}\\ \\mid\\ \\alpha_{1},\\dots,\\alpha_{n}\\in\\mathbb{F} \\right\\} \\] である。 \\(\\alpha_{1}x_{1}+\\cdots+\\alpha_{n}x_{n}\\) の形のベクトルは \\(\\{x_{1},\\dots,x_{n}\\}\\) の線形結合（linear combination）であるという。 2つの部分空間 \\(Y\\)，\\(W\\) があったとき，積集合 \\(Y\\cap W\\) はまた線形部分空間になる。 一方，和集合 \\(Y\\cup W\\) は一般には線形部分空間にならない。 定義 5.4 \\(X\\) を\\(\\mathbb{F}\\) 上の線形空間，\\(Y,W\\) を\\(X\\) の線形部分空間とする。部分空間の和（sum） \\(Y+W\\) を\\(Y\\) と\\(W\\) の元をすべて含む最小の線形部分空間と定義する22。すなわち \\[\\begin{equation} Y+W=\\{\\alpha y+\\beta w\\ \\mid\\ y\\in Y,\\ w\\in W,\\ \\alpha,\\beta\\in\\mathbb{F}\\}.\\tag{5.1} \\end{equation}\\] 上の定義 (5.1) は和とスカラー倍をすべて含むように作られており， 命題5.1 により線形部分空間となる。 定義 5.5 \\(M\\)，\\(N\\) を\\(X\\) の線形部分空間とする。\\(M \\cap N = \\{ 0 \\}\\) のとき，和 \\(M + N\\) を特に直和といい， \\(M \\oplus N\\) と記す。 \\(M \\oplus N\\) のベクトル \\(x\\) は一意的な分解 \\(x = u + v\\)，\\(u\\in M\\)，\\(v\\in N\\) をもつ。 証明. \\(u,u&#39;\\in M\\) と \\(v,v&#39;\\in N\\)がともに \\(x=u+v\\) と \\(x=u&#39;+v&#39;\\) を満たすとしよう。\\(u-u&#39;=v&#39;-v\\in M\\cap N=\\{0\\}\\) より \\(u=u&#39;\\) と \\(v=v&#39;\\) が分かる。 5.3 1次独立性 定義 5.6 ベクトル \\(x_{1},\\dots,x_{d}\\in X\\) が1次独立であるとは， \\[ \\alpha_{1}x_{1} + \\cdots + \\alpha_{d}x_{d} = 0 \\Longrightarrow \\alpha_{1} = \\cdots = \\alpha_{d} = 0 \\] が成り立つことをいう。1次独立でないとき 1次従属であるという。 ベクトルの組 \\(\\{x_{1},\\dots,x_{d}\\}\\) が1次従属であれば，そのうちの1つが他のベクトルの1次結合でかける。 例えば，1つはゼロでない \\((\\alpha_{1},\\dots,\\alpha_{d})\\) が存在して， \\[ \\alpha_{1}x_{1}+\\cdots+\\alpha_{d}x_{d}=0 \\] となるとする。一般性を失うことなく，\\(\\alpha_{1}\\neq0\\) とできて \\[ x_{1}=\\sum_{k=2}^{d}-\\frac{\\alpha_{k}}{\\alpha_{1}}x_{k} \\] と書ける。 逆が成り立つことを確認せよ。 定義 5.7 ある自然数 \\(N\\) が存在し，\\(N\\)個より大きい数のベクトルの組が必ず1次従属になるとき，\\(X\\) は有限次元であるという。 そのような \\(N\\) が存在しないとき，\\(X\\) は無限次元であるという。 定義 5.8 有限次元ベクトル空間 \\(X\\) のベクトルの組 \\(V=\\{v_{1},\\dots,v_{n}\\}\\) が基底であるとは，\\(V\\) が1次独立であり，\\(V\\) に\\(X\\) のいかなるベクトルを加えても必ず1次従属になることをいう。あるいは同じことだが，\\(\\mathrm{span}(V)=X\\)。 \\(X\\) を\\(\\mathbb{F}\\)上の有限次元ベクトル空間，\\(\\{v_{1},\\dots,v_{n}\\}\\) を \\(X\\) の任意の基底とする。 任意の \\(x\\in X\\) は \\(\\{v_{1},\\dots,v_{n}\\}\\) の線形結合で書ける。すなわち，\\(\\alpha_{1},\\dots,\\alpha_{n}\\in\\mathbb{F}\\) があって \\[ x=\\alpha_{1}v_{1}+\\cdots+\\alpha_{n}v_{n} \\] となる。この表現は (基底ごとに) 一意的に定まる。 証明. 確認せよ。 定理 5.1 有限次元ベクトル空間の基底の数は基底の選び方にかかわらず一定である。 証明. \\(X\\)を有限次元ベクトル空間とする。\\(\\{v_{1},\\dots,v_{n}\\}\\) と\\(\\{w_{1},\\dots,w_{m}\\}\\) がともに基底であるとしよう. \\(m\\neq n\\) を仮定して矛盾を導く。一般性を失うことなく \\(m&gt;n\\) としてよい。\\(w_{1},\\dots,w_{m}\\) はすべて \\(v_{1},\\dots,v_{n}\\) の1次結合でかけるので， \\[ w_{i}=\\sum_{j=1}^{n}\\alpha_{ij}v_{j},\\quad\\alpha_{ij}\\in\\mathbb{F},\\ i=1,\\dots,m. \\] \\(\\beta_{1},\\dots,\\beta_{m}\\in\\mathbb{F}\\) を\\(\\beta_{1}w_{1}+\\cdots+\\beta_{m}w_{m}=0\\) が成り立つ数としよう。 \\[ \\begin{aligned} \\beta_{1}w_{1}+\\cdots+\\beta_{m}w_{m} &amp; =\\sum_{i=1}^{m}\\beta_{i}\\sum_{j=1}^{n}\\alpha_{ij}v_{j}\\\\ &amp; =\\sum_{j=1}^{n}\\left(\\sum_{i=1}^{m}\\beta_{i}\\alpha_{ij}\\right)v_{j} \\end{aligned} \\] であり，\\(\\{v_{1},\\dots,v_{n}\\}\\) は基底であるから， \\[\\begin{equation} \\sum_{i=1}^{m}\\beta_{i}\\alpha_{ij}=0,\\quad j=1,\\dots,n\\tag{5.2} \\end{equation}\\] が成り立たなければならない。線形方程式を行列表示すると \\[ \\begin{bmatrix}\\beta_{1} &amp; \\cdots &amp; \\beta_{m}\\end{bmatrix}\\begin{bmatrix}\\alpha_{11} &amp; \\cdots &amp; \\alpha_{1n}\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\alpha_{m1} &amp; \\cdots &amp; \\alpha_{mn} \\end{bmatrix}=0. \\] 同値な変形を施して， \\[ \\begin{bmatrix} \\beta_{1} &amp; \\cdots &amp; \\beta_{m} \\end{bmatrix} P^{-1} \\begin{bmatrix} I_{n\\times n}\\\\ O_{(m-n)\\times n} \\end{bmatrix}Q^{-1}=0 \\] を得る（命題 3.1）。さらに右から\\(Q\\)をかければ \\[ \\begin{bmatrix}\\beta_{1} &amp; \\cdots &amp; \\beta_{m}\\end{bmatrix}P^{-1}\\begin{bmatrix}I_{n\\times n}\\\\ O_{(m-n)\\times n} \\end{bmatrix}=\\begin{bmatrix}\\tilde{\\beta}_{1} &amp; \\cdots &amp; \\tilde{\\beta}_{m}\\end{bmatrix}\\begin{bmatrix}I_{n\\times n}\\\\ O_{(m-n)\\times n} \\end{bmatrix}=0. \\] したがって， \\[ \\begin{bmatrix}\\tilde{\\beta}_{1} &amp; \\cdots &amp; \\tilde{\\beta}_{m}\\end{bmatrix}=\\begin{bmatrix}0 &amp; \\cdots &amp; 0 &amp; c_{n+1} &amp; \\cdots &amp; c_{m}\\end{bmatrix} \\] である。\\(c_{n+1},\\dots,c_{m}\\) は任意の定数。逆変形を行えば \\[ \\begin{bmatrix}\\beta_{1} &amp; \\cdots &amp; \\beta_{m}\\end{bmatrix}=\\begin{bmatrix}0 &amp; \\cdots &amp; 0 &amp; c_{n+1} &amp; \\cdots &amp; c_{m}\\end{bmatrix}P \\] より，\\(\\beta_{1},\\dots,\\beta_{m}\\) のすべてがゼロでなくても (5.2) を成り立たせることができることが分かる (右から掛ける \\(P\\) は列基本変形であることに注目する)。\\(\\{w_{1},\\dots,w_{m}\\}\\) が基底であることに矛盾する。 定義 5.9 有限次元ベクトル空間 \\(X\\) の基底に含まれるベクトルの数は空間固有の量である。これを次元と呼び \\(\\dim X\\) と書く. 定理 5.2 \\(X\\) を\\(n\\)次元ベクトル空間とする。1次独立なベクトルの組 \\(\\{x_{1},\\dots,x_{r}\\}\\subset X\\)， \\(r&lt;n\\)，に \\((n-r)\\) 本のベクトル \\(\\{x_{m+1},\\dots,x_{n}\\}\\) を付け加えて \\(\\{x_{1},\\dots,x_{m},x_{m+1},\\dots,x_{n}\\}\\) が \\(X\\) の基底となるようにできる。 証明. \\(\\{v_{1},\\dots,v_{n}\\}\\) を\\(X\\)の基底とする。\\(v_{1},\\dots,v_{n}\\) の中に \\(x_{1},\\dots,x_{r}\\) の1次結合で書けないものが存在する。さもなくば \\(\\{x_{1},\\dots,x_{r}\\}\\) が基底となり，定理 5.1 により矛盾。\\(v_{1}\\) がそのような元であるとしよう。すると \\(\\{x_{1},\\dots,x_{r},x_{r+1}\\}\\)， \\(x_{r+1}:=v_{1}\\)，は1次独立になる。これが基底であれば終了，そうでなければ \\(\\{v_{2},\\dots,v_{n}\\}\\) の中に\\(\\{x_{1},\\dots,x_{r},x_{r+1}\\}\\) の1次結合で書けないものが存在する。このような手続きを \\(n-r\\) 回実行すれば基底 \\(\\{x_{1},\\dots,x_{n}\\}\\) を構成できる。 5.4 線形写像 5.4.1 基本的な定義の復習 \\(X\\)，\\(Y\\) を集合とする。以下の概念は基本的である。 写像・グラフ 集合 \\(M\\) を\\(X\\times Y\\) の部分集合であって，任意の\\(x\\in X\\) について \\((x,y)\\in M\\) を満たす \\(y\\in Y\\) が必ず存在し，しかもその数が1つだけであるものとしよう。この \\(y\\) を \\(f(x)\\) と書けば \\(M=\\{(x,f(x))\\ \\mid\\ x\\in X\\}\\) が成り立っている。\\(M\\) によって定まる \\(x\\) から \\(y\\) への対応関係 (\\(x\\mapsto y\\)) を \\(X\\) から \\(Y\\) への写像 (mapping)といい，\\(f:X\\to Y\\) と書く。\\(M\\) を \\(f\\) のグラフ (graph) といい，\\(\\mathrm{graph}f\\) と書く。 像・逆像 \\(f:X\\to Y\\)，\\(M\\subset X\\) に対して， \\[ f(M)=\\{f(x)\\in Y\\ \\mid\\ x\\in M\\}\\subset Y \\] を\\(M\\) の\\(f\\) による像 (image) という。\\(f(X)\\) を \\(\\mathrm{im}X\\) と書くこともある。 \\(N\\subset X\\) に対して， \\[ f^{-1}(N)=\\{x\\in X\\ \\mid\\ f(x)\\in N\\}\\subset X \\] を\\(N\\)の\\(f\\) による逆像 (inverse image) と呼ぶ。逆像は空集合となることもある。 恒等写像 写像 \\(x\\mapsto x\\) を恒等写像といい \\(\\mathrm{Id}_{X}\\) あるいは \\(I_{X}\\)と書く。 全射 任意の \\(y\\in Y\\) について，\\(x\\in X\\) が存在して\\(y=f(x)\\) が成り立つとき，\\(f\\) は全射 (surjection， onto mapping)であるという23 単射 \\(f(x_{1})=f(x_{2})\\) ならば \\(x_{1}=x_{2}\\) が成り立つとき，\\(f\\) は単射 (injection， one-to-one mapping) であるという。 全単射・逆写像 全射かつ単射である写像を全単射 (bijection，one-to-one and onto mapping)という。\\(f:X\\to Y\\) を全単射とすれば，\\(f^{-1}:Y\\to X\\) を \\[ \\mathrm{graph}f^{-1}=\\{(y,x)\\ \\mid\\ (x,y)\\in\\mathrm{graph}f\\} \\] によって定義できる。\\(f^{-1}\\) を \\(f\\) の逆写像 (inverse) という。 合成写像 \\(f:X\\to Y\\)，\\(g:Y\\to Z\\) に対して，写像 \\(X\\to Z\\) を \\[ x\\mapsto g(f(x)) \\] によって定める。これを \\(f\\) と \\(g\\) の合成写像といい，\\(g\\circ f\\) と記す。全単射 \\(f:X\\to Y\\)に対して， \\(f^{-1}\\circ f=\\mathrm{Id}_{X}\\)，\\(f\\circ f^{-1}=\\mathrm{Id}_{Y}\\) である。写像の合成には結合法則が成り立つ。すなわち \\[ f\\circ(g\\circ h)=(f\\circ g)\\circ h. \\] したがって，\\(f\\circ g\\circ h\\) などと書いても曖昧さは生じない。 5.4.2 線形性 定義 5.10 \\(X,Y\\) を\\(\\mathbb{F}\\)上の線形空間とする。写像 \\(f\\) が線形であるとは，任意の \\(\\alpha,\\beta\\in\\mathbb{F}\\)， \\(x_{1},x_{2}\\in X\\) について \\[ f(\\alpha x_{1}+\\beta x_{2})=\\alpha f(x_{1})+\\beta f(x_{2}) \\] が成り立つことをいう。 写像が線形である場合，\\(f(x)\\) と書かずに括弧を省略して \\(fx\\) と書くこともある。無限次元空間の間の写像を作用素 (operator) ということもある。 \\(X\\)と\\(Y\\)の間の全単射線形写像を(線形)同型写像という。同型写像が存在するとき，\\(X\\) と \\(Y\\) は同型であるといい， \\(X\\simeq Y\\) と書く。 例 5.6 期待値が有限である実数値確率変数の空間における期待値作用素 \\(\\mathbb{E}\\) は線形である。(本質的には積分の線形性である.) 例 5.7 \\(P^{n}[x]\\) をたかだか\\(n\\) 次の多項式が作る線形空間とする。すなわち \\[ P^{n}[x]=\\left\\{ p(x)=\\sum_{k=0}^{n}a_{k}x^{k}\\ \\mid\\ a_{0},\\dots,a_{n}\\in\\mathbb{R}\\right\\} \\] 微分作用素 \\[ \\frac{d}{dx}:\\sum_{k=0}^{n}a_{k}x^{k}\\mapsto\\sum_{k=1}^{n}ka_{k}x^{k-1} \\] は線形である。 5.4.3 像空間，核，商空間 \\(X,Y\\) を\\(\\mathbb{F}\\)上の線形空間とする。線形写像 \\(f:X\\to Y\\) の核 (kernel) \\[ \\ker f:=\\{x\\in X\\ \\mid\\ f(x)=0\\}\\subset X， \\] および像 \\[ \\mathrm{im}f:=\\{f(x)\\ \\mid\\ x\\in X\\}\\subset Y \\] はそれぞれ，\\(X,Y\\) の線形部分空間である。\\(\\mathrm{im}f\\) の次元をランクという: \\(\\mathrm{rank}f=\\dim\\mathrm{im}f\\)。 \\(X\\)，\\(Y\\) を線形空間，線形写像 \\(f:X\\to Y\\) が単射であることと \\(\\ker f=\\{0\\}\\) は同値である。 \\(M\\) を\\(X\\) の部分空間とする。\\(X\\) の2項関係 \\(\\sim_{M}\\) を \\[ x&#39;\\sim_{M}x&#39;&#39;:\\Leftrightarrow x&#39;-x&#39;&#39;\\in M \\] によって定める。\\(\\sim_{M}\\) は同値関係である。すなわち， \\[ x\\sim_{M}x, \\] \\[ x&#39;\\sim_{M}x&#39;&#39;\\Rightarrow x&#39;&#39;\\sim_{M}x&#39;, \\] および \\[ x&#39;\\sim_{M}x&#39;&#39;,\\ x&#39;&#39;\\sim_{M}x&#39;&#39;&#39;\\Rightarrow x&#39;\\sim_{M}x&#39;&#39;&#39; \\] を満たす。同値類と呼ばれる集合を \\[ [x]:=\\{x&#39;\\sim_{M}x\\ \\mid\\ x&#39;\\in X\\} \\] と定義し，同値類の間の演算を \\[ \\begin{aligned} [x]+[x&#39;] &amp; :=[x+x&#39;],\\\\ \\alpha[x] &amp; :=[\\alpha x] \\end{aligned} \\] と定めれば同値類全体の集合 \\[ \\left\\{ [x]\\ \\mid\\ x\\in X\\right\\} \\] は線形空間をなす(確認せよ)。この空間を商空間 (quotient space)と呼び \\(X/M\\) と書く。 補題 5.1 \\(X\\) を有限次元ベクトル空間，\\(M\\) を \\(X\\) の部分空間とする。このとき \\(\\dim X/M=\\dim X-\\dim M\\) が成り立つ。 証明. \\(\\dim M=r\\)，\\(\\dim X=n\\) とする。\\(M\\) の基底 \\(\\{v_{1},\\dots,v_{r}\\}\\) を定め， これに \\(n-r\\) 本のベクトルを追加して\\(X\\) の基底 \\(\\{v_{1},\\dots,v_{r},\\dots,v_{n}\\}\\) を構成する。\\(\\{[v_{r+1}],\\dots,[v_{n}]\\}\\) が\\(X/M\\) の基底をなすことを示せば十分である。\\([0]=M\\) であるから， \\[ \\alpha_{r+1}[v_{r+1}]+\\cdots+\\alpha_{n}[v_{n}]=[\\alpha_{r+1}v_{r+1}+\\cdots+\\alpha_{n}v_{n}]=[0] \\] は \\[ \\alpha_{r+1}v_{r+1}+\\cdots+\\alpha_{n}v_{n}\\in M \\] を意味する。しかし，\\(v_{r+1},\\dots,v_{n}\\) の選び方からこれが成り立つのは \\(\\alpha_{r+1}=\\cdots=\\alpha_{n}=0\\) のときだけである。よって，\\(\\{[v_{r+1}],\\dots,[v_{n}]\\}\\) は\\(X/M\\) の1次独立なベクトルの組である。 任意の \\(x\\in X\\) について \\[ [x]=\\left[\\sum_{i=1}^{n}x_{i}v_{i}\\right]=\\left[\\sum_{i=r+1}^{n}x_{i}v_{i}\\right]=\\sum_{i=r+1}^{n}x_{i}[v_{i}] \\] が成り立つので，\\(X/M=\\mathrm{span}\\{[v_{r+1}],\\dots,[v_{n}]\\}\\) が成り立つ。 \\(f:X\\to Y\\) を線形写像とする。\\(M=\\ker f\\) に対する商空間 \\(X/\\ker f\\) はひときわ重要な性質をもっている。 定理 5.3 (準同型定理) \\(X/\\ker f\\simeq\\mathrm{im}f\\)。 証明. 同型写像が存在することを示せばよい。表記を簡略化するため \\(\\sim_{\\ker f}\\) を単に \\(\\sim\\) と書く。\\(x\\sim x&#39;\\Leftrightarrow x-x&#39;\\in\\ker f\\Leftrightarrow f(x)=f(x&#39;)\\) に注意せよ。写像 \\(\\pi:X/\\ker f\\to\\mathrm{im}f\\) を \\[ \\pi:[x]\\mapsto f(x) \\] によって定めれば，\\(\\pi\\)は全単射になる。線形性も定義から明らかである。 定理 5.4 \\(\\dim X=\\dim(\\ker f)+\\mathrm{rank}f.\\) 証明. 補題と定理より， \\[ \\mathrm{rank}f=\\dim\\mathrm{im}f=\\dim(X/\\ker f)=\\dim X-\\dim\\ker f. \\] % 系 5.1 \\(X\\) を有限次元線形空間とする。線形写像 \\(f:X\\to X\\) は単射であれば全射である。 証明. \\(\\dim\\mathrm{im}f=\\dim X-\\dim\\{0\\}=\\dim X\\) より，\\(\\mathrm{im}f=X\\) が成り立つ。 5.4.4 座標 \\(X\\) を \\(\\mathbb{F}\\) 上の有限次元ベクトル空間，\\(\\{v_{1},\\dots,v_{n}\\}\\) を\\(X\\) の基底とする。1次結合による表現 \\[ x=x_{1}v_{1}+\\cdots+x_{n}v_{n},\\quad x_{1},\\dots,x_{n}\\in\\mathbb{F} \\] は一意的なので \\[ \\begin{aligned} &amp; x\\mapsto x_{1}\\\\ &amp; \\quad\\vdots\\\\ &amp; x\\mapsto x_{n} \\end{aligned} \\] なる写像が定まる。これを \\(x_{1}(x),\\dots,x_{n}(x)\\) のように書くと 写像 \\(x\\mapsto x_{i}\\)，\\(i=1,2,\\dots,n\\) は線形写像である。 証明. \\(x&#39;,x&#39;&#39;\\in X\\)，\\(\\alpha,\\beta\\in\\mathbb{F}\\) とする。\\(\\alpha x&#39;+\\beta x&#39;&#39;=\\sum_{k=1}^{n}(\\alpha x&#39;_{k}+\\beta x_{k}&#39;&#39;)v_{k}\\) より，\\(x_{i}(\\alpha x&#39;+\\beta x&#39;&#39;)=\\alpha x_{i}(x&#39;)+\\beta x_{i}(x&#39;&#39;)\\) が成り立つ。 写像 \\[ X\\ni x\\mapsto\\begin{bmatrix}x_{1}(x)\\\\ \\vdots\\\\ x_{n}(x) \\end{bmatrix}\\in\\mathbb{F}^{n} \\] は全単射線形写像である。したがって，\\(\\mathbb{F}\\) 上の \\(n\\) 次元ベクトル空間はすべて \\(\\mathbb{F}^{n}\\) と同型である。 5.5 行列 5.5.1 線形写像の表現としての行列 \\(X,Y\\) を\\(\\mathbb{F}\\)上の有限次元ベクトル空間とする。\\(X\\) の基底 \\(\\{v_{1},\\dots,v_{n}\\}\\)と \\(Y\\) の基底 \\(\\{w_{1},\\dots,w_{m}\\}\\) を1つ定める。線形写像 \\(f:X\\to Y\\)，\\(x\\in X\\) の1次結合による表現を \\(x=x_{1}v_{1}+\\cdots+x_{n}v_{n}\\) とすれば \\[ f(x)=x_{1}f(v_{1})+\\cdots+x_{n}f(v_{n}). \\] したがって，\\(f\\) による基底の像を定めれば線形写像が定まる。\\(f(v_{1}),\\dots,f(v_{n})\\) を \\(\\{w_{1},\\dots,w_{n}\\}\\) の1次結合で表現すると， \\[ \\begin{aligned} f(v_{1}) &amp; =a_{11}w_{1}+\\cdots+a_{m1}w_{m}\\\\ &amp; \\vdots\\\\ f(v_{n}) &amp; =a_{1n}w_{1}+\\cdots+a_{mn}w_{m}. \\end{aligned} \\] したがって， \\[ \\begin{aligned} f(x) &amp; =x_{1}f(v_{1})+\\cdots+x_{n}f(v_{n})\\\\ &amp; =\\left(a_{11}x_{1}+\\cdots+a_{1n}x_{n}\\right)w_{1}\\\\ &amp; \\qquad+\\cdots+\\left(a_{m1}x_{1}+\\cdots+a_{mn}x_{n}\\right)w_{m}. \\end{aligned} \\] 対応関係 \\[ X\\ni x=x_{1}v_{1}+\\cdots+x_{n}v_{n}\\leftrightarrow\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n} \\end{bmatrix}\\in\\mathbb{F}^{n} \\] および \\[ Y\\ni y=y_{1}w_{1}+\\cdots+y_{m}w_{m}\\leftrightarrow\\begin{bmatrix}y_{1}\\\\ \\vdots\\\\ y_{m} \\end{bmatrix}\\in\\mathbb{F}^{m} \\] によって次の関係を得る。 \\[ f(x)\\leftrightarrow\\begin{bmatrix}a_{11}x_{1}+\\cdots+a_{1n}x_{n}\\\\ \\vdots\\\\ a_{m1}x_{1}+\\cdots+a_{mn}x_{n} \\end{bmatrix}=\\begin{bmatrix}a_{11} &amp; \\cdots &amp; a_{1n}\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{m1} &amp; \\cdots &amp; a_{mn} \\end{bmatrix}\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n} \\end{bmatrix}. \\] \\(y=f(x)\\) という関係が特定の基底に依存しない表現であったのに対し，座標を用いた表現 \\[ \\begin{bmatrix}y_{1}\\\\ \\vdots\\\\ y_{m} \\end{bmatrix}=\\begin{bmatrix}a_{11} &amp; \\cdots &amp; a_{1n}\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{m1} &amp; \\cdots &amp; a_{mn} \\end{bmatrix}\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n} \\end{bmatrix} \\] は，\\(X\\) および \\(Y\\) の基底を定めた上で得られたことに注意してほしい。 5.5.2 基底の変換 線形写像 \\(f:X\\to Y\\) に関して，\\(X\\) の基底 \\(\\{v_{1},\\dots,v_{n}\\}\\) と \\(Y\\) の基底 \\(\\{w_{1},\\dots,w_{m}\\}\\) を決めて得られた行列表現 \\[ \\begin{bmatrix}y_{1}\\\\ \\vdots\\\\ y_{m} \\end{bmatrix}=\\begin{bmatrix}a_{11} &amp; \\cdots &amp; a_{1n}\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{m1} &amp; \\cdots &amp; a_{mn} \\end{bmatrix}\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n} \\end{bmatrix} \\] は基底を取り替えることでどのように変化するだろうか。今，\\(X\\) の新しい基底 \\(\\{\\tilde{v}_{1},\\dots,\\tilde{v}_{n}\\}\\) と，\\(Y\\) の新しい基底 \\(\\{\\tilde{w}_{1},\\dots,\\tilde{w}_{m}\\}\\) に取り替えるとしよう。 \\(\\{v_{1},\\dots,v_{n}\\}\\) と \\(\\{\\tilde{v}_{1},\\dots,\\tilde{v}_{n}\\}\\) との間には互いに線形結合によって表現できるという関係があるので，正則行列 \\(P\\) を用いて \\[ \\begin{bmatrix}v_{1} &amp; \\cdots &amp; v_{n}\\end{bmatrix}P=\\begin{bmatrix}\\tilde{v}_{1} &amp; \\cdots &amp; \\tilde{v}_{n}\\end{bmatrix} \\] と表現できる24。 同様に正則行列 \\(Q\\) を用いて \\[ \\begin{bmatrix}w_{1} &amp; \\cdots &amp; w_{m}\\end{bmatrix}Q=\\begin{bmatrix}\\tilde{w}_{1} &amp; \\cdots &amp; \\tilde{w}_{m}\\end{bmatrix} \\] とできる。 \\[ \\begin{aligned} \\begin{bmatrix}f(\\tilde{v}_{1}) &amp; \\cdots &amp; f(\\tilde{v}_{n})\\end{bmatrix} &amp; =\\begin{bmatrix}\\sum_{i=1}^{n}p_{i1}f(v_{i}) &amp; \\cdots &amp; \\sum_{i=1}^{n}p_{in}f(v_{n})\\end{bmatrix}\\\\ &amp; =\\begin{bmatrix}f(v_{1}) &amp; \\cdots &amp; f(v_{n})\\end{bmatrix}P \\end{aligned} \\] および \\[ \\begin{bmatrix}f(v_{1}) &amp; \\cdots &amp; f(v_{n})\\end{bmatrix}=\\begin{bmatrix}w_{1} &amp; \\cdots &amp; w_{m}\\end{bmatrix}A \\] に注意すれば， \\[\\begin{align} \\begin{bmatrix}f(\\tilde{v}_{1}) &amp; \\cdots &amp; f(\\tilde{v}_{n})\\end{bmatrix} &amp; =\\begin{bmatrix}f(v_{1}) &amp; \\cdots &amp; f(v_{n})\\end{bmatrix}P\\nonumber \\\\ &amp; =\\begin{bmatrix}w_{1} &amp; \\cdots &amp; w_{m}\\end{bmatrix}AP\\nonumber \\\\ &amp; =\\begin{bmatrix}\\tilde{w}_{1} &amp; \\cdots &amp; \\tilde{w}_{m}\\end{bmatrix}Q^{-1}AP\\tag{5.3} \\end{align}\\] を得る。これは，新しい基底に関する\\(f\\) の行列表現が \\(Q^{-1}AP\\) であることを表している。 5.6 不変部分空間 \\(X\\) を有限次元ベクトル空間，\\(M\\subset X\\) を部分空間とする。線形写像 \\(f:X\\to X\\) が\\(M\\) \\[ f(M)\\subset M \\] を満たすとき，\\(M\\) は\\(f\\) の不変部分空間 (invariant subspace) であるという。 5.6.1 ブロック対角化 \\(X\\) が不変部分空間の直和として \\[ X=M_{1}\\oplus M_{2}\\oplus\\cdots\\oplus M_{k} \\] と分解されるとき，\\(f\\) の行列表現がブロック対角行列となることを示そう。 \\(M_{i}\\) の基底を \\(\\{v_{i1},\\dots,v_{in_{i}}\\}\\)，\\(i=1,\\dots,k\\)，とすると， \\[ \\bigcup_{i=1}^{k}\\{v_{i1},\\dots,v_{in_{i}}\\},\\qquad\\dim X=n_{1}+\\cdots+n_{k} \\] は\\(X\\)の基底を成す。\\(f(M_{i})\\subset M_{i}\\) より \\(v_{j\\ell}\\)，\\(\\ell=1,\\dots,n_{\\ell}\\)， \\(j\\neq i\\) に対して \\(f(v_{j\\ell})=0\\) となる。したがって， \\[ \\begin{aligned} f(v_{i1}) &amp; =a_{i,11}v_{i1}+\\cdots+a_{i,n_{i}1}v_{in_{i}}\\\\ &amp; \\qquad\\vdots\\\\ f(v_{in_{i}}) &amp; =a_{i,1n_{i}}v_{i1}+\\cdots+a_{i,n_{i}n_{i}}v_{in_{i}}. \\end{aligned} \\] \\(f|_{M_{i}}:M_{i}\\to M_{i}\\) の行列表現を \\(A_{i}=(a_{i,st})\\) とすれば \\[ \\begin{bmatrix}f(v_{i1}) &amp; \\cdots &amp; f(v_{in_{i}})\\end{bmatrix}=\\begin{bmatrix}v_{i1} &amp; \\cdots &amp; v_{in_{i}}\\end{bmatrix}A_{i}. \\] \\(f\\) の行列表現\\(A\\)は \\[\\begin{multline*} \\begin{bmatrix}f(v_{11}) &amp; \\cdots &amp; f(v_{1n_{1}}) &amp; \\cdots &amp; \\cdots &amp; f(v_{k1}) &amp; \\cdots &amp; f(v_{kn_{k}})\\end{bmatrix}\\\\ =\\begin{bmatrix}v_{11} &amp; \\cdots &amp; v_{1n_{1}} &amp; \\cdots &amp; \\cdots &amp; v_{k1} &amp; \\cdots &amp; v_{kn_{k}}\\end{bmatrix}\\begin{bmatrix}A_{1}\\\\ &amp; \\ddots\\\\ &amp; &amp; A_{k} \\end{bmatrix} \\end{multline*}\\] 5.6.2 固有値分解 \\(X\\) を\\(n\\)次元ベクトル空間，\\(f:X\\to X\\) が相異なる固有値 \\(\\lambda_{1},\\dots,\\lambda_{n}\\); \\[ f(v_{i})=\\lambda_{i}v_{i},\\quad v_{i}\\neq0,\\quad i=1,\\dots,n \\] を持つとしよう。このとき， \\[ E_{i}:=\\{v_{i}\\ \\mid\\ f(v_{i})=\\lambda_{i}v_{i}\\} \\] は1次元の線形部分空間であり， \\[ X=E_{1}\\oplus\\cdots\\oplus E_{n} \\] が成り立つ。\\(E_{i}\\) を固有値 \\(\\lambda_{i}\\) に対応する固有空間という。非ゼロベクトル \\(v_{1}\\in E_{1},\\dots,v_{n}\\in E_{n}\\) は\\(X\\) の基底をなす。 線形写像 \\(f\\) を適当な基底 \\(\\{e_{1},\\dots,e_{n}\\}\\) について行列表現したものを \\(A\\) とする。 固有空間が決める新しい基底 \\(\\{v_{1},\\dots,v_{n}\\}\\) への変換行列を \\(P\\) とすれば \\[ \\begin{aligned} \\begin{bmatrix}v_{1} &amp; \\cdots &amp; v_{n}\\end{bmatrix} &amp; =\\begin{bmatrix}e_{1} &amp; \\cdots &amp; e_{n}\\end{bmatrix}P \\end{aligned} \\] である。式 (5.3) より \\[ \\begin{aligned} \\begin{bmatrix}f(v_{1}) &amp; \\cdots &amp; f(v_{n})\\end{bmatrix} &amp; =\\begin{bmatrix}f(e_{1}) &amp; \\cdots &amp; f(e_{n})\\end{bmatrix}P\\\\ &amp; =\\begin{bmatrix}e_{1} &amp; \\cdots &amp; e_{n}\\end{bmatrix}AP\\\\ &amp; =\\begin{bmatrix}v_{1} &amp; \\cdots &amp; v_{n}\\end{bmatrix}P^{-1}AP \\end{aligned} \\] が成り立つ。前小節の結果から，\\(P^{-1}AP\\) は対角行列であることが分かる。すなわち， \\[ P^{-1}AP=\\begin{bmatrix}\\lambda_{1}\\\\ &amp; \\ddots\\\\ &amp; &amp; \\lambda_{n} \\end{bmatrix}. \\] 固有値は一般には複素数であるので，実行列であっても\\(\\mathbb{C}^{n}\\)の範囲で対角化は考える必要がある。\\(\\mathbb{C}\\)が\\(\\mathbb{R}^{2}\\)で表されることをふまえると， 見かけ上次元が増えてしまうようにも思える。しかし，複素固有値 \\(\\lambda=a+bj\\)，\\(a,b\\in\\mathbb{R}\\) に対しては共役複素数 \\(a-bj\\) も固有値であること，\\(a+bj\\)に対する複素固有ベクトルは \\(v+jw\\)，\\(v,w\\in\\mathbb{R}^{n}\\) は \\(a-bj\\) に対応する固有ベクトル \\(v-jw\\) と必ずペアで現れる。これは \\[ \\begin{aligned} A(v+jw) &amp; =(a+bj)(v+jw) \\end{aligned} \\] の共役複素数を取ると \\[ A(v-jw)=(a-bj)(v-jw) \\] が成り立つことから直ちに分かる。これら2式を足し上げると， \\[ Av=-bw+av. \\] 2式の差を取ると \\[ Aw=aw+bv \\] を得る。すなわち，2次元の(実) 部分空間 \\(\\mathrm{span}\\{w,v\\}\\) が複素固有値 \\(\\lambda\\) に対応している。これを基底にしたときの表現行列は \\[ \\begin{aligned} \\begin{bmatrix}Aw &amp; Av\\end{bmatrix} &amp; =\\begin{bmatrix}w &amp; v\\end{bmatrix}\\begin{bmatrix}a &amp; -b\\\\ b &amp; a \\end{bmatrix}. \\end{aligned} \\] すなわち \\(aI+bJ\\)。 以上をまとめると，次の定理を得る。 定理 5.5 有限次元ベクトル空間上の線形写像 \\(f:X\\to X\\) が相異なる固有値を持つとき，表現行列を複素数の範囲で対角化できる。実数の範囲ではブロック対角化できる。 \\(f: X \\to Y\\) という記法は，すべての \\(x \\in X\\) に対して\\(f(x)\\) がただ1つ定まり \\(f(x) \\in Y\\) が成り立つということ。\\(X\\)を定義域（domain），\\(Y\\)を終域（codomain）という。\\(x \\mapsto f(x)\\) という記法は，関数\\(f\\)の終域での表現に注目している。例えば，\\(f(x) = x^2 + 1\\) という書き方に馴染みのある読者も多いだろうが，そう書く代わりに \\(x \\mapsto x^2 + 1\\) と書いていると思えばよい。↩ \\(\\mathbb{E}[X+Y]=\\mathbb{E}X+\\mathbb{E}Y\\) といった式が成り立つ。↩ 集合 \\(A\\) がある性質をもつ「最小の集合」であるとは，\\(A\\) がその性質を満たし，かつ，\\(A\\) の真部分集合でその性質を満たすものが存在しないことをいう。↩ \\(f:X\\to Y\\) は通常，“\\(f\\) maps \\(X\\) into \\(Y\\)” と読むが，\\(f\\) が全射のときは “\\(f\\) maps \\(X\\) onto \\(Y\\)” と読む。↩ \\(P=[p_{ij}]_{ij}\\) とすれば，\\(\\tilde{v}_{i}=\\sum_{i=1}^{n}p_{ij}v_{i}\\) なる関係をこのように表現している。あるいは列に対する初等変形と考えてもよい。↩ "],
["section-6.html", "第6章 行列の標準形 6.1 一般固有空間への分解 6.2 一般化固有値問題 6.3 ハミルトン・ケーリーの定理の証明 6.4 ワイエルシュトラス標準形の導出", " 第6章 行列の標準形 前回までに固有値がすべて異なる場合の固有値問題を検討した. ここでは一般の場合と, 比較的高度な話題の導入を行う. 6.1 一般固有空間への分解 6.1.1 ハミルトン・ケーリーの定理 \\(X\\) を有限次元線形空間, \\(f:X\\to X\\) を線形とする. 特定の基底に関する行列表現を \\(A\\) としたとき, 特性多項式を \\[ \\phi_{A}(z)=\\det(zI-A) \\] と定義した. 基底の取り替えによって \\(f\\) の表現が \\(P^{-1}AP\\) となるようにした場合, \\[ \\phi_{P^{-1}AP}(z)=\\det(zI-P^{-1}AP)=\\det P^{-1}\\det(zI-A)\\det P=\\phi_{A}(z) \\] が成り立つことに注意すると, 固有多項式は基底に依存しないことが分かる. したがって, 行列の固有値問題はその背後にある線形写像の固有値問題である. 以下では, 固有多項式を \\(\\phi_{f}(z)\\) と書く. 定理 6.1 (ハミルトン・ケーリーの定理) \\(X\\) を有限次元線形空間, \\(f:X\\to X\\) を線形とする. このとき, 任意の \\(x\\in X\\) について \\(\\phi_{f}(f)x=0\\) が成り立つ. すなわち \\(f\\) の任意の行列表現 \\(A\\) について, \\(\\phi_{f}(A)\\) はゼロ行列となる. % 証明. 付録を参照. \\(\\dim X=n\\) とすると方程式 \\(\\phi_{f}(z)=0\\) には重複度を込めて\\(n\\) 個の解を持つ. 重複を除いた解を \\[ \\{z\\in\\mathbb{C}\\ \\mid\\ \\phi_{f}(z)=0\\}=\\{\\lambda_{1},\\dots,\\lambda_{r}\\} \\] とし, \\(n_{i}\\) を \\(\\lambda_{i}\\), \\(i=1,\\dots,r\\) の重複度とすると, \\(\\phi_{f}(z)\\) は次のように因数分解できる. \\[ \\phi_{f}(z)=(z-\\lambda_{1})^{n_{1}}\\cdots(z-\\lambda_{r})^{n_{r}}. \\] ハミルトン・ケーリーの定理により \\[ (f-\\lambda_{1})^{n_{1}}\\cdots(f-\\lambda_{r})^{n_{r}}=0 \\] が成り立つ. 各 \\((f-\\lambda_{1})^{n_{1}},\\dots,(f-\\lambda_{r})^{n_{r}}\\) は互いに可換であるから, 積 (合成) の順序が自由に交換できることに注意してほしい. 定義 6.1 多項式 \\(p(z)\\), \\(q(z)\\) が互いに素であるとは, 多項式 \\(\\tilde{p}(z)\\), \\(\\tilde{q}(z)\\), \\(c(z)\\) が \\(p(z)=\\tilde{p}(z)c(z)\\), \\(q(z)=\\tilde{q}(z)c(z)\\) であれば必ず \\(c(z)=1\\) が成り立つことをいう. 補題 6.1 多項式 \\(p(z)\\), \\(q(z)\\) が互いに素であれば, ある多項式 \\(a(z)\\), \\(b(z)\\) が存在して \\[ a(z)p(z)+b(z)q(z)=1 \\] が成り立つ. この等式をベズーの等式という. 単項イデアル整域と呼ばれる特別な環では常に成り立つ. 多項式全体が作る環は単項イデアル整域である. 身近な例としては整数の集合 \\(\\mathbb{Z}\\) は単項イデアル整域であり, 互いに素な整数\\(p,q\\)にもこの等式を満たす整数 \\(x,y\\) がある. 補題 6.2 実(複素)係数多項式 \\(p(z)\\), \\(q(z)\\) が互いに素であることの必要十分条件は, \\(\\mathbb{C}\\) において共通根をもたないことである. % 補題 6.3 \\(X\\) を次のように直和分解できる. \\[ X=\\ker(f-\\lambda_{1})^{n_{1}}\\oplus\\cdots\\oplus\\ker(f-\\lambda_{r})^{n_{r}}. \\] 各 \\(\\ker(f-\\lambda_{i})^{n_{i}}\\) は \\(f\\) の不変部分空間であり, 一般固有空間という. それらの基底に関する \\(f\\) の行列表現はブロック対角行列である. 証明. 定理の前半は, 非ゼロベクトル \\(x\\) が \\((f-\\lambda_{i})^{n_{i}}x_{1}=0\\) と \\(\\prod_{j\\neq i}(f-\\lambda_{j})^{n_{j}}x_{2}=0\\) を満たす2つのベクトルの和として一意的に表現できることを示せばよい. 多項式 \\(p(z)=(z-\\lambda_{i})^{n_{i}}\\) と \\(q(z)=\\prod_{j\\neq i}(z-\\lambda_{j})^{n_{j}}\\) は互いに素なので, ベズー等式により多項式 \\(a(z)\\), \\(b(z)\\) が存在して \\[ a(z)p(z)+b(z)q(z)=1 \\] が成立する. 任意の \\(x\\in X\\) について \\(a(z)p(z)x+b(z)q(z)x=x\\) が成り立つので, \\[ a(f)p(f)x+b(f)q(f)x=x \\] を得る. ハミルトン・ケーリーの定理により \\[ a(f)p(f)x\\in\\ker q(f),\\quad b(f)q(f)x\\in\\ker p(f) \\] なので, \\(\\ker p(f)+\\ker q(f)=X\\) が成り立つ. あとは, \\(v\\in\\ker p(f)\\cap\\ker q(f)\\) なら \\(v=0\\) であることを示せばよい. 再びハミルトン・ケーリーの定理により \\[\\begin{align} v &amp; =a(f)p(f)v+b(f)q(f)v=0. \\end{align}\\] 最後の等式は \\(v\\in\\ker p(f)\\cap\\ker q(f)\\) を使った. 次に, 各 \\(\\ker(f-\\lambda_{i})^{n_{i}}\\) は \\(f\\) の不変部分空間であることを示す. 実際, \\(x\\in\\ker(f-\\lambda_{i})^{n_{i}}\\) とすれば, \\(f(f-\\lambda_{i})^{n_{i}}x=(f-\\lambda_{i})^{n_{i}}f(x)=(f-\\lambda_{i})^{n_{i}}\\lambda_{i}x=\\lambda_{i}(f-\\lambda_{i})^{n_{i}}x=0\\). 上の様に構成した基底の下で \\(f\\) はブロック対角行列になる. [TODO: Cross-reference] % 線形写像の固有値が定める部分空間によって \\(X\\simeq\\mathbb{F}^{n}\\) を直和分解できることが分かった. \\[ \\dim X=n_{1}+\\cdots+n_{r}=n \\] だから, 固有値の代数的重複度が対応する部分空間の次元とならなければならない (証明せよ). 例 6.1 次の2つの行列 \\[ I_{2}=\\begin{bmatrix}1 &amp; 0\\\\ 0 &amp; 1 \\end{bmatrix},\\quad J_{2}(1)=\\begin{bmatrix}1 &amp; 1\\\\ 0 &amp; 1 \\end{bmatrix} \\] について \\[ \\ker(I_{2}-1)=\\ker(I_{2}-1)^{2}=\\mathbb{R}^{2} \\] である一方で \\[ \\ker(J_{2}(1)-1)\\subsetneq\\ker(J_{2}(1)-1)^{2}=\\mathbb{R}^{2} \\] である. 固有値と代数的重複度が同じでも一般固有空間の構造が同じとは限らない. 6.1.2 最小多項式 ハミルトン・ケーリーの定理より固有多項式が \\(\\phi_{f}(f)=0\\) を満たすことを確認した. しかし, \\(\\phi_{f}(z)\\) はこの性質をもつ唯一の多項式ではない. 定義 6.2 最高次の係数が 1 であり, \\(m_{f}(f)=0\\) を満たす多項式 \\(m_{f}(z)\\) の中で次数が最小のものを最小多項式という. 最小多項式は特定の行列表現に依存しないことに注意. 例 6.2 例 の記号を用いる. \\(I_{2}\\) と \\(J_{2}(1)\\) の最小多項式はそれぞれ \\[\\begin{align} m_{I_{2}}(z) &amp; =z-1,\\\\ m_{J_{2}(1)}(z) &amp; =(z-1)^{2}. \\end{align}\\] 最小多項式 \\(m_{f}\\) は\\(\\phi_{f}\\) と共通のゼロ点をもつので, 因数分解すると \\[ m_{f}(z)=(z-\\lambda_{1})^{\\tilde{n}_{1}}\\cdots(z-\\lambda_{r})^{\\tilde{n}_{r}} \\] とできる. 定理 6.2 線形写像が対角化可能であることの必要十分条件は最小多項式が重根を持たないことである. 証明. \\(f\\) が対角化可能であるとしよう. 対角成分には固有値が並ぶことから \\(m(z)=(z-\\lambda_{1})\\cdots(z-\\lambda_{r})\\) が \\(m(A)=0\\) を満たす最小多項式であることを簡単に示せる. 逆に \\(m_{f}=(z-\\lambda_{1})\\cdots(z-\\lambda_{r})\\) であれば, ある多項式 \\(p_{1}(z),\\dots,p_{r}(z)\\) が存在して, \\[ p_{1}(z)(z-\\lambda_{1})+\\cdots+p_{r}(z)(z-\\lambda_{r})=1 \\] が成り立つ (これはベズー等式を拡張したものである). したがって, \\[ \\mathbb{F}^{n}=\\ker(A-\\lambda_{1})\\oplus\\cdots\\oplus\\ker(A-\\lambda_{r}). \\] \\(\\ker(A-\\lambda_{1}),\\dots,\\ker(A-\\lambda_{r})\\) の基底 (つまり固有ベクトル) について \\(f\\) を表現すれば対角行列を得る. 補題 を強めて次のように分解できる. 証明は同様である. 定理 6.3 \\(X\\) を次のように直和分解できる. \\[ X=\\ker(f-\\lambda_{1})^{\\tilde{n}_{1}}\\oplus\\cdots\\oplus\\ker(f-\\lambda_{r})^{\\tilde{n}_{r}}. \\] 6.1.3 一般固有空間への分解 一般固有空間の構造を定めよう. 表記の簡単のため, \\(\\tilde{n}:=\\tilde{n}_{i}\\), \\(F:=f-\\lambda_{i}\\), \\(M_{k}:=\\ker F^{k}\\)とする. \\[ \\mathbb{F}^{n}=M_{\\tilde{n}}\\supsetneq M_{\\tilde{n}-1}\\supsetneq\\cdots\\supsetneq M_{1}\\supsetneq M_{0}=\\{0\\} \\] に注意する. \\(M_{\\tilde{n}_{-1}}\\) の基底に \\(V_{\\tilde{n}}=\\{v_{1}^{\\tilde{n}},\\dots,v_{r(\\tilde{n})}^{\\tilde{n}}\\}\\) を付け加えて, \\(M_{\\tilde{n}}=\\mathbb{F}^{n_{i}}\\) の基底になるようにできる. すなわち \\[ \\mathbb{F}^{n}=\\mathrm{span}V_{\\tilde{n}}\\oplus M_{\\tilde{n}-1}. \\] このとき, \\(FV_{\\tilde{n}}:=\\{Fv_{1}^{\\tilde{n}},\\dots,Fv_{r(\\tilde{n})}^{\\tilde{n}}\\}\\subset M_{\\tilde{n}-1}\\) である. これらのベクトルは1次独立であり, \\[ \\mathrm{span}FV_{\\tilde{n}}\\cap M_{\\tilde{n}-2}=\\{0\\} \\] が成り立つ. なぜなら, \\[ \\alpha_{1}Fv_{1}^{\\tilde{n}}+\\cdots+\\alpha_{r(\\tilde{n})}Fv_{r(\\tilde{n})}^{\\tilde{n}}\\in M_{\\tilde{n}-2} \\] とすれば, \\[ \\alpha_{1}v_{1}^{\\tilde{n}}+\\cdots+\\alpha_{r(\\tilde{n})}v_{r(\\tilde{n})}^{\\tilde{n}}\\in M_{\\tilde{n}-1}. \\] \\(V_{\\tilde{n}}\\) の構成により, \\(\\alpha_{1}=\\cdots=\\alpha_{r(\\tilde{n})}=0\\) となる. したがって, \\(M_{\\tilde{n}-2}\\) の基底に \\[ FV_{\\tilde{n}}\\cup\\{v_{1}^{\\tilde{n}-1},\\dots,v_{r(\\tilde{n}-1)}^{\\tilde{n}-1}\\}=:FV_{\\tilde{n}}\\cup V_{\\tilde{n}-1}=\\bigcup_{k=0}^{1}F^{1-k}V_{\\tilde{n}-k}=:W_{\\tilde{n}-1} \\] を付け加えて (\\(V_{\\tilde{n}-1}=\\emptyset\\) かもしれない, \\(0\\le r(\\tilde{n}-1)\\)), \\(M_{\\tilde{n}-1}\\) の基底を構成できる. \\(FW_{\\tilde{n}-1}\\) の元は1次独立であり, \\(FW_{\\tilde{n}-1}\\cap M_{\\tilde{n}-3}=\\{0\\}\\) が成り立つ. したがって, \\(M_{\\tilde{n}-3}\\) の基底に, \\[ W_{\\tilde{n}-2}:=FV_{\\tilde{n}-1}\\cup\\{v_{1}^{\\tilde{n}-2},\\dots,v_{r(\\tilde{n}-2)}^{\\tilde{n}-2}\\}=:FV_{\\tilde{n}-1}\\cup V_{\\tilde{n}-2} \\] を付け加えて, \\(M_{\\tilde{n}-2}\\) の基底を構成できる. 同様の手続きを \\((\\tilde{n}-1)\\) 回続けると, \\(M_{1}\\) の基底 \\[ W_{1}:=FV_{2}\\cup\\{v_{1}^{1},\\dots,v_{r(1)}^{1}\\}=:FV_{2}\\cup V_{1} \\] を得る. このようにして得た, \\[ V=\\begin{Bmatrix}F^{\\tilde{n}-1}V_{\\tilde{n}} &amp; F^{\\tilde{n}-2}V_{\\tilde{n}-1} &amp; F^{\\tilde{n}-3}V_{\\tilde{n}-2} &amp; \\cdots &amp; FV_{2} &amp; V_{1}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; V_{2}\\\\ F^{2}V_{\\tilde{n}} &amp; FV_{\\tilde{n}-1} &amp; V_{\\tilde{n}-2}\\\\ FV_{\\tilde{n}} &amp; V_{\\tilde{n}-1}\\\\ V_{\\tilde{n}} \\end{Bmatrix}\\label{eq:V} \\] は\\(\\mathbb{F}^{n}\\) の基底を成す. 基底の並べ方は, 次のように縦方向にならんでいると考えよう. \\[ V=\\left\\{ \\begin{array}{ccccc} F^{\\bullet}V_{\\tilde{n}} &amp; F^{\\bullet}V_{\\tilde{n}-1} &amp; F^{\\bullet}V_{\\tilde{n}-2} &amp; \\cdots &amp; V_{1}\\\\ \\hline 1,6 &amp; 11,15 &amp; 19,23 &amp; \\vdots\\vdots &amp; m-1,m\\\\ 2,7 &amp; 12,16 &amp; 20,24 &amp; \\vdots\\vdots\\\\ 3,8 &amp; 13,17 &amp; 21,25 &amp; \\vdots\\vdots\\\\ 4,9 &amp; 14,18 &amp; 22,26\\\\ 5,10 \\end{array}\\right\\} \\label{eq:V-1} \\] 定理 6.4 基底 \\(V\\) に関する \\(f\\) の表現行列はジョルダン標準形である. 証明. 式 の適当な1列を \\(\\{v_{1},\\dots,v_{k}\\}\\) と表そう. 基底の構成方法より, \\[ (f-\\lambda_{i})v_{s+1}=v_{s},\\quad s=1,\\dots,k-1 \\] および \\[ (f-\\lambda_{i})v_{1}=0 \\] が成り立つ. \\(\\mathrm{span}\\{v_{1},\\dots,v_{k}\\}\\) は不変部分空間を成すから, \\(f\\) の行列表現はこの典型的な例に対する表現を対角成分に並べたブロック対角行列になる. 上の関係を整理すると, \\[\\begin{multline} \\begin{bmatrix}fv_{1} &amp; fv_{2} &amp; \\cdots &amp; fv_{k-1} &amp; fv_{k}\\end{bmatrix}\\\\ =\\begin{bmatrix}v_{1} &amp; v_{2} &amp; \\cdots &amp; v_{k-1} &amp; v_{k}\\end{bmatrix}\\begin{bmatrix}\\lambda_{i} &amp; 1\\\\ &amp; \\lambda_{i} &amp; 1\\\\ &amp; &amp; \\ddots &amp; \\ddots\\\\ &amp; &amp; &amp; \\lambda_{i} &amp; 1\\\\ &amp; &amp; &amp; &amp; \\lambda_{i} \\end{bmatrix} \\end{multline}\\] を得る. これはジョルダン細胞 \\(J_{k}(\\lambda_{i})\\) に他ならない. 6.2 一般化固有値問題 固有値問題を拡張した \\[ Av=\\lambda Ev,\\quad v\\neq0,\\quad v\\in\\mathbb{C}^{n} \\] を \\((E,A)\\) に対する一般化固有値問題という. 標準的な状態方程式 \\(x_{t+1}=Ax_{t}\\) に対しては, 固有値問題 \\(Av=\\lambda v\\) が重要であったように, デスクリプタシステム \\(Ex_{t+1}=Ax_{t}\\) の分析では一般化固有値問題 \\(Av=\\lambda Ev\\) が重要な役割を果たす. 一般に因果性が成り立たないデスクリプタシステムについては, 解が未来の入力に依存することがある. 一般化固有値問題はシステムを前向き成分と後ろ向き成分に直和分解するシステマティックな方法を与えてくれる. 通常の固有値問題は, \\(A\\) が「良い形」になるような基底を探すことを目標とした. 一般化固有値問題では, \\((E,A)\\) が同時に「良い形」になるような基底を探す. 1点違いを強調しておくと, 定義域と終域ではことなる基底をとる. したがって, 変換された行列は \\((P^{-1}EQ,P^{-1}AQ)\\) によって与えられる. このような問題を扱う場合には, ペンシルと呼ばれる多項式行列 \\(zE-A\\) を考えると便利なことが多いので, 同じ問題が「ペンシルの正準形」の問題と呼ばれることもある. この節は Frank L. Lewis (1984), F. L. Lewis (1986), 片山徹 (1999) および Berger, Ilchmann, and Trenn (2012) を参考にした. 付録 B で Berger, Ilchmann, and Trenn (2012) に基づく証明を述べる. 6.2.1 一般化固有値 \\((E,A)\\) の固有多項式を \\[ \\varphi_{E,A}(z)=\\det(zE-A) \\] と定義する. 次の仮定を置く. \\(E=(e_{ij}),A=(a_{ij})\\in\\mathbb{R}^{2\\times2}\\) として固有多項式を計算してみると, \\[ \\varphi_{E,A}(z)=(\\det E)z^{2}-(e_{11}a_{22}+e_{22}a_{11}+e_{12}a_{21}+e_{21}a_{12})z+\\det A \\] が成り立つ. \\(\\det E=0\\) であれば, \\(\\varphi_{E,A}(z)\\) の次数は \\(n\\) を下回る. 一般に, \\(\\varphi_{E,A}(z)\\) が恒等的にゼロでなければ, \\(\\varphi_{E,A}(z)=0\\) は \\(d\\le n\\) 個の解を持つ. これらの解を \\((E,A)\\) の有限固有値 (finite eigenvalues) という. 有限固有値の集合を \\(\\mathrm{sp}(E,A)=\\{z\\in\\mathbb{C}\\ \\mid\\ \\varphi_{E,A}(z)=0\\}\\) と定義する. 同時に, \\(\\det E=0\\) であればゼロ固有値 (重複度 \\(n-d\\)) が存在し, \\[ Ev=0,\\quad v\\neq0,\\quad v\\in\\mathbb{C}^{n} \\] なるベクトルが存在する. これらを \\((E,A)\\) の無限大固有値 (infinite eigenvalue) という. 一般化固有ベクトルの構成はジョルダン標準形のケースとほとんど同じである. 6.2.1.0.1 1次の有限固有ベクトル \\(\\lambda_{i}\\in\\mathrm{sp}(E,A)\\) とする. \\[ (A-\\lambda_{i}E)v_{ij}^{1}=0 \\] なる非ゼロベクトル \\(v_{ij}^{1}\\), \\(j=1,\\dots,\\eta_{i}=\\dim\\ker(A-\\lambda_{i}E)\\) を1次独立に選ぶことができる. 6.2.1.0.2 \\(k\\)次の有限固有ベクトル \\(\\mathrm{span}\\{v_{i1}^{1},\\dots,v_{i\\eta_{i}}^{1}\\}\\) が\\(\\lambda_{i}\\) の代数的重複度と同じ次元を持てば, \\(\\lambda_{i}\\) に対応する一般化固有空間が完成している. この場合は行列が対角化されている. さもなくば, 各 \\(\\lambda_{i}\\in\\mathrm{sp}(E,A)\\) と各 \\(j=1,\\dots,\\eta_{i}\\) について, 高次の一般化固有ベクトル, すなわち \\[ (A-\\lambda_{i}E)v_{ij}^{k+1}=Ev_{ij}^{k},\\quad k\\ge1 \\] なる固有ベクトルを探す. ジョルダン標準形の理論ではまさに同じ手続きが重複固有値に対するジョルダン標準形の構造を決定するのであった. このようにして, \\(\\lambda_{i}\\) の代数的重複度と同じ数のベクトルの組 \\[ V_{i}=\\begin{bmatrix}v_{i1}^{1} &amp; \\cdots &amp; v_{i1}^{k_{i1}} &amp; | &amp; \\cdots &amp; \\cdots &amp; | &amp; v_{i\\eta_{i}}^{1} &amp; \\cdots &amp; v_{i\\eta_{i}}^{k_{i\\eta_{i}}}\\end{bmatrix} \\] が, 1次独立になるようにできる. 6.2.1.0.3 表現行列 各\\(v_{ij}\\), \\(j=1,\\dots,\\eta_{i}\\) に対して定まる部分行列の表現は次のようになる. \\[\\begin{multline} \\begin{bmatrix}Av_{ij}^{1} &amp; Av_{ij}^{2} &amp; \\cdots &amp; Av_{ij}^{k_{ij}-1} &amp; Av_{ij}^{k_{ij}}\\end{bmatrix}\\\\ =\\begin{bmatrix}Ev_{ij}^{1} &amp; Ev_{ij}^{2} &amp; \\cdots &amp; Ev_{ij}^{k_{ij}-1} &amp; Ev_{ij}^{k_{ij}}\\end{bmatrix}\\begin{bmatrix}\\lambda_{i} &amp; 1\\\\ &amp; \\lambda_{i} &amp; 1\\\\ &amp; &amp; \\ddots &amp; \\ddots\\\\ &amp; &amp; &amp; \\lambda_{i} &amp; 1\\\\ &amp; &amp; &amp; &amp; \\lambda_{i} \\end{bmatrix}.\\label{eq:weier_a} \\end{multline}\\] 終域の基底として, \\(\\begin{bmatrix}Ev_{ij}^{1} &amp; Ev_{ij}^{2} &amp; \\cdots &amp; Ev_{ij}^{k_{ij}-1} &amp; Ev_{ij}^{k_{ij}}\\end{bmatrix}\\) を取っていることに注意せよ. 6.2.1.0.4 1次の無限大固有ベクトル \\(E\\) のゼロ固有値に対応する固有ベクトルを求める. \\[ Ev_{\\infty j}^{1}=0, \\] \\(v_{\\infty j}^{1}\\neq0\\), \\(j=1,\\dots,\\eta=\\dim\\ker E\\). 6.2.1.0.5 \\(k\\)次の無限大固有ベクトル 非ゼロベクトルの列, \\(v_{\\infty j}^{1},\\dots,v_{\\infty j}^{k_{\\infty j}}\\) を \\[ Ev_{\\infty j}^{k+1}=Av_{\\infty j}^{k},\\quad k\\ge1 \\] が成り立つように選ぶ. 6.2.1.0.6 表現行列 各\\(v_{\\infty j}\\), \\(j=1,\\dots,\\eta_{i}\\) に対して定まる部分行列の表現は. \\[\\begin{multline} \\begin{bmatrix}Ev_{\\infty j}^{1} &amp; Ev_{\\infty j}^{2} &amp; \\cdots &amp; Ev_{\\infty j}^{k_{\\infty j}-1} &amp; Ev_{\\infty j}^{k_{\\infty j}}\\end{bmatrix}\\\\ =\\begin{bmatrix}Av_{\\infty j}^{1} &amp; Av_{\\infty j}^{2} &amp; \\cdots &amp; Av_{\\infty j}^{k_{\\infty j}-1} &amp; Av_{\\infty j}^{k_{\\infty j}}\\end{bmatrix}\\begin{bmatrix}0 &amp; 1\\\\ &amp; 0 &amp; 1\\\\ &amp; &amp; \\ddots &amp; \\ddots\\\\ &amp; &amp; &amp; 0 &amp; 1\\\\ &amp; &amp; &amp; &amp; 0 \\end{bmatrix}.\\label{eq:weier_e} \\end{multline}\\] 終域の基底として \\(\\begin{bmatrix}Av_{\\infty j}^{1} &amp; Av_{\\infty j}^{2} &amp; \\cdots &amp; Av_{\\infty j}^{k_{\\infty j}-1} &amp; Av_{\\infty j}^{k_{\\infty j}}\\end{bmatrix}\\) を取っていることに注意せよ. 定理 6.5 (ワイエルシュトラス形式) \\((E,A)\\) はレギュラーであるとする. このとき, 上のように得た非ゼロベクトルを並べた行列 \\[ \\bar{V}=[v_{ij}^{k}\\ |\\ v_{\\infty j}^{k}],\\quad\\bar{W}=[Ev_{ij}^{k}\\ \\mid Av_{\\infty j}^{k}] \\] は正則であり, \\[\\begin{align} \\bar{W}^{-1}E\\bar{V} &amp; =\\begin{bmatrix}I\\\\ &amp; N \\end{bmatrix},\\label{eq:e}\\\\ \\bar{W}^{-1}A\\bar{V} &amp; =\\begin{bmatrix}J\\\\ &amp; I \\end{bmatrix},\\label{eq:a} \\end{align}\\] ただし, \\(N\\) は優対角成分にのみ 1をもつべきゼロ行列. \\(J\\) は有限固有値を対角成分にもつジョルダン標準形行列である. 証明. 付録を参照. デスクリプタシステムについて次の結果を得る. 定理 6.6 \\((E,A)\\) をレギュラーとする. デスクリプタシステム \\[ Ex_{t+1}=Ax_{t}+Bu_{t}\\label{eq:desc} \\] を前向きのシステム方程式と, 後ろ向きのシステム方程式に分解することができる. すなわち, 適当な変数変換のもとで, \\[\\begin{align} \\hat{x}_{t+1}^{f} &amp; =J\\hat{x}_{t}^{f}+B^{f}u_{t},\\label{eq:forward}\\\\ \\hat{x}_{t}^{b} &amp; =N\\hat{x}_{t+1}^{b}+B^{b}u_{t}.\\label{eq:backward} \\end{align}\\] と分解できる. 証明. 式 をワイエルシュトラス形式に変換しよう. \\[ (W^{-1}EV)(V^{-1}x_{t+1})=(W^{-1}AV)(V^{-1}x_{t})+W^{-1}Bu_{t}. \\] \\(\\hat{x}_{t}:=V^{-1}x_{t}\\), \\(\\hat{x}_{t}=(\\hat{x}_{t}^{f},\\hat{x}_{t}^{b})\\), \\(W^{-1}B=(B^{f},-B^{b})\\) の分解を式, と適合するようにすれば \\[ \\begin{bmatrix}I\\\\ &amp; N \\end{bmatrix}\\begin{bmatrix}\\hat{x}_{t+1}^{f}\\\\ \\hat{x}_{t+1}^{b} \\end{bmatrix}=\\begin{bmatrix}J\\\\ &amp; I \\end{bmatrix}\\begin{bmatrix}\\hat{x}_{t}^{f}\\\\ \\hat{x}_{t}^{b} \\end{bmatrix}+\\begin{bmatrix}B^{f}\\\\ -B^{b} \\end{bmatrix}u_{t}. \\] これを整理すれば結果の式を得る. % レギュラーなデスクリプタシステムは, \\(\\hat{x}^{f}\\) の初期条件と, \\(\\hat{x}^{b}\\) の終端条件, および \\(u_{t}\\) によって解軌道が一意的に定まる. これは Luenberger (1977) の結果である. % 6.3 ハミルトン・ケーリーの定理の証明 \\(A\\) を\\(n\\times n\\) 行列とする. \\(A\\) の第\\(i\\)行目と第\\(j\\)行目を除いた \\((n-1)\\times(n-1)\\) 行列の行列式に \\((-1)^{i+j}\\) を掛けたものを\\(A\\)の \\((i,j)\\) 余因子といい, \\(\\Delta_{i,j}\\) と記す. 任意の \\(i=1,\\dots,n\\) と \\(j=1,\\dots,n\\) に対して次が成り立つ. \\[ \\begin{aligned} \\det A &amp; =a_{i1}\\Delta_{i1}+\\cdots+a_{in}\\Delta_{in}\\\\ \\det A &amp; =a_{1j}\\Delta_{1j}+\\cdots+a_{nj}\\Delta_{nj}. \\end{aligned} \\] 証明. 練習問題とする. % \\(A\\) の随伴行列 (adjugate matrix) を \\[ \\mathrm{adj}A=\\begin{bmatrix}\\Delta_{11} &amp; \\Delta_{21} &amp; \\cdots &amp; \\Delta_{n1}\\\\ \\Delta_{12} &amp; \\Delta_{22} &amp; \\cdots &amp; \\Delta_{n2}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\Delta_{1n} &amp; \\Delta_{2n} &amp; \\cdots &amp; \\Delta_{nn} \\end{bmatrix} \\] と定義する. このとき, 次の結果が得られる. 定理 6.7 任意の \\(A\\in\\mathbb{R}^{n\\times n}\\) に対して次が成り立つ. \\[ A\\left(\\mathrm{adj}A\\right)=\\left(\\mathrm{adj}A\\right)A=\\left(\\det A\\right)I_{n\\times n}. \\] 証明. 行列積の定義より \\[ \\begin{aligned} \\left[A\\left(\\mathrm{adj}A\\right)\\right]_{ij} &amp; =\\sum_{k=1}^{n}A_{ik}\\left(\\mathrm{adj}A\\right)_{kj}\\\\ &amp; =\\sum_{k=1}^{n}a_{ik}\\Delta_{jk}. \\end{aligned} \\] \\(i=j\\) のときは事実 より \\(\\det A\\) に一致する. \\(i\\neq j\\) としよう. \\(\\Delta_{j1},\\dots,\\Delta_{jn}\\) は\\(A\\) の \\(j\\) 行目を取り除いて作られるので, \\(A\\) の \\(j\\) 行目を変更してもこれらの小行列式は不変である. したがって, \\(A\\) の \\(j\\) 行目を \\(i\\)行目と同じ値をもつようにしてもよい. このように作った \\(\\tilde{A}\\) は \\(\\det\\tilde{A}=0\\) が成り立つ. この行列を\\(j\\) 行目に関して展開すると, \\[ 0=\\det\\tilde{A}=\\sum_{k=1}^{n}a_{jk}\\Delta_{jk}=\\sum_{k=1}^{n}a_{ik}\\Delta_{jk} \\] を得る. \\(\\left(\\mathrm{adj}A\\right)A=\\left(\\det A\\right)I_{n\\times n}\\) についても同様に証明できる. % 定理 ef{thm:HC} の証明 定理 より \\[ \\begin{aligned} \\det(zI-A)\\cdot I &amp; =(zI-A)\\left[\\mathrm{adj}(zI-A)\\right]\\\\ &amp; =\\left[\\mathrm{adj}(zI-A)\\right](zI-A) \\end{aligned} \\] が成り立つ. \\(\\mathrm{adj}(zI-A)\\) は高々 \\((n-1)\\)次の多項式を成分とする行列なので, \\(\\left[\\mathrm{adj}(zI-A)\\right]_{ij}=\\Delta_{ji}=b_{ij,0}+b_{ij,1}z+\\cdots+b_{ij,n-1}z^{n-1}\\) と書ける. 整理すると \\[ \\mathrm{adj}(zI-A)=B_{0}+B_{1}z+\\cdots+B_{n-1}z^{n-1}. \\] \\(A\\) と 各 \\(B_{0},\\dots,B_{n-1}\\) が可換であることに注意しよう. \\[ \\begin{aligned} \\phi_{A}(z) &amp; =z\\left(B_{0}+B_{1}z+\\cdots+B_{n-1}z^{n-1}\\right)-A\\left(B_{0}+B_{1}z+\\cdots+B_{n-1}z^{n-1}\\right)\\\\ &amp; =\\left(B_{0}z+B_{1}z^{2}+\\cdots+B_{n-1}z^{n}\\right)-\\left(B_{0}A+B_{1}Az+\\cdots+B_{n-1}Az^{n-1}\\right)\\\\ &amp; =-B_{0}A+\\left(B_{0}-B_{1}A\\right)z+\\cdots+\\left(B_{n-2}-B_{n-1}A\\right)z^{n-1}+B_{n-1}z^{n}. \\end{aligned} \\] ここで \\(z=A\\) を代入すると, \\(\\phi_{A}(A)=0\\) が成り立つことが分かる. % 6.4 ワイエルシュトラス標準形の導出 Berger, Ilchmann, and Trenn (2012) (以下, {[}BIT{]})の議論に沿ってワイエルシュトラス標準形を導出しよう. 以下で定義する部分空間列は, Wong (1974) で導入された. 極限空間がデスクリプタシステムの分析に重要な役割を果たすことが知られている. {[}@Lewis1984{]} 定義 6.3 (Wong sequences) \\[ \\begin{array}{cccccccc} \\mathcal{V}_{k+1} &amp; := &amp; A^{-1}(E\\mathcal{V}_{k}), &amp; \\quad &amp; \\mathcal{V}_{0}=\\mathbb{F}^{n}, &amp; \\quad &amp; k=1,2,\\dots\\\\ \\mathcal{W}_{\\ell+1} &amp; := &amp; E^{-1}(A\\mathcal{W}_{\\ell}), &amp; \\quad &amp; \\mathcal{W}_{0}=\\{0\\}, &amp; \\quad &amp; \\ell=1,2,\\dots \\end{array} \\] ウォン列は単調列であり, 有限ステップで収束する. すなわち, ある \\(k^{*}\\), \\(\\ell^{*}\\) が存在して, \\[ \\begin{array}{ccccccccccc} \\mathcal{V}_{0} &amp; \\supsetneq &amp; \\mathcal{V}_{1} &amp; \\supsetneq &amp; \\cdots &amp; \\supsetneq &amp; \\mathcal{V}_{k} &amp; \\to &amp; \\mathcal{V}_{k^{*}} &amp; =: &amp; \\mathcal{V}^{*}\\\\ \\mathcal{W}_{0} &amp; \\subsetneq &amp; \\mathcal{W}_{1} &amp; \\subsetneq &amp; \\cdots &amp; \\subsetneq &amp; \\mathcal{W}_{\\ell} &amp; \\to &amp; \\mathcal{W}_{\\ell^{*}} &amp; =: &amp; \\mathcal{W}^{*} \\end{array} \\] が成り立つ. また, \\[ \\begin{array}{ccc} \\mathcal{V}^{*} &amp; = &amp; A^{-1}(E\\mathcal{V}^{*})\\\\ \\mathcal{W}^{*} &amp; = &amp; E^{-1}(A\\mathcal{W}^{*}) \\end{array} \\] および, その帰結として \\[ \\begin{array}{ccc} A\\mathcal{V}^{*} &amp; \\subset &amp; E\\mathcal{V}^{*}\\\\ E\\mathcal{W}^{*} &amp; \\subset &amp; A\\mathcal{W}^{*} \\end{array} \\] が成り立つ. 証明. \\(\\mathbb{F}^{n}=\\mathcal{V}_{0}\\supset\\mathcal{V}_{1}\\) は明らか. \\(\\mathcal{V}_{k-1}\\supset\\mathcal{V}_{k}\\) が成り立つとすれば, \\[ \\begin{aligned} v\\in\\mathcal{V}_{k+1} &amp; \\Longrightarrow Av\\in E\\mathcal{V}_{k}\\\\ &amp; \\Longrightarrow\\exists w\\in\\mathcal{V}_{k}\\ \\text{s.t.}\\ Av=Ew\\\\ &amp; \\Longrightarrow\\exists w\\in\\mathcal{V}_{k-1}\\ \\text{s.t.}\\ Av=Ew\\\\ &amp; \\Longrightarrow Av\\in E\\mathcal{V}_{k-1}\\\\ &amp; \\Longrightarrow v\\in A^{-1}(E\\mathcal{V}_{k-1})=\\mathcal{V}_{k}. \\end{aligned} \\] 数学的帰納法により, 任意の \\(k\\) について \\(\\mathcal{V}_{k}\\supset\\mathcal{V}_{k+1}\\) が成り立つ. 同様に, \\(\\mathcal{W}_{k}\\subset\\mathcal{W}_{k+1}\\) も証明できる. % 定義 6.4 \\(\\mathcal{V}^{*}\\) を初期値部分空間 (initial manifold), \\(\\mathcal{W}^{*}\\) を終端値部分空間 (final manifold) とよぶ. 補題 6.4 ({Lemma 2.3, {[}BIT{]}}) \\((E,A)\\) をレギュラーとする. このとき, 任意の \\(\\lambda\\in\\mathbb{F}\\setminus\\mathrm{sp}(E,A)\\) と \\(i=1,2,\\dots\\) に対して, \\[\\begin{align} \\mathcal{V}_{k} &amp; =\\mathrm{im}\\left((A-\\lambda E)^{-1}E\\right)^{k}\\label{eq:vs}\\\\ \\mathcal{W}_{k} &amp; =\\ker\\left((A-\\lambda E)^{-1}E\\right)^{k}\\label{eq:ws} \\end{align}\\] が成り立つ. 特に, \\[ \\dim\\mathcal{V}_{k}+\\dim\\mathcal{W}_{k}=n. \\] 証明. \\(\\lambda\\in\\mathbb{F}\\setminus\\mathrm{sp}(E,A)\\) を固定する. 記号の簡単化のため \\(\\hat{E}:=(A-\\lambda E)^{-1}E\\) と記す. まず, を帰納法で示す. \\(k=0\\) のとき, \\[ \\mathcal{V}_{0}=\\mathbb{F}^{n}=\\mathrm{im}\\hat{E}^{0} \\] は自明である. \\(\\mathcal{V}_{k}=\\mathrm{im}\\hat{E}^{k}\\) が成り立つとしよう. \\(\\mathcal{V}_{k+1}\\subset\\mathrm{im}\\hat{E}^{k+1}\\) と \\(\\mathcal{V}_{k+1}\\supset\\mathrm{im}\\hat{E}^{k+1}\\) を示せばよい. {[}\\(\\mathcal{V}_{k+1}\\subset\\mathrm{im}\\hat{E}^{k+1}\\){]} \\(v\\in\\mathcal{V}_{k+1}\\) とする. \\(w\\in\\mathcal{V}_{k}\\) を \\(Av=Ew\\) が成り立つように選ぶ. \\[ (A-\\lambda E)v=E(w-\\lambda v) \\] であるから, \\[ \\begin{aligned} v &amp; =(A-\\lambda E)^{-1}E(w-\\lambda v)=\\hat{E}(w-\\lambda v)\\in\\mathrm{im}\\hat{E}^{k+1}. \\end{aligned} \\] {[}\\(\\mathcal{V}_{k+1}\\supset\\mathrm{im}\\hat{E}^{k+1}\\){]} \\(v\\in\\mathrm{im}\\hat{E}^{k+1}\\) としよう. ある \\(w\\in\\mathrm{im}\\hat{E}^{k}\\) が存在して, \\(v=\\hat{E}w=(A-\\lambda E)^{-1}Ew\\) が成り立つ. したがって, \\[ Av=E(w+\\lambda v). \\] \\(w+\\lambda v\\in\\mathcal{V}_{k}\\) だから, \\(v\\in\\mathcal{V}_{k+1}\\) がしたがう. 次に を帰納法で示す. \\(k=0\\) のとき, \\(\\mathcal{W}_{0}=\\{0\\}=\\ker\\left((A-\\lambda E)^{-1}E\\right)^{0}\\) が成り立つ. \\(\\mathcal{W}_{k}=\\ker\\hat{E}^{k}\\) が成り立つとしよう. {[}\\(\\mathcal{W}_{k+1}\\subset\\ker\\hat{E}^{k+1}\\){]} \\(v\\in\\mathcal{W}_{k+1}\\) とする. \\(Ev=Aw\\) なる \\(w\\in\\mathcal{W}_{k}\\) が存在する. \\[ \\begin{aligned} (A-\\lambda E)w &amp; =E(v-\\lambda w)\\\\ w &amp; =\\hat{E}(v-\\lambda w)\\\\ \\hat{E}v= &amp; w+\\lambda\\hat{E}w\\in\\ker\\hat{E}^{k}. \\end{aligned} \\] したがって, \\(v\\in\\ker\\hat{E}^{k+1}\\). {[}\\(\\mathcal{W}_{k+1}\\supset\\ker\\hat{E}^{k+1}\\){]} \\(v\\in\\ker\\hat{E}^{k+1}\\) としよう. \\[ \\hat{E}^{k}\\left(\\hat{E}v\\right)=0, \\] \\(w=\\hat{E}v\\) とすれば, \\(w\\in\\ker\\hat{E}^{k}=\\mathcal{W}_{k}\\subset\\mathcal{W}_{k+1}\\). \\[ (A-\\lambda E)w=Ev \\] したがって, \\(E(v+\\lambda w)=Aw\\), 定義により \\(v+\\lambda w\\in\\mathcal{W}_{k+1}\\). \\(\\lambda w\\in\\mathcal{W}_{k+1}\\) より, \\(v\\in\\mathcal{W}_{k+1}\\). % 定理 6.8 ({Proposition 2.4, {[}BIT{]}}) \\((E,A)\\) をレギュラーとする. \\(k^{*}=\\ell^{*}\\), \\(\\mathcal{V}^{*}\\oplus\\mathcal{W}^{*}=\\mathbb{F}^{n}\\), \\(\\ker E\\cap\\mathcal{V}^{*}=\\{0\\}\\), \\(\\ker A\\cap\\mathcal{W}^{*}=\\{0\\}\\), and \\(\\ker E\\cap\\ker A=\\{0\\}\\). 証明. 練習問題とする. 準ワイエルシュトラス形式と呼ばれる次の正準形が得られる. 定理 6.9 ({Theorem 2.6, {[}BIT{]}}) \\((E,A)\\) をレギュラーとする. \\(n_{1}:=\\dim\\mathcal{V}^{*}\\), \\(n_{2}:=\\dim\\mathcal{W}^{*}\\) とし, 行列 \\(V\\in\\mathbb{F}^{n\\times n_{1}}\\), \\(W\\in\\mathbb{F}^{n\\times n_{1}}\\) を \\[ \\begin{aligned} \\mathcal{V}^{*} &amp; =\\mathrm{im}V,\\qquad\\mathcal{W}^{*}=\\mathrm{im}W \\end{aligned} \\] が成り立つように選ぶ. このとき, \\([V\\ W]\\) および \\([EV\\ AW]\\) は正則であり, \\(J\\in\\mathbb{F}^{n_{1}\\times n_{1}}\\) と \\(N\\in\\mathbb{F}^{n_{2}\\times n_{2}}\\) に対して \\[\\begin{align} [EV\\ AW]^{-1}E[V\\ W] &amp; =\\begin{bmatrix}I_{n_{1}}\\\\ &amp; N \\end{bmatrix},\\label{eq:quasi0}\\\\{} [EV\\ AW]^{-1}A[V\\ W] &amp; =\\begin{bmatrix}J\\\\ &amp; I_{n_{2}} \\end{bmatrix}\\label{eq:quasi} \\end{align}\\] が成り立つ. さらに, \\(\\tilde{N}\\) はべきゼロ行列であり, \\(N^{k^{*}}=0\\) が成り立つ. % , は \\[ AV=EVJ,\\quad EW=AWN \\] と同値であることが容易に確認できる. 証明. \\([V\\ W]\\) が正則であることは, \\(\\mathcal{V}^{*}=\\mathrm{im}V\\) と \\(\\mathcal{W}^{*}=\\mathrm{im}W\\) および \\(\\mathcal{V}^{*}\\oplus\\mathcal{W}^{*}=\\mathbb{F}^{n}\\) より直接従う. \\([EV\\ AW]\\) の正則性を示す. ある\\(\\xi_{1}\\in\\mathbb{F}^{n_{1}}\\), \\(\\xi_{2}\\in\\mathbb{F}^{n_{2}}\\) に対して \\[ \\begin{bmatrix}EV &amp; AW\\end{bmatrix}\\begin{bmatrix}\\xi_{1}\\\\ \\xi_{2} \\end{bmatrix}=0 \\] が成り立つとしよう. \\(V\\xi_{1}\\in\\mathcal{V}^{*}\\cap\\ker E\\) と \\(W\\xi_{2}\\in\\mathcal{W}^{*}\\cap\\ker A\\) および定理 (3) より, \\[ \\begin{bmatrix}V &amp; W\\end{bmatrix}\\begin{bmatrix}\\xi_{1}\\\\ \\xi_{2} \\end{bmatrix}=0. \\] \\([V\\ W]\\) は正則なので, \\(\\xi_{1}=0\\), \\(\\xi_{2}=0\\) が従う. \\([V\\ W]\\) および \\([EV\\ AW]\\) がともに \\(\\mathbb{F}^{n}\\) の基底であるから, 定義域の基底を \\([V\\ W]\\), 終域の基底を \\([EV\\ AW]\\) に取り替えたときの行列表現は, ブロック対角行列になっているはずである. すなわち, \\[ \\begin{aligned} A[V\\ W] &amp; =[EV\\ AW]\\begin{bmatrix}J\\\\ &amp; I_{n_{2}} \\end{bmatrix}\\\\ E[V\\ W] &amp; =[EV\\ AW]\\begin{bmatrix}I_{n_{1}}\\\\ &amp; N \\end{bmatrix} \\end{aligned} \\] なる \\(J\\) および \\(N\\) が存在する. あとは, \\(N\\) がべきゼロであることを示せばよい. 実は, \\[ \\mathrm{im}WN^{k}\\subset\\mathcal{W}_{k^{*}-k},\\quad k=0,1,\\dots,k^{*}\\label{eq:wnk} \\] が成り立つ. これを示すことができれば, \\(\\mathrm{im}WN^{k^{*}}\\subset\\mathcal{W}_{0}=\\{0\\}\\) が成り立つ. \\(W\\) は列フルランクなので単射, したがって \\(N^{k^{*}}=0\\) が従う. 式 を帰納法で示そう. \\(k=0\\) に対しては, \\(\\mathrm{im}WN^{0}=\\mathrm{im}W=\\mathcal{W}^{*}\\) より明らか. \\(k=s\\) で \\[ \\mathrm{im}WN^{s}\\subset\\mathcal{W}_{k^{*}-s} \\] が成り立つとしよう. \\[ \\begin{aligned} y\\in\\mathrm{im}WN^{s+1} &amp; \\Longrightarrow\\exists x\\ \\text{s.t.}\\ y=WN^{s+1}x\\\\ &amp; \\Longrightarrow\\exists x\\ \\text{s.t.}\\ Ay=AWN\\cdot N^{s}x\\\\ &amp; \\Longrightarrow\\exists x\\ \\text{s.t.}\\ Ay=EW\\cdot N^{s}x &amp; &amp; (AWN=EW)\\\\ &amp; \\Longrightarrow Ay\\in E\\mathcal{W}_{k^{*}-s}\\\\ &amp; \\Longrightarrow Ay\\in A\\mathcal{W}_{k^{*}-s-1} &amp; &amp; (E\\mathcal{W}_{k+1}\\subset A\\mathcal{W}_{k})\\\\ &amp; \\Longrightarrow y\\in\\mathcal{W}_{k^{*}-s-1} \\end{aligned} \\] 最後のステップは次のように証明できる. 今, \\(\\bar{y}\\not\\in\\mathrm{im}WN^{s+1}\\setminus\\mathcal{W}_{k^{*}-s-1}\\) が存在して, \\(Ay=A\\bar{y}\\in A\\mathcal{W}_{k^{*}-s-1}\\) が成り立つとしよう. \\(y-\\bar{y}\\in\\ker A\\cap\\mathrm{im}WN^{s+1}\\) でなければならないが, \\(\\mathrm{im}WN^{s+1}\\subset\\mathrm{im}W=\\mathcal{W}^{*}\\) および, \\(\\ker A\\cap\\mathcal{W}^{*}=\\{0\\}\\) であったので, \\(\\bar{y}=y\\) が成り立たなければならない. したがって, 上のような \\(\\bar{y}\\) を選ぶことはできない. よって, \\(\\mathrm{im}WN^{s+1}\\subset\\mathcal{W}_{k^{*}-s-1}\\)が成り立つ. 定義 6.5 (一般化固有空間) \\(\\lambda\\in\\mathrm{sp}(E,A)\\cup\\{\\infty\\}\\) に対して固有空間列 \\(\\mathcal{G}_{\\lambda}^{1},\\mathcal{G}_{\\lambda}^{2},\\dots\\) を次のように定義する. まず, \\[ \\begin{aligned} \\mathcal{G}_{\\lambda}^{0}: &amp; =\\{0\\},\\\\ \\mathcal{G}_{\\lambda}^{k+1} &amp; :=\\begin{cases} (A-\\lambda E)^{-1}\\left(E\\mathcal{G}_{\\lambda}^{k}\\right) &amp; \\text{if}\\ \\lambda\\in\\mathrm{sp}(E,A)\\\\ E^{-1}\\left(A\\mathcal{G}_{\\lambda}^{k}\\right) &amp; \\text{if}\\ \\lambda=\\infty. \\end{cases} \\end{aligned} \\] ペンシル \\((E,A)\\) の \\(\\lambda\\) に対応する一般化固有空間を \\[ \\mathcal{G}_{\\lambda}=\\bigcup_{k=1}^{\\infty}\\mathcal{G}_{\\lambda}^{k} \\] と定義する. % 代数的重複度, 幾何的重複度など通常の固有値問題と同様に定義する. 固有多項式 \\[ \\varphi_{E,A}(z)=\\prod_{\\lambda\\in\\mathrm{spec}(E,A)}(z-\\lambda)^{n_{\\lambda}} \\] の各因数にかかる指数が有限固有値の代数的重複度であり, 次のように書く. \\[ \\mathrm{am}(\\lambda):=n_{\\lambda},\\qquad\\lambda\\in\\mathrm{spec}(E,A). \\] 無限大固有値の代数的重複度は, \\[ \\mathrm{am}(\\infty):=n-\\sum_{\\lambda\\in\\mathrm{spec}(E,A)}n_{\\lambda}=n-d. \\] 最後に, \\[ \\begin{aligned} \\mathrm{gm}(\\lambda) &amp; :=\\dim\\mathcal{G}_{\\lambda}^{1},\\qquad\\lambda\\in\\mathrm{spec}(E,A)\\cup\\{\\infty\\} \\end{aligned} \\] を幾何的重複度と呼ぶ. % 補題 6.5 各固有値 \\(\\lambda\\in\\mathrm{sp}(E,A)\\cup\\{\\infty\\}\\) に対して, ある \\(p(\\lambda)\\in\\mathbb{N}\\) が存在して, \\[ \\mathcal{G}_{\\lambda}^{0}\\subsetneq\\mathcal{G}_{\\lambda}^{1}\\subsetneq\\mathcal{G}_{\\lambda}^{2}\\subsetneq\\cdots\\subsetneq\\mathcal{G}_{\\lambda}^{p(\\lambda)}=\\mathcal{G}_{\\lambda}^{p(\\lambda)+1} \\] が成り立つ. 証明. 拡大列が有限であることは, 有限次元空間の部分空間であることから明らか. 包含関係を示そう. \\[ \\mathcal{G}_{\\lambda}^{k-1}\\subset\\mathcal{G}_{\\lambda}^{k},\\quad k\\in\\mathbb{N} \\] を数学的帰納法によって示す. \\(k=0\\) のときは明らか. \\(k=s\\) で成り立つとしよう. まず, \\(\\lambda\\in\\mathrm{sp}(E,A)\\) の場合, \\[ \\begin{aligned} v\\in\\mathcal{G}_{\\lambda}^{s} &amp; \\Longrightarrow v\\in(A-\\lambda E)^{-1}\\left(E\\mathcal{G}_{\\lambda}^{s-1}\\right)\\\\ &amp; \\Longrightarrow(A-\\lambda E)v\\in E\\mathcal{G}_{\\lambda}^{s-1}\\\\ &amp; \\Longrightarrow(A-\\lambda E)v\\in E\\mathcal{G}_{\\lambda}^{s}\\\\ &amp; \\Longrightarrow v\\in(A-\\lambda E)^{-1}\\left(E\\mathcal{G}_{\\lambda}^{s}\\right)\\\\ &amp; \\Longrightarrow v\\in\\mathcal{G}_{\\lambda}^{s+1}. \\end{aligned} \\] \\(\\lambda=\\infty\\) の場合も同様に示すことができる. % 本編で用いた表現との対応について確認しておこう. \\(\\{\\lambda_{1},\\dots,\\lambda_{r}\\}=\\mathrm{sp}(E,A)\\) とする. まず, 各 \\(i=1,\\dots,r\\) に対して1次独立なベクトル \\[ v_{i1}^{1},\\cdots,v_{i\\eta_{i}}^{1} \\] を \\[ (A-\\lambda E)v_{ij}^{1}=0,\\qquad j=1,\\dots,\\eta_{i}\\label{eq:g1} \\] となるように選んだ. \\(\\mathcal{G}_{\\lambda_{i}}^{1}=(A-\\lambda E)^{-1}\\left(E\\mathcal{G}_{\\lambda_{i}}^{0}\\right)=(A-\\lambda E)^{-1}(\\{0\\})=\\ker(A-\\lambda E)\\) だから, は \\[ v_{ij}^{1}\\in\\mathcal{G}_{\\lambda_{i}}^{1},\\qquad j=1,\\dots,\\eta_{i} \\] と同値である. 次のステップでは, \\[ (A-\\lambda E)v_{ij}^{2}=Ev_{ij}^{1},\\qquad j=1,\\dots,\\eta_{i} \\] なる, \\(v_{ij}^{2}\\) を探した. これはもちろん \\[ v_{ij}^{2}\\in\\mathcal{G}_{\\lambda_{i}}^{2},\\qquad j=1,\\dots,\\eta_{i} \\] である. 同様の手続きで, \\[ v_{ij}^{k}\\in\\mathcal{G}_{\\lambda_{i}}^{k},\\qquad j=1,\\dots,\\eta_{i},\\ k=1,\\dots,k_{ij} \\] を見つけるのが, 本編で述べたアルゴリズムの骨子である. 各 \\(i\\) と \\(j\\) についてこの手続は有限で終了するのは, 補題 により \\(p(\\lambda_{i})\\) が存在することの帰結である. つまり, \\(k_{ij}\\le p(\\lambda_{i})\\) が成り立つ. 容易に確かめられるように無限大固有値 \\(\\lambda=\\infty\\) のときも同様である. このようにして得られたベクトル列 \\((v_{ij}^{1},v_{ij}^{2},\\dots,v_{ij}^{k})\\), \\(k\\leq k_{ij}\\), を固有値 \\(\\lambda_{i}\\in\\mathrm{sp}(E,A)\\) に対応する固有ベクトル鎖と呼ぶ. 無限大固有値に対する固有ベクトル鎖 \\((v_{\\infty j}^{1},v_{\\infty j}^{2},\\dots,v_{\\infty j}^{k_{\\infty j}})\\), \\(k\\leq k_{\\infty j}\\), も同様に定義する. \\(\\lambda=\\infty\\) を \\(i=\\infty\\) と読み替えれば \\(\\lambda\\in\\mathrm{sp}(E,A)\\cup\\{\\infty\\}\\) を添字 \\(i=1,\\dots,r,\\infty\\) を用いて統一的に扱うことができる. 次の定理では下付き添字を省略している. 補題 6.6 任意の \\(k\\in\\mathbb{N}\\cup\\{0\\}\\) について, \\[ \\mathcal{G}_{\\lambda}^{k}=\\begin{cases} V\\ker(J-\\lambda I)^{k} &amp; \\text{if}\\quad\\lambda\\in\\mathrm{sp}(E,A)\\\\ W\\ker N^{k}=\\mathcal{W}_{k} &amp; \\text{if}\\quad\\lambda=\\infty \\end{cases} \\] が成り立つ. 証明. 定理 により, 行列 \\(V\\in\\mathbb{F}^{n\\times n_{1}}\\), \\(W\\in\\mathbb{F}^{n\\times n_{2}}\\) が存在して \\([V\\ W]\\in\\mathbb{F}^{n\\times n}\\) は正則で \\[ AV=EVJ,\\quad EW=AWN \\] が成り立つことに注意する. . \\(k=0\\) のときは \\(\\mathcal{G}_{\\lambda}^{0}=\\{0\\}=V\\ker(J-\\lambda I)^{0}\\) より明らか. ある \\(s\\) について \\(\\mathcal{G}_{\\lambda}^{s}=V\\ker(J-\\lambda I)^{s}\\) が成り立つとしよう. \\(v^{s+1}\\), \\(v^{s}\\) を \\[ v^{s+1}\\in\\mathcal{G}_{\\lambda}^{s+1}\\setminus\\{0\\},\\qquad v^{s}\\in\\mathcal{G}_{\\lambda}^{s},\\quad\\text{s.t.}\\quad(A-\\lambda E)v^{s+1}=Ev^{s} \\] が成り立つように選ぶ. 命題 (2) によって, \\(\\xi_{1}\\in\\mathbb{F}^{n_{1}}\\), \\(\\xi_{2}\\in\\mathbb{F}^{n_{2}}\\) が一意的に存在して \\[ v^{s+1}=V\\xi_{1}+W\\xi_{2} \\] が成り立つことが分かる. \\[ \\begin{aligned} (A-\\lambda E)v^{s+1}=Ev^{s} &amp; \\Leftrightarrow(A-\\lambda E)(V\\xi_{1}+W\\xi_{2})=Ev^{s}\\\\ &amp; \\Leftrightarrow AV\\xi_{1}+AW\\xi_{2}-\\lambda EV\\xi_{1}-\\lambda EW\\xi_{2}=Ev^{s}\\\\ &amp; \\Leftrightarrow EVJ\\xi_{1}+AW\\xi_{2}-\\lambda EV\\xi_{1}-\\lambda AWN\\xi_{2}=Ev^{s}\\\\ &amp; \\Leftrightarrow EV(J\\xi_{1}-\\lambda I)\\xi_{1}+AW(I-\\lambda N)\\xi_{2}=Ev^{s}\\\\ &amp; \\Leftrightarrow AW(I-\\lambda N)\\xi_{2}=Ev^{s}+EV(\\lambda I-J)\\xi_{1}. \\end{aligned} \\] 帰納法の仮説より, \\(v^{s}\\in\\mathcal{G}_{\\lambda}^{s}\\subset V\\ker(J-\\lambda I)^{s}\\) が成り立つ. さらに, \\(\\mathrm{im}V=\\mathcal{V}^{*}\\) なので, \\[ Ev^{s}\\in E\\mathcal{V}^{*},\\quad EV(\\lambda I-J)\\xi_{1}\\in E\\mathcal{V}^{*}. \\] したがって, \\[ W(I-\\lambda N)\\xi_{2}\\in A^{-1}(E\\mathcal{V}^{*})=\\mathcal{V}^{*}. \\] \\(\\mathcal{W}^{*}=\\mathrm{im}W\\) なので, 実は \\[ W(I-\\lambda N)\\xi_{2}\\in\\mathcal{V}^{*}\\cap\\mathcal{W}^{*}=\\{0\\}. \\] \\(W\\) は単射なので, \\((I-\\lambda N)\\xi_{2}=0\\), すなわち, \\(\\xi_{2}=\\lambda N\\xi_{2}\\). \\(N\\) はべきゼロなので, \\[ \\xi_{2}=\\lambda N\\xi_{2}=\\lambda^{2}N^{2}\\xi_{2}=\\cdots=\\lambda^{k^{*}}N^{k^{*}}\\xi_{2}=0. \\] 一方, \\(v^{s}\\in V\\ker(J-\\lambda I)^{s}\\) としているので, ある \\(u\\in\\ker(J-\\lambda I)^{s}\\) が存在して, \\(v^{s}=Vu\\) とできる. \\[ \\begin{aligned} (A-\\lambda E)v^{s+1}=Ev^{s} &amp; \\Rightarrow(A-\\lambda E)v^{s+1}=EVu\\\\ &amp; \\Rightarrow(A-\\lambda E)V\\xi_{1}=EVu\\\\ &amp; \\Rightarrow EV(J-\\lambda I)\\xi_{1}=EVu, \\end{aligned} \\] \\(EV\\) は単射なので, \\((J-\\lambda I)\\xi_{1}=u\\). したがって, \\[ v^{s+1}=V\\xi_{1},\\quad\\xi_{1}\\in\\ker(J-\\lambda I)^{s+1} \\] が成り立つ. したがって, \\(\\mathcal{G}_{\\lambda}^{s+1}\\subset V\\ker(J-\\lambda I)^{s+1}\\) が成り立つ. 逆の包含関係を示すために, \\(v^{s+1}\\in\\ker(J-\\lambda I)^{s+1}\\) としよう. \\(v^{s}\\in\\ker(J-\\lambda I)^{s}\\) を \\[ (J-\\lambda I)v^{s+1}=v^{s} \\] が成り立つように選ぶ. \\(EV\\) は単射なので, これは \\[ EV(J-\\lambda I)v^{s+1}=EVv^{s} \\] と同値である. さらにこれは \\(EVJ=AV\\) により \\[ (A-\\lambda E)Vv^{s+1}=EVv^{s} \\] と同値. 帰納法の仮定 \\(\\mathcal{G}_{\\lambda}^{s}=V\\ker(J-\\lambda I)^{s}\\) より, \\(Vv^{s}\\in\\mathcal{G}_{\\lambda}^{s}\\) だから, \\[ Vv^{s+1}\\in\\mathcal{G}_{\\lambda}^{s+1}=(A-\\lambda E)^{-1}\\left(E\\mathcal{G}_{\\lambda}^{s}\\right) \\] が成り立つ. よって, \\(V\\ker(J-\\lambda I)^{s+1}\\subset\\mathcal{G}_{\\lambda}^{s+1}\\). これで, 任意の \\(k\\) について \\(\\mathcal{G}_{\\lambda}^{k}=V\\ker(J-\\lambda I)^{k}\\) が示された. . まず, \\(\\mathcal{G}_{\\infty}^{k}=\\mathcal{W}_{k}\\) であることは2つの定義が一致していることから直ちに分かる. \\(\\mathcal{G}_{\\infty}^{k}=W\\ker N^{k}\\) を示す. \\(k=0\\) に対しては自明である. ある \\(s\\) について, \\(\\mathcal{G}_{\\infty}^{s}=W\\ker N^{s}\\) が成り立つと仮定する. \\(v^{s+1}\\in\\mathcal{G}_{\\infty}^{s+1}\\setminus\\{0\\}\\) を任意に選ぶ. ある \\(v^{s}\\in\\mathcal{G}_{\\infty}^{s}=W\\ker N^{s}\\) が存在して, \\[ Ev^{s+1}=Av^{s} \\] が成り立つ. 帰納法の仮定より, \\(u\\in\\mathbb{F}^{n_{2}}\\) が存在して \\(v^{s}=Wu\\), \\(N^{s}u=0\\) とできる. さらに, \\(v^{s+1}=V\\xi_{1}+W\\xi_{2}\\) と分解すると, \\[ EV\\xi_{1}+EW\\xi_{2}=AWu,\\quad u\\in\\ker N^{s} \\] を得る. \\(EW=AWN\\) だから, \\[ EV\\xi_{1}=AW(u-N\\xi_{2}) \\] あるいは \\[ \\begin{bmatrix}EV &amp; AW\\end{bmatrix}\\begin{bmatrix}\\xi_{1}\\\\ u-N\\xi_{2} \\end{bmatrix}=0. \\] \\([EV\\ AW]\\) は正則なので, \\(\\xi_{1}=0\\), \\(u=N\\xi_{2}\\). すなわち, \\[ v^{s+1}=W\\xi_{2},\\quad\\xi_{2}\\in\\ker N^{s+1}. \\] これは, \\(v^{s+1}\\in W\\ker N^{s+1}\\) を意味している. よって, \\(\\mathcal{G}_{\\lambda}^{s+1}\\subset W\\ker N^{s+1}\\) 次に, \\(v^{s+1}\\in W\\ker N^{s+1}\\) としよう. このとき, ある \\(u\\in\\ker N^{s+1}\\) が存在して \\[ v^{s+1}=Wu \\] とできる. \\(E\\) は単射なので, これは次と同値. \\[ \\begin{aligned} Ev^{s+1} &amp; =EWu\\\\ &amp; =AWNu. \\end{aligned} \\] \\(Nu\\in\\ker N^{s}\\) と \\(W\\ker N^{s}=\\mathcal{G}_{\\infty}^{s}\\) に注意すると, \\[ v^{s+1}\\in E^{-1}\\left(AW\\ker N^{s}\\right)=E^{-1}\\left(A\\mathcal{G}_{\\infty}^{s}\\right)=\\mathcal{G}_{\\infty}^{s+1}. \\] よって, \\(W\\ker N^{s+1}\\subset\\mathcal{G}_{\\lambda}^{s+1}\\). これで任意の \\(k\\) について \\(\\mathcal{G}_{\\infty}^{k}=W\\ker N^{k}\\) が示された. 補題 と, \\(V\\), \\(W\\) の単射性が述べているのは, \\((E,A)\\) の固有空間の構造は, 通常の固有値問題に対する一般固有空間の構造を決定する方法と同様に決定できる. したがって, 次の定理はジョルダン標準形の理論から導かれる. 定理 6.10 ({Proposition 3.5, {[}BIT{]}}) ペンシル \\((E,A)\\) をレギュラーとする. 任意の \\(\\lambda\\in\\mathrm{sp}(E,A)\\cup\\{\\infty\\}\\) に対して, 次の 1〜5 が成り立つ. 任意の固有ベクトル鎖 \\((v^{1},v^{2},\\dots,v^{k})\\) に対して, \\(v^{s}\\in\\mathcal{G}_{\\lambda}^{s}\\setminus\\mathcal{G}_{\\lambda}^{s-1}\\), \\(s=1,\\dots,k\\). 任意の \\(k\\le p(\\lambda)\\) と任意の \\(v\\in\\mathcal{G}_{\\lambda}^{k}\\setminus\\mathcal{G}_{\\lambda}^{k-1}\\) に対して, \\(v^{k}=v\\) なる固有ベクトル鎖 \\((v^{1},\\dots,v^{k})\\) が一意的に存在する. 任意の固有ベクトル鎖 \\((v^{1},v^{2},\\dots,v^{k})\\) は1次独立である. 次の包含関係が成り立つ \\[ \\mathcal{G}_{\\lambda}\\subset\\begin{cases} \\mathcal{V}^{*} &amp; \\text{if}\\quad\\lambda\\in\\mathrm{sp}(E,A)\\\\ \\mathcal{W}^{*} &amp; \\text{if}\\quad\\lambda=\\infty. \\end{cases} \\] 任意の \\(\\lambda\\in\\mathrm{sp}(E,A)\\cup\\{\\infty\\}\\) に対して \\[ \\dim\\mathcal{G}_{\\lambda}=\\mathrm{am}(\\lambda). \\] % 証明. 省略する. % ベクトル鎖の定義 \\((A-\\lambda E)v^{1}=Ev^{0}=0\\), \\((A-\\lambda E)v^{s}=Ev^{s-1}\\), \\(v^{s}\\in V\\ker(J-\\lambda I)^{s}\\) より, \\(u^{s}\\in\\ker(J-\\lambda I)^{s}\\) が一意的に存在して \\((A-\\lambda E)Vu^{s}=EVu^{s-1}\\) が成り立つ. \\(AV=EVJ\\) より \\[ EV(J-\\lambda I)u^{s}=EVu^{s-1}. \\] \\(EV\\) は単射なので, これは \\[ (J-\\lambda I)u^{s}=u^{s-1} \\] あるいは \\[ Ju^{s}=\\lambda u^{s}+u^{s-1} \\] と同値である. したがって, \\[ J[u^{1}\\ \\cdots\\ u^{k}]=[u^{1}\\ \\cdots\\ u^{k}]\\begin{bmatrix}\\lambda &amp; 1\\\\ &amp; \\lambda &amp; \\ddots\\\\ &amp; &amp; \\ddots &amp; 1\\\\ &amp; &amp; &amp; \\lambda \\end{bmatrix}. \\] 両辺に左から \\(EV\\) を掛けると, \\[ A[Vu^{1}\\ \\cdots\\ Vu^{k}]=[EVu^{1}\\ \\cdots\\ EVu^{k}]\\begin{bmatrix}\\lambda &amp; 1\\\\ &amp; \\lambda &amp; \\ddots\\\\ &amp; &amp; \\ddots &amp; 1\\\\ &amp; &amp; &amp; \\lambda \\end{bmatrix}, \\] したがって \\[ A[v^{1}\\ \\cdots\\ v^{k}]=[Ev^{1}\\ \\cdots\\ Ev^{k}]\\begin{bmatrix}\\lambda &amp; 1\\\\ &amp; \\lambda &amp; \\ddots\\\\ &amp; &amp; \\ddots &amp; 1\\\\ &amp; &amp; &amp; \\lambda \\end{bmatrix} \\] が成り立つ. これは () ですでに得たワイエルシュトラス正準形と一致する. 同様に, \\(E\\) の変換を考える. ベクトル鎖の定義 \\(Ew^{1}=Aw^{0}=0\\), \\(Ew^{s}=Aw^{s-1}\\), \\(w^{s}\\in W\\ker N^{s}\\) より, \\(u^{s}\\in\\ker N^{s}\\) が一意的に存在して \\(EWu^{s}=AWu^{s-1}\\) が成り立つ. \\(EW=AWN\\) により, \\[ AW(Nu^{s}-u^{s-1})=0. \\] \\(AW\\) は単射なので, これは \\[ Nu^{s}-u^{s-1}=0 \\] と同値である. \\(N\\) の \\([u^{1}\\ \\cdots\\ u^{k}]\\) に関する次の行列表現を得る. \\[ N[u^{1}\\ \\cdots\\ u^{k}]=[u^{1}\\ \\cdots\\ u^{k}]\\begin{bmatrix}0 &amp; 1\\\\ &amp; 0 &amp; \\ddots\\\\ &amp; &amp; \\ddots &amp; 1\\\\ &amp; &amp; &amp; 0 \\end{bmatrix}. \\] 両辺に左から \\(AW\\) を掛けると, \\[ AWN[u^{1}\\ \\cdots\\ u^{k}]=AW[u^{1}\\ \\cdots\\ u^{k}]\\begin{bmatrix}0 &amp; 1\\\\ &amp; 0 &amp; \\ddots\\\\ &amp; &amp; \\ddots &amp; 1\\\\ &amp; &amp; &amp; 0 \\end{bmatrix}. \\] あるいは, \\[ E[Wu^{1}\\ \\cdots\\ Wu^{k}]=[AWu^{1}\\ \\cdots\\ AWu^{k}]\\begin{bmatrix}0 &amp; 1\\\\ &amp; 0 &amp; \\ddots\\\\ &amp; &amp; \\ddots &amp; 1\\\\ &amp; &amp; &amp; 0 \\end{bmatrix}. \\] したがって, \\[ E[w^{1}\\ \\cdots\\ w^{k}]=[Aw^{1}\\ \\cdots\\ Aw^{k}]\\begin{bmatrix}0 &amp; 1\\\\ &amp; 0 &amp; \\ddots\\\\ &amp; &amp; \\ddots &amp; 1\\\\ &amp; &amp; &amp; 0 \\end{bmatrix} \\] を得る. これは式 () と一致する. 定理 6.11 \\((E,A)\\) をレギュラーとする. 任意の \\(i=1,\\dots,r\\) と任意の \\(j=1,\\dots,\\eta_{i}=\\mathrm{gm}(\\lambda_{i})\\) について, ある \\(1\\le k_{ij}\\le p(\\lambda_{i})\\) と固有ベクトル鎖 \\(V_{ij}:=\\left[v_{ij}^{1}\\ v_{ij}^{2}\\ \\cdots\\ v_{ij}^{k_{ij}}\\right]\\) が存在して, \\[ \\mathcal{G}_{\\lambda_{i}}=\\bigoplus_{j=1}^{\\mu_{i}}\\mathrm{im}V_{ij} \\] が成り立つ. さらに \\[ \\begin{aligned} \\mathcal{V}^{*} &amp; =\\bigoplus_{i=1}^{r}\\mathcal{G}_{\\lambda_{i}},\\\\ \\mathcal{W}^{*} &amp; =\\mathcal{G}_{\\infty} \\end{aligned} \\] が成り立つ. 証明. \\(\\lambda,\\mu\\in\\mathrm{sp}(E,A)\\), \\(\\lambda\\neq\\mu\\) に対して \\(\\mathcal{G}_{\\lambda}\\cap\\mathcal{G}_{\\mu}=\\{0\\}\\) であることを示そう. \\(v\\neq0\\) が \\[ v\\in\\mathcal{G}_{\\lambda}\\cap\\mathcal{G}_{\\mu} \\] を満たすとしよう. 補題 から \\[ \\begin{aligned} v &amp; \\in\\mathcal{G}_{\\lambda}=V\\ker(J-\\lambda I)^{p(\\lambda)}\\\\ v &amp; \\in\\mathcal{G}_{\\mu}=V\\ker(J-\\mu I)^{p(\\mu)} \\end{aligned} \\] が成り立つ. \\(V:\\mathbb{F}^{n_{1}}\\to\\mathbb{F}^{n}\\) は単射なので, \\[ \\begin{aligned} \\dim\\ker(J-\\lambda I)^{p(\\lambda)} &amp; =\\dim\\mathcal{G}_{\\lambda}=\\mathrm{am}(\\lambda)\\\\ \\dim\\ker(J-\\mu I)^{p(\\mu)} &amp; =\\dim\\mathcal{G}_{\\mu}=\\mathrm{am}(\\mu). \\end{aligned} \\] したがって, \\(J\\) の最小多項式が \\((z-\\lambda)^{p(\\lambda)}\\) と \\((z-\\mu)^{p(\\mu)}\\) を因数として持つことを表している. \\(J\\) に対するハミルトン・ケーリーの定理より \\[ \\ker(J-\\lambda I)^{p(\\lambda)}\\cap\\ker(J-\\mu I)^{p(\\mu)}=\\{0\\} \\] が従うので, \\(v=0\\) でなければならない. 定理 で構成したベクトル鎖を並べたもの \\[ \\bar{V}=\\left[v_{ij}^{1},\\dots,v_{ij}^{k_{ij}}\\ \\mid\\ i=1,\\dots,r,\\ j=1,\\dots,\\eta_{i}\\right] \\] は \\(\\mathcal{V}^{*}\\) の基底をなす. また, \\[ \\bar{W}=\\left[v_{\\infty j}^{1},\\dots,v_{\\infty j}^{k_{\\infty j}}\\ \\mid\\ j=1,\\dots,\\eta\\right] \\] は \\(\\mathcal{W}^{*}\\) の基底をなす. これで定理 の証明が完了する. 参考文献 "],
["blanchardkahn.html", "第7章 線形システムの分析 7.1 行列公式 7.2 \\(E=I\\) のケース 7.3 \\(E \\neq I\\) のケース", " 第7章 線形システムの分析 動学的マクロ経済モデルの典型的なパターンは, 均衡経路の拘束条件をストック変数に関する初期条件と, 価格に関する終端条件 (横断性条件) で与えるというものである. ストックは過去の経済活動によって定まるため過去の情報から決定される (先決変数). 一方, 価格は将来価値を反映して決まるものなので, 未来の情報を用いて決定される (非先決変数). したがって, 経済モデルは時間の流れに沿って決まっていく成分と, 時間の流れを逆行して決まる成分が組み合わさって成り立っているのが普通である. ゆえに経済モデルの解析は線形システムであっても比較的複雑なものになる. 基本モデルの解法をきちんと理解しておくことは大変重要である. 本稿では, Blanchard and Kahn (1980) および Klein (2000) の決定論バージョンを考察しよう25。具体的には, ジョルダン標準形およびワイエルシュトラス標準形を用いた分析を確率的な要因が一切存在しないモデルに対して適用する. 確率的な要因を入れたモデルでも基本的な考え方は変わらないので, ここでの分析手法をよく理解してほしい. 条件付き期待値のルールを一通り理解すれば確率的なモデルに容易に拡張できる. ただし, すでに指摘した通り, ジョルダン標準形やワイエルシュトラス標準形は数値計算では通常用いることができない. 固有値の重複度に関して浮動小数点数を用いた判定が難しいからである. この問題はSchur 分解や QZ 分解あるいは特異値分解を利用することで解決するのであるが, それは次回以降に譲って理論的な結果をまず見てみよう. 7.1 行列公式 本題に入る前にいくつかの行列公式をまとめておこう. 7.1.1 逆行列公式 本稿の計算結果を既存研究の結果と比較すると (記号の違いという明白な相違点以外にも) 見かけ上異なる解を得ていることが分かるだろう. これは導出方法が異なるために発生した見せかけの違いであって本質的なものではない. 文献と比較する際には次の行列公式を念頭においておくとよい. 定理 7.1 \\(A,B,C,D\\) は適当な次元の行列であるとする. \\(A\\) および \\(D-CA^{-1}B\\) を正則とする. このとき\\(\\left[\\begin{smallmatrix}A &amp; B\\\\ C &amp; D \\end{smallmatrix}\\right]\\) は正則であり, 次式が成立する. \\[ \\begin{bmatrix}A &amp; B\\\\ C &amp; D \\end{bmatrix}^{-1}=\\begin{bmatrix}A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} &amp; -A^{-1}B(D-CA^{-1}B)^{-1}\\\\ -(D-CA^{-1}B)^{-1}CA^{-1} &amp; (D-CA^{-1}B)^{-1} \\end{bmatrix}. \\] 系 7.1 \\(A,C,D\\) は適当な次元の行列であるとする. \\(A\\) および \\(D\\) を正則とする. このとき\\(\\left[\\begin{smallmatrix}A &amp; 0\\\\ C &amp; D \\end{smallmatrix}\\right]\\) は正則であり, 次式が成立する. \\[ \\begin{bmatrix}A &amp; 0\\\\ C &amp; D \\end{bmatrix}^{-1}=\\begin{bmatrix}A^{-1} &amp; 0\\\\ -D^{-1}CA^{-1} &amp; D^{-1} \\end{bmatrix}. \\] 定理7.1 および系@ref{cor:blockcor}を証明せよ. 7.1.2 シルベスタ方程式 2つの演算を定義しよう. 行列の各列を縦に積み上げる写像を \\(\\mathrm{vec}:\\mathbb{F}^{m\\times n}\\to\\mathbb{F}^{mn}\\) と書く. すなわち, \\[ \\begin{aligned} \\mathrm{vec}\\left(\\begin{bmatrix}a_{11} &amp; \\cdots &amp; a_{1n}\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{m1} &amp; \\cdots &amp; a_{mn} \\end{bmatrix}\\right)=\\begin{bmatrix}a_{11} &amp; \\cdots &amp; a_{m1} &amp; \\cdots &amp; \\cdots &amp; a_{1n} &amp; \\cdots &amp; a_{mn}\\end{bmatrix}^{\\top}. \\end{aligned} \\] 容易に分かることだが, \\(\\mathrm{vec}\\) は線形写像である. \\(A\\in\\mathbb{F}^{m\\times n}\\) と \\(B\\in\\mathbb{F}^{p\\times q}\\) のクロネッカー積 \\(A\\otimes B\\in\\mathbb{F}^{mp\\times nq}\\) を \\[ A\\otimes B:=\\begin{bmatrix}a_{11}B &amp; \\cdots &amp; a_{1n}B\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{m1}B &amp; \\cdots &amp; a_{mn}B \\end{bmatrix} \\] によって定義する. 次の補題を証明しよう. 補題 7.1 \\(A\\), \\(B\\), \\(C\\) を適当なサイズの行列とする. このとき, \\(\\mathrm{vec}(ABC)=(C^{\\top}\\otimes A)\\mathrm{vec}(B)\\) が成り立つ. 証明. \\(A\\in\\mathbb{F}^{m\\times n}\\), \\(B\\in\\mathbb{F}^{n\\times p}\\), \\(C\\in\\mathbb{F}^{p\\times q}\\) とする. \\(B=\\begin{bmatrix}B_{1} &amp; \\cdots &amp; B_{p}\\end{bmatrix}\\), \\(C=\\begin{bmatrix}C_{1} &amp; \\cdots &amp; C_{q}\\end{bmatrix}\\)とすれば \\[ \\begin{aligned} \\left(C^{\\top}\\otimes A\\right)\\mathrm{vec}(B) &amp; =\\begin{bmatrix}c_{11}A &amp; \\cdots &amp; c_{p1}A\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ c_{1q}A &amp; \\cdots &amp; c_{pq}A \\end{bmatrix}\\begin{bmatrix}B_{1}\\\\ \\vdots\\\\ B_{p} \\end{bmatrix}=\\begin{bmatrix}c_{11}AB_{1}+\\cdots+c_{p1}AB_{p}\\\\ \\vdots\\\\ c_{1q}AB_{1}+\\cdots+c_{pq}AB_{p} \\end{bmatrix}\\\\ &amp; =\\begin{bmatrix}\\begin{bmatrix}AB_{1} &amp; \\cdots &amp; AB_{p}\\end{bmatrix}C_{1}\\\\ \\vdots\\\\ \\begin{bmatrix}AB_{1} &amp; \\cdots &amp; AB_{p}\\end{bmatrix}C_{q} \\end{bmatrix}=\\begin{bmatrix}ABC_{1}\\\\ \\vdots\\\\ ABC_{q} \\end{bmatrix}=\\mathrm{vec}(ABC). \\end{aligned} \\] 定理 7.2 (離散時間シルベスタ方程式) \\(A\\), \\(B\\), C を適当なサイズの行列, \\(I-\\left(B^{\\top}\\otimes A\\right)\\) は正則であるとする. 行列方程式 \\(AXB-X+C=0\\)の解 \\(X\\) は, \\[ \\mathrm{vec}(X)=\\left[I-\\left(B^{\\top}\\otimes A\\right)\\right]^{-1}\\mathrm{vec}(C) \\] によって与えられる. 証明. 全体を \\(\\mathrm{vec}\\) で写して, \\(\\mathrm{vec}(AXB)-\\mathrm{vec}(X)+\\mathrm{vec}(C)=0\\)を得る. 補題 7.1 より\\(\\left(B^{\\top}\\otimes A\\right)\\mathrm{vec}(X)-\\mathrm{vec}(X)=-\\mathrm{vec}(C)\\). したがって, \\(\\mathrm{vec}(X)=\\left[I-\\left(B^{\\top}\\otimes A\\right)\\right]^{-1}\\mathrm{vec}(C)\\). 7.2 \\(E=I\\) のケース 無限期間のシステム方程式 \\(Ex_{t+1}=Ax_{t}+Bu_{t}\\) において \\(E=I\\) なるケースを考える. すなわち, \\[\\begin{equation} x_{t+1}=Ax_{t}+Bu_{t},\\quad t=0,1,\\dots,\\tag{7.1} \\end{equation}\\] 各ベクトルと行列のサイズは \\(x\\in\\mathbb{R}^{n}\\), \\(u\\in\\mathbb{R}^{m}\\), \\(A\\in\\mathbb{R}^{n\\times n}\\), \\(B\\in\\mathbb{R}^{n\\times m}\\) とする. \\(u\\) は外生的に与えられるショックである. 序文に述べた通り, \\(x\\) を構成する変数の一部に初期条件が与えられている状況を想定している. 具体的には変数 \\(x\\) が \\(x^{1}\\in\\mathbb{R}^{n_{1}}\\) および \\(x^{2}\\in\\mathbb{R}^{n_{2}}\\) によって \\[\\begin{equation} x_{t}=\\begin{bmatrix}x_{t}^{1}\\\\ x_{t}^{2} \\end{bmatrix} \\tag{7.2} \\end{equation}\\] と記述でき, 初期条件が \\[ x_{0}^{1}=\\bar{x}_{0}^{1}, \\] で与えられているとする. 終端条件として経路の発散スピードが幾何数列の発散スピードを超えないことを仮定する. すなわち, 任意の \\(-1&lt;\\rho&lt;1\\) について \\[\\begin{equation} \\lim_{T\\to\\infty}\\rho^{T}\\|x_{T}\\|=0 \\tag{7.3} \\end{equation}\\] が成り立つ \\(x\\) を実行可能な解であるとする. 式 (7.1) を式 (7.2) に適合するようにブロック行列表現すると \\[ \\begin{bmatrix}x_{t+1}^{1}\\\\ x_{t+1}^{2} \\end{bmatrix}=\\begin{bmatrix}A_{11} &amp; A_{12}\\\\ A_{21} &amp; A_{22} \\end{bmatrix}\\begin{bmatrix}x_{t}^{1}\\\\ x_{t}^{2} \\end{bmatrix}+\\begin{bmatrix}B_{1}\\\\ B_{2} \\end{bmatrix}u_{t} \\] と書ける. \\(A\\) のジョルダン分解 \\[ \\begin{bmatrix}A_{11} &amp; A_{12}\\\\ A_{21} &amp; A_{22} \\end{bmatrix}=P\\begin{bmatrix}J_{s} &amp; 0\\\\ 0 &amp; J_{u} \\end{bmatrix}P^{-1} \\] に基いて状態方程式(7.1)を変形し, \\[ P^{-1}\\begin{bmatrix}x_{t+1}^{1}\\\\ x_{t+1}^{2} \\end{bmatrix}=\\begin{bmatrix}J_{s} &amp; 0\\\\ 0 &amp; J_{u} \\end{bmatrix}P^{-1}\\begin{bmatrix}x_{t}^{1}\\\\ x_{t}^{2} \\end{bmatrix}+P^{-1}\\begin{bmatrix}B_{1}\\\\ B_{2} \\end{bmatrix}u_{t} \\] を得る. \\(P\\) をうまく選んで, \\(J_{s}\\) の対角成分には絶対値が1以下の固有値, \\(J_{u}\\) の対角成分には絶対値が \\(1\\) より大きい固有値が並んでいるとしよう. この結果, \\(J_{u}\\) は正則になる. \\(y:=P^{-1}x\\), \\(C:=P^{-1}B\\) と変数変換することで \\[\\begin{equation} \\begin{bmatrix}y_{t+1}^{s}\\\\ y_{t+1}^{u} \\end{bmatrix}=\\begin{bmatrix}J_{s} &amp; 0\\\\ 0 &amp; J_{u} \\end{bmatrix}\\begin{bmatrix}y_{t}^{s}\\\\ y_{t}^{u} \\end{bmatrix}+\\begin{bmatrix}C_{s}\\\\ C_{u} \\end{bmatrix}u_{t} \\tag{7.4} \\end{equation}\\] を得る. ただし, \\[ y^{s}\\in\\mathbb{R}^{n_{s}},\\ y^{u}\\in\\mathbb{R}^{n_{u}} \\] であり, 行列, およびその各ブロックはすべて適合するサイズをもつものとする. 典型的なケースでは初期条件の数が変数の次元に対して不足しているため, 最終期までの情報を使って不足した \\(t=0\\) 時点の条件を補ってから解を決定する. やや天下り的ではあるが, 式(7.4) 下段のシステムを, \\(t\\) 期から \\(T\\) 期までの情報を使って後ろ向きに (forward-looking に) 解いてみよう26。 \\[\\begin{align} y_{t}^{u} &amp; =J_{u}^{-1}y_{t+1}^{u}-J_{u}^{-1}C_{u}u_{t}\\nonumber \\\\ &amp; =J_{u}^{-1}\\left(J_{u}^{-1}y_{t+2}^{u}-J_{u}^{-1}C_{u}u_{t+1}\\right)-J_{u}^{-1}C_{u}u_{t}\\nonumber \\\\ &amp; =J_{u}^{-2}\\left(J_{u}^{-1}y_{t+3}^{u}-J_{u}^{-1}C_{u}u_{t+2}\\right)-J_{u}^{-2}C_{u}u_{t+1}-J_{u}^{-1}C_{u}u_{t}\\nonumber \\\\ &amp; =J_{u}^{-3}y_{t+3}^{u}-J_{u}^{-3}C_{u}u_{t+2}-J_{u}^{-2}C_{u}u_{t+1}-J_{u}^{-1}C_{u}u_{t}\\nonumber \\\\ &amp; =\\cdots\\nonumber \\\\ &amp; =J_{u}^{t-T}y_{T}^{u}-\\sum_{k=0}^{T-t-1}J_{u}^{-k-1}C_{u}u_{t+k}. \\tag{7.5} \\end{align}\\] ここで, \\(J_{u}^{-1}\\) は安定行列であり, \\(T\\to\\infty\\) のとき幾何数列のオーダーで減衰する. 一方 \\(y_{T}^{u}\\) は \\(x_{T}\\) の1次結合であるから, 仮定(7.3)により\\(\\lim_{T\\to\\infty}\\|J_{u}^{t-T}y_{T}^{u}\\|=0\\). つまり, \\[\\begin{equation} y_{t}^{u}=-\\sum_{k=0}^{\\infty}J_{u}^{-k-1}C_{u}u_{t+k} \\tag{7.6} \\end{equation}\\] が成り立つ.27 変数 \\(x\\) と \\(y\\) の関係から任意の \\(t\\) に対して \\[\\begin{equation} \\begin{bmatrix}x_{t}^{1}\\\\ x_{t}^{2} \\end{bmatrix}=P\\begin{bmatrix}y_{t}^{s}\\\\ y_{t}^{u} \\end{bmatrix}=\\begin{bmatrix}P_{1s} &amp; P_{1u}\\\\ P_{2s} &amp; P_{2u} \\end{bmatrix}\\begin{bmatrix}y_{t}^{s}\\\\ y_{t}^{u} \\end{bmatrix} \\tag{7.7} \\end{equation}\\] が成り立つ. \\(t=0\\) したとき, (7.7) に現れる変数の内 \\(x_{0}^{1}=\\bar{x}_{0}^{1}\\) と \\(y_{0}^{u}=\\bar{y}_{0}^{u}\\) が定まっている. ここで, \\(P_{1s}\\in\\mathbb{R}^{n_{1}\\times n_{s}}\\), \\(P_{1u}\\in\\mathbb{R}^{n_{1}\\times n_{u}}\\), \\(P_{2s}\\in\\mathbb{R}^{n_{2}\\times n_{s}}\\) および \\(P_{2u}\\in\\mathbb{R}^{n_{2}\\times n_{u}}\\) に注意せよ. この情報から, \\(x_{0}^{2}\\) および \\(y_{0}^{s}\\)を決定できれば解軌道が完全に定まる. \\(t=0\\) で成立する線形方程式 \\[\\begin{align} \\bar{x}_{0}^{1} &amp; =P_{1s}y_{0}^{s}+P_{1u}\\bar{y}_{0}^{u} \\tag{7.8}\\\\ x_{0}^{2} &amp; =P_{2s}y_{0}^{s}+P_{2u}\\bar{y}_{0}^{u}\\tag{7.9} \\end{align}\\] を変形し, 未知量 \\((x_{0}^{2},y_{0}^{s})\\) に関する次の線形方程式を得る. \\[\\begin{equation} \\begin{bmatrix}P_{1s} &amp; 0\\\\ P_{2s} &amp; -I_{n_{2}} \\end{bmatrix}\\begin{bmatrix}y_{0}^{s}\\\\ x_{0}^{2} \\end{bmatrix}=\\begin{bmatrix}I_{n_{1}} &amp; -P_{1u}\\\\ 0 &amp; -P_{2u} \\end{bmatrix}\\begin{bmatrix}\\bar{x}_{0}^{1}\\\\ \\bar{y}_{0}^{u} \\end{bmatrix}.\\tag{7.10} \\end{equation}\\] ベクトルと行列のサイズはそれぞれ \\[ \\begin{bmatrix}y_{0}^{s}\\\\ x_{0}^{s} \\end{bmatrix}\\in\\mathbb{R}^{n_{s}+n_{2}},\\quad\\begin{bmatrix}\\bar{x}_{0}^{1}\\\\ \\bar{y}_{0}^{u} \\end{bmatrix}\\in\\mathbb{R}^{n_{1}+n_{u}}, \\] \\[ \\Pi_{1}:=\\begin{bmatrix}P_{1s} &amp; 0\\\\ P_{2s} &amp; -I \\end{bmatrix}\\in\\mathbb{R}^{n\\times(n_{s}+n_{2})},\\quad\\Pi_{2}:=\\begin{bmatrix}I &amp; -P_{1u}\\\\ 0 &amp; -P_{2u} \\end{bmatrix}\\in\\mathbb{R}^{n\\times(n_{1}+n_{u})}. \\] 解の決定に際して, 3つの場合に場合分けをする. \\(n_{s}+n_{2}&gt;n\\) \\[ n_{s}+n_{2}&gt;n=n_{1}+n_{2}\\Longleftrightarrow n_{s}&gt;n_{1} \\] が成り立つとすれば, 方程式 (7.10) の係数行列 \\(\\Pi_{1}\\in\\mathbb{R}^{n\\times(n_{s}+n_{2})}\\) は横長の行列である. したがって, 方程式 (7.10) を満たす \\((y_{0}^{s},x_{0}^{2})\\) は無数に存在する. 解軌道が無数に存在するので解は不決定 (indeterminate)であるという. 一般に, 初期値の選び方に自由度がある場合に不決定性があるという. \\(n_{s}+n_{2}&lt;n\\) \\[ n_{s}+n_{2}&lt;n=n_{1}+n_{2}\\Longleftrightarrow n_{s}&lt;n_{1} \\] が成り立つとしよう. このとき, 係数行列 \\(\\Pi_{1}\\) は縦長の行列であるから, 方程式 (7.10) を満たす \\((y_{0}^{s},x_{0}^{2})\\) は一般には存在しない. 解の存在を保証するためには, \\[ \\Pi_{2}\\begin{bmatrix}\\bar{x}_{0}^{1}\\\\ \\bar{y}_{0}^{u} \\end{bmatrix}\\in\\mathrm{im}\\Pi_{1} \\] となるように初期条件と入力を決めなければならない. \\(n_{s}+n_{2}=n\\) 最後に \\[ n_{s}+n_{2}=n=n_{1}+n_{2}\\Longleftrightarrow n_{s}=n_{1} \\] が成り立つとしよう. このとき, \\(\\Pi_{1}\\) は正方行列である. さらにこれが正則であれば解 \\((y_{0}^{1},x_{0}^{2})\\) が一意的に定まる. \\(\\det\\Pi_{1}=\\det P_{1s}\\) より, \\(\\Pi_{1}\\) の正則性は \\(P_{1s}\\) の正則性と同値である. \\(P_{1s}\\) が正則でない場合は再び解の不決定性が現れる. 系7.1 より\\(P_{1s}\\) が正則であれば \\[ \\Pi_{1}^{-1}=\\begin{bmatrix}P_{1s}^{-1} &amp; 0\\\\ P_{2s}P_{1s}^{-1} &amp; -I \\end{bmatrix}. \\] したがって, \\[ \\begin{aligned} \\begin{bmatrix}y_{0}^{s}\\\\ x_{0}^{2} \\end{bmatrix} &amp; =\\begin{bmatrix}P_{1s}^{-1} &amp; 0\\\\ P_{2s}P_{1s}^{-1} &amp; -I \\end{bmatrix}\\begin{bmatrix}I &amp; -P_{1u}\\\\ 0 &amp; -P_{2u} \\end{bmatrix}\\begin{bmatrix}\\bar{x}_{0}^{1}\\\\ \\bar{y}_{0}^{u} \\end{bmatrix}\\\\ &amp; =\\begin{bmatrix}P_{1s}^{-1} &amp; -P_{1s}^{-1}P_{1u}\\\\ P_{2s}P_{1s}^{-1} &amp; P_{2u}-P_{2s}P_{1s}^{-1}P_{1u} \\end{bmatrix}\\begin{bmatrix}\\bar{x}_{0}^{1}\\\\ \\bar{y}_{0}^{u} \\end{bmatrix} \\end{aligned} \\] によって \\(t=0\\) における変数値が完全に決定される. この等式は任意の \\(t=0\\) 以外でも成り立つことに注意をしておく. \\[ \\begin{aligned} \\begin{bmatrix}y_{t}^{s}\\\\ x_{t}^{2} \\end{bmatrix} &amp; =\\begin{bmatrix}P_{1s}^{-1} &amp; -P_{1s}^{-1}P_{1u}\\\\ P_{2s}P_{1s}^{-1} &amp; P_{2u}-P_{2s}P_{1s}^{-1}P_{1u} \\end{bmatrix}\\begin{bmatrix}x_{t}^{1}\\\\ y_{t}^{u} \\end{bmatrix} \\end{aligned} \\] この関係を使って, 次の方程式を得る. \\[\\begin{align} \\begin{bmatrix}x_{t+1}^{1}\\\\ x_{t+1}^{2} \\end{bmatrix} &amp; =P\\begin{bmatrix}y_{t+1}^{s}\\\\ y_{t+1}^{u} \\end{bmatrix}\\nonumber \\\\ &amp; =\\begin{bmatrix}P_{1s} &amp; P_{1u}\\\\ P_{2s} &amp; P_{2u} \\end{bmatrix}\\begin{bmatrix}J_{s}\\\\ &amp; J_{u} \\end{bmatrix}\\begin{bmatrix}y_{t}^{s}\\\\ y_{t}^{u} \\end{bmatrix}+Bu_{t}\\nonumber \\\\ &amp; =\\begin{bmatrix}P_{1s} &amp; P_{1u}\\\\ P_{2s} &amp; P_{2u} \\end{bmatrix}\\begin{bmatrix}J_{s}\\\\ &amp; J_{u} \\end{bmatrix}\\begin{bmatrix}P_{1s}^{-1}x_{t}^{1}-P_{1s}^{-1}P_{1u}y_{t}^{u}\\\\ y_{t}^{u} \\end{bmatrix}+Bu_{t}\\nonumber \\\\ &amp; =\\begin{bmatrix}P_{1s} &amp; P_{1u}\\\\ P_{2s} &amp; P_{2u} \\end{bmatrix}\\begin{bmatrix}J_{s}P_{1s}^{-1}x_{t}^{1}-J_{s}P_{1s}^{-1}P_{1u}y_{t}^{u}\\\\ J_{u}y_{t}^{u} \\end{bmatrix}+Bu_{t}\\nonumber \\\\ &amp; =\\begin{bmatrix}P_{1s}J_{s}P_{1s}^{-1}x_{t}^{1}+(P_{1u}J_{u}-P_{1s}J_{s}P_{1s}^{-1}P_{1u})y_{t}^{u}\\\\ P_{2s}J_{s}P_{1s}^{-1}x_{t}^{1}+(P_{2u}J_{u}-P_{2s}J_{s}P_{1s}^{-1}P_{1u})y_{t}^{u} \\end{bmatrix}+Bu_{t}.\\tag{7.11} \\end{align}\\] \\(y_{t}^{u}\\) は式(7.6) によって決定していることに注意せよ. なお, 式(7.11) には \\(x^{2}\\) が過去の情報に依存して決まっているという問題がある. \\(x^{2}\\) は初期条件が与えられない変数なので, 同時点の \\(x^{1}\\), \\(u\\) および未来の \\(u\\) にのみ依存する形で記述できなければならない. 少し計算を進めて Blanchard and Kahn (1980) の再帰公式 (p. 1308 (2), (3)式) を導出しよう. 彼らのモデルは確率的なモデルであるが, われわれの決定論モデルでも解の構造はまったく変わらない. 式(7.11) を使って, \\(x_{t}^{1}\\) を消去する. \\(x_{t}^{1}\\) の係数行列をそろえて \\[ \\begin{aligned} P_{2s}P_{1s}^{-1}x_{t+1}^{1} &amp; =P_{2s}J_{s}P_{1s}^{-1}x_{t}^{1}+(P_{2s}P_{1s}^{-1}P_{1u}J_{u}-P_{2s}J_{s}P_{1s}^{-1}P_{1u})y_{t}^{u}+P_{2s}P_{1s}^{-1}B_{1}u_{t}\\\\ x_{t+1}^{2} &amp; =P_{2s}J_{s}P_{1s}^{-1}x_{t}^{1}+(P_{2u}J_{u}-P_{2s}J_{s}P_{1s}^{-1}P_{1u})y_{t}^{u}+B_{2}u_{t}, \\end{aligned} \\] これらの差を取る. \\[ \\begin{aligned} &amp; x_{t+1}^{2}-P_{2s}P_{1s}^{-1}x_{t+1}^{1}\\\\ &amp; =(P_{2u}-P_{2s}P_{1s}^{-1}P_{1u})J_{u}y_{t}^{u}+(B_{2}-P_{2s}P_{1s}^{-1}B_{1})u_{t}\\\\ &amp; =-(P_{2u}-P_{2s}P_{1s}^{-1}P_{1u})J_{u}\\sum_{k=0}^{\\infty}J_{u}^{-k-1}C_{u}u_{t+k}+(B_{2}-P_{2s}P_{1s}^{-1}B_{1})u_{t}\\\\ &amp; =-(P_{2u}-P_{2s}P_{1s}^{-1}P_{1u})\\sum_{k=0}^{\\infty}J_{u}^{-k}C_{u}u_{t+k}+(B_{2}-P_{2s}P_{1s}^{-1}B_{1})u_{t}\\\\ &amp; =-(P_{2u}-P_{2s}P_{1s}^{-1}P_{1u})\\left(C_{u}u_{t}+\\sum_{k=0}^{\\infty}J_{u}^{-k-1}C_{u}u_{t+k+1}\\right)+(B_{2}-P_{2s}P_{1s}^{-1}B_{1})u_{t}\\\\ &amp; =(P_{2u}-P_{2s}P_{1s}^{-1}P_{1u})y_{t+1}^{u}. \\end{aligned} \\] 最後の等式には次の関係を使った. \\[ (P_{2u}-P_{2s}P_{1s}^{-1}P_{1u})C_{u}=B_{2}-P_{2s}P_{1s}^{-1}B_{1}. \\] 定理としてまとめておこう. 定理 7.3 \\(n_{1}=n_{s}\\) かつ \\(\\det P_{1s}\\neq0\\) とする. このとき, (7.1) の解は一意に定まり, \\(t=0,1,\\dots\\) について \\(x_{t}\\) は次式で与えられる. \\[ \\begin{aligned} x_{t+1}^{1} &amp; =P_{1s}J_{s}P_{1s}^{-1}x_{t}^{1}+(P_{1u}J_{u}-P_{1s}J_{s}P_{1s}^{-1}P_{1u})y_{t}^{u} + B_1 u_t,\\\\ x_{t}^{2} &amp; =P_{2s}P_{1s}^{-1}x_{t}^{1}+(P_{2u}-P_{2s}P_{1s}^{-1}P_{1u})y_{t}^{u},\\\\ y_{t}^{u} &amp; =-\\sum_{k=0}^{\\infty}J_{u}^{-k-1}C_{u}u_{t+k},\\\\ x_{0}^{1} &amp; :\\ \\text{given.} \\end{aligned} \\] Blanchard and Kahn (1980) では再帰的でない解表現を得ているが, 単純な計算で導出できるのでここではあえて述べることはしない. 各自確認してほしい. 図解 2次元の場合に Blanchard-Kahn の定理を図解しよう. 図7.1 はジョルダン標準形に変換する前の座標系を表している. 初期時点 \\(t=0\\) で \\(x\\) が取りうる範囲は赤線で表された直線である. 1次元の自由度が与えられているのは, \\(x_{0}^{1}\\) が固定されている一方で, \\(x_{0}^{2}\\) が未知であることを反映している. 図 7.1: 変換前の直交座標系. \\(x_{0}^{1}\\) だけが所与であり, \\((x_{0}^{1},x_{0}^{2})\\)は赤色実線上のどこかにある. 図 7.2 はジョルダン標準形に変換した後の座標系である. すなわち, \\(P=[P_{1}\\ P_{2}]\\) を基底とした座標系を表している. 一般に\\(P\\) は直交性を保たないので, 斜交座標系を描いている. 異なる座標系は同一の点を表現する別の方法を定めるだけであるから, \\((x_{0}^{1},x_{0}^{2})\\) の位置を表す赤色の直線は動かない. あくまで目盛りを変えただけである. 座標系の変更により \\(y^{u}\\) 軸に平行なサブシステム（不安定固有値に対応する固有空間上のシステム）と\\(y^{s}\\)軸に平行なサブシステム（安定固有値に対応する固有空間上のシステム）に分離することができる. 式 (7.4) を確認せよ. 図 7.2: ジョルダン標準形に変換した後の斜交座標系. 座標系を変更しても赤色実線は動かない. 式 (7.6) は不安定固有値に対応する固有空間 (\\(y_{u}\\) 軸) に沿って後ろ向きに \\(y_{0}^{u}\\) を見つける手続きである. 将来にわたって \\(u\\) が決定していれば, \\(t=\\infty\\) から \\(t=0\\) まで時間を逆向きに解いて \\(y_{0}^{u}\\) を決めることができる. この時点ではまだ \\(y_{0}^{s}\\) が決定しないので, \\((y_{0}^{s},y_{0}^{u})\\) は \\((0,y_{0}^{u})\\) を通り \\(y^{s}\\) 軸と平行な直線上にある. 図7.3 の青色実線を参照せよ. この直線は安定行列 \\(J_{u}^{-1}\\) によって特徴づけられる時間反転システムの極限を集めたものである. 直交座標の \\((x_{0}^{1},x_{0}^{2})\\) と斜交座標の\\((y_{0}^{s},y_{0}^{u})\\) が同じ点を表す場合に限り, それらは式(7.1) の\\(t=0\\) における値として適切に選ばれていると言ってよい. 一方を他方の座標系で表して一致することを確かめればよいので, 式(7.8), (7.9) を解けばよいことが分かるだろう. もちろんこれは (7.10) と同値である. 解の一意決定性のための条件は, 赤線と青線の交点が一意に存在するための条件に外ならない (図???). なお, \\(P_{1s}\\) の正則性は赤色の実線と青色の実線が平行にならないことを保証している. 図 7.3: 安定な後ろ向き (forward-looking) サブシステム \\(y_{t}^{u}=J_{u}^{-1}y_{t+1}^{u}-J_{u}^{-1}C_{u}u_{t}\\) の 極限 (\\(t=0\\))で \\(y_{0}^{u}\\) が決まる. 図 7.4: 青色と赤色の直線が交わる点で \\((x_{0}^{1},x_{0}^{2})\\) あるいは \\((y_{0}^{s},y_{0}^{u})\\) が定まる. \\((y_{0}^{s},y_{0}^{u})\\) あるいは \\((x_{0}^{1},x_{0}^{2})\\) が定まれば, 式(7.1) を繰り返し適用することで解軌道を決定できる. 最後に, 解が存在しないケースと不決定の場合について述べる. まず, 固有値が2つとも不安定固有値である場合, すなわち \\(n_{u}=n=2\\), \\(0=n_{s}&lt;n_{1}=1\\) のケースを考えよう. 図7.5 で示されている通り, 後ろ向き (forward-looking) サブシステム \\(y_{t}^{u}=J_{u}^{-1}y_{t+1}^{u}-J_{u}^{-1}C_{u}u_{t}\\) は一点 \\((y_{0}^{u1},y_{0}^{u2})\\) を定める. 赤色の実線と一般には交わらないので, 解が存在しない. 次に \\(n_{u}=0\\), \\(2=n_{s}&gt;n_{1}=1\\) のケースを考える. この場合には後ろ向きのサブシステムはないので, \\(y_{0}\\) に関して情報を得ることができない. したがって, 2次元平面\\((y^{s1},y^{s2})\\)のすべての点が \\((y_{0}^{s1},y_{0}^{s2})\\) の候補となる (図 7.6). 青色領域と赤色直線が交わる点は1次元の自由度を持つ無限個の点の集まりであるから \\(x_{0}^{2}\\) を定めることもできない. 図 7.5: \\(n_{s}&lt;n_{1}\\) のケース. バックワードサブシステムが過剰に初期値を定めてしまう. 一般に解は存在しない. 図 7.6: \\(n_{s}&gt;n_{1}\\) のケース. バックワードサブシステムの情報が不足しており, 赤色直線上の任意の点から出発した経路が解の性質を満たす. 7.3 \\(E \\neq I\\) のケース 7.3.1 アイデア 次にシステム方程式がデスクリプタ方程式 \\[ Ex_{t+1}=Ax_{t}+Bu_{t},\\quad t=0,1,\\dots,\\tag{7.12} \\] で与えられている場合の解を求めよう. 前節と同様に, 初期時点における変数 \\[ x_{0}=\\begin{bmatrix}x_{0}^{1}\\\\ x_{0}^{2} \\end{bmatrix}\\in\\mathbb{R}^{n_{1}}\\times\\mathbb{R}^{n_{2}} \\] について \\(x_{0}^{1}=\\bar{x}_{0}^{1}\\in\\mathbb{R}^{n_{1}}\\) のみ拘束条件が与えられているとする. \\(E=I\\) のケースでは \\(A\\) のジョルダン標準形を求めることで解を計算したが, \\(E\\neq I\\) の場合, 特に \\(\\det E\\neq0\\) の場合には\\((E,A)\\) のワイエルシュトラス標準形によって解を特徴付ける. 資料 {[}16ED04{]} で見たとおり, 適当な正則行列 \\(V,W\\) が存在して, \\[\\begin{align} W^{-1}EV &amp; =\\begin{bmatrix}I\\\\ &amp; N \\end{bmatrix},\\tag{7.13}\\\\ W^{-1}AV &amp; =\\begin{bmatrix}J\\\\ &amp; I \\end{bmatrix},\\tag{7.14} \\end{align}\\] が成り立つ . ここで, \\(J\\) は\\((E,A)\\) の有限固有値を対角成分にもつジョルダン標準形行列, \\(N\\) は対角成分がすべてゼロであるジョルダン標準形行列である. もちろん \\(\\det E\\neq0\\) であれば, \\(N\\) は現れない. ここで, \\(J\\) は \\[\\begin{equation} J=\\begin{bmatrix}J_{fs}\\\\ &amp; J_{fu} \\end{bmatrix}\\tag{7.15} \\end{equation}\\] というブロックに分かれており, \\(J_{fs}\\) の対角成分に絶対値が1以下の一般固有値, \\(J_{fu}\\)の対角成分に絶対値が1未満の一般固有値が並ぶようにする. \\(V\\), \\(W\\) の選び方は {[}16ED04{]} で解説した通りである. \\(V\\) は一般化固有ベクトルを並べたもの, \\(W\\) はそれに \\((E,A)\\) を掛けたものであるから, ベクトルの並びによって固有値の順序を自由に設定できる. 変数変換 \\(y=V^{-1}x\\), 変数の分割 \\(y=(y^{fs},y^{fu},y^{b})\\in\\mathbb{R}^{n_{fs}}\\times\\mathbb{R}^{n_{fu}}\\times\\mathbb{R}^{n_{b}}\\), \\(W^{-1}B=(C_{fs},C_{fu},C_{b})\\) を施して \\[ \\begin{bmatrix}I_{n_{fs}}\\\\ &amp; I_{n_{fu}}\\\\ &amp; &amp; N \\end{bmatrix}\\begin{bmatrix}y_{t+1}^{fs}\\\\ y_{t+1}^{fu}\\\\ y_{t+1}^{b} \\end{bmatrix}=\\begin{bmatrix}J_{fs}\\\\ &amp; J_{fu}\\\\ &amp; &amp; I \\end{bmatrix}\\begin{bmatrix}y_{t}^{fs}\\\\ y_{t}^{fu}\\\\ y_{t}^{b} \\end{bmatrix}+\\begin{bmatrix}C_{fs}\\\\ C_{fu}\\\\ C_{b} \\end{bmatrix}u_{t}. \\] を得る. このシステムを以下のように2つのサブシステムに分解する. \\[\\begin{align} y_{t+1}^{fs} &amp; =J_{fs}y_{t}^{fs}+C_{fs}u_{t}\\tag{7.16}\\\\ \\begin{bmatrix}y_{t}^{fu}\\\\ y_{t}^{b} \\end{bmatrix} &amp; =\\begin{bmatrix}J_{fu}^{-1}\\\\ &amp; N \\end{bmatrix}\\begin{bmatrix}y_{t+1}^{fu}\\\\ y_{t+1}^{b} \\end{bmatrix}-\\begin{bmatrix}J_{fu}^{-1}C_{fu}\\\\ C_{b} \\end{bmatrix}u_{t}\\tag{7.17}\\\\ &amp; =:\\hat{J}_{u}y_{t+1}^{u}-D_{u}u_{t}. \\end{align}\\] {[}16ED04{]} の場合と異なり, 式 (7.17) が不安定な有限固有値に対応するサブシステムを時間反転して付加していることに注意してほしい. \\((E,A)\\) の有限不安定固有値と無限大固有値をいずれも不安定固有値とみなすならば, 前節の結果を応用すれば解軌道が求まることも想像に難くないだろう. ただし, \\(N\\) の非正則性に伴うデリケートな問題が生じることに注意をしておこう. システム(7.1) では, 初期値を定めるために後ろ向きのシステムを作ったが, あくまでも順方向に決定されるシステムであり, 再帰的な構造を見つけることができた. しかし, システム (7.12) には無限大固有値に対応する本質的に非因果的な成分が含まれるので, \\(\\det E=0\\) である場合, 一般には再帰的に (因果的に) 記述できるとは限らない. 7.3.2 解の存在と一意性 まずは, 式 (7.17) を解いてみよう. \\[\\begin{align} y_{t}^{fu} &amp; =-\\sum_{k=0}^{\\infty}J_{fu}^{-k-1}C_{fu}u_{t+k},\\tag{7.18}\\\\ y_{t}^{b} &amp; =N\\left(Ny_{t+2}^{b}-C_{b}u_{t+1}\\right)-C_{b}u_{t}\\nonumber \\\\ &amp; =N^{n_{b}}y_{t+n_{b}}^{b}-\\sum_{k=0}^{n_{b}-1}N^{k}C_{b}u_{t+k}\\nonumber \\\\ &amp; =-\\sum_{k=0}^{n_{b}-1}N^{k}C_{b}u_{t+k}.\\tag{7.19} \\end{align}\\] ここでも, \\(\\lim_{T}\\|J_{fu}^{-T}C_{fu}u_{T}\\|=0\\) と \\(y_{t}^{fu}\\) に現れる級数の収束性が成り立つように \\(u_{t}\\) を制限しているものとする. 初期時点の変数を決定する線形方程式は次式で与えられる. \\[\\begin{align} \\begin{bmatrix}\\bar{x}_{0}^{1}\\\\ x_{0}^{2} \\end{bmatrix} &amp; =\\left[\\begin{array}{c|cc} V_{1fs} &amp; V_{1fu} &amp; V_{1b}\\\\ \\hline V_{2fs} &amp; V_{2fu} &amp; V_{2b} \\end{array}\\right]\\left[\\begin{array}{c} y_{0}^{fs}\\\\ \\hline \\bar{y}_{0}^{fu}\\\\ \\bar{y}_{0}^{b} \\end{array}\\right].\\tag{7.20}\\\\ &amp; =:\\left[\\begin{array}{c|c} V_{1fs} &amp; V_{1u}\\\\ \\hline V_{2fs} &amp; V_{2u} \\end{array}\\right]\\begin{bmatrix}y_{0}^{fs}\\\\ \\bar{y}_{0}^{u} \\end{bmatrix} \\end{align}\\] すでに決定している変数には上線を付している. 第7.2節のケースと同様に式(7.20)を \\((y_{0}^{fs},x_{0}^{2})\\) について解くことができれば解軌道の存在が示される. 未知変数を左辺に集めると, \\[\\begin{equation} \\begin{bmatrix}V_{1fs} &amp; 0\\\\ V_{2fs} &amp; -I \\end{bmatrix}\\begin{bmatrix}y_{0}^{fs}\\\\ x_{0}^{2} \\end{bmatrix}=\\begin{bmatrix}I &amp; -V_{1fu} &amp; -V_{1b}\\\\ 0 &amp; -V_{2fu} &amp; -V_{2b} \\end{bmatrix}\\begin{bmatrix}\\bar{x}_{0}^{1}\\\\ \\bar{y}_{0}^{fu}\\\\ \\bar{y}_{0}^{b} \\end{bmatrix}=\\begin{bmatrix}I &amp; -V_{1u}\\\\ 0 &amp; -V_{2u} \\end{bmatrix}\\begin{bmatrix}\\bar{x}_{0}^{1}\\\\ \\bar{y}_{0}^{u} \\end{bmatrix}\\tag{7.21} \\end{equation}\\] が得られる. 式(7.20) から (7.21) は \\(t=0\\) について記述しているが, \\(t=0\\) 以外の時点でも同様に成り立つことに注意せよ. 定理 7.3 の拡張となる次の結果を得る. 定理 7.4 システム (7.12) に関して, \\(n_{1}=n_{fs}\\) かつ \\(\\det V_{1fs}\\neq0\\) が成り立てば, \\((y_{0}^{fs},x_{0}^{2})\\) および解が一意的に定まる. 式 (7.21) を用いて, 解の不存在および不決定性が起こる場合の議論を完成させなさい. ここで得られた結果は, 有限不安定固有値と無限大固有値の区別に依存していないことに注意せよ. 無限大固有値を不安定固有値とみなせば Blanchard-Kahn の定理7.3 の条件とまったく同じものと言ってよい. ただし, 解公式は同じにはならない. 前節の \\(J_{u}^{-1}\\) は可逆 (これは当たり前のことである) であったのに対して, \\(\\det E=0\\) の場合 \\(\\hat{J}_{u}\\) は可逆ではない. したがって, 前節とは違って後ろ向きの方程式に対応する順方向の方程式は存在せず, 定理 7.3 の公式は適用できない. 7.3.3 \\(u\\) の特定化 一般の \\(u\\) について解を特徴付けようとするとかなり煩雑になるので, Klein (2000) に倣って \\(u\\) に強い条件を課した上で再帰公式を求めよう. Klein (2000) では \\(u\\) が状態方程式 \\[ u_{t+1}=\\Phi u_{t},\\quad\\Phi\\in\\mathbb{R}^{m\\times m},\\quad u_{0}:\\text{ given} \\] に従うと仮定する.28 式 (7.18), (7.19) を次のように書き換えておく.29 \\[ y_{t}^{u}=-\\sum_{k=0}^{\\infty}\\hat{J}_{u}^{k}D_{u}u_{t+k}. \\] \\(u\\) に対する仮定から\\(u_{t+k}=\\Phi^{k}u_{t}\\), \\(k=0,1,\\dots\\), が成り立つので, \\[ \\begin{aligned} y_{t}^{u} &amp; =-\\left(\\sum_{k=0}^{\\infty}\\hat{J}_{u}^{k}D_{u}\\Phi^{k}\\right)u_{t}.\\\\ &amp; =:Mu_{t}. \\end{aligned} \\] 行列\\(M\\)を計算しよう. 簡単な計算から \\[ M =-\\left(\\sum_{k=0}^{\\infty}\\hat{J}_{u}^{k}D_{u}\\Phi^{k}\\right) =-D_{u}-\\sum_{k=1}^{\\infty}\\hat{J}_{u}^{k}D_{u}\\Phi^{k} =-D_{u}+\\hat{J}_{u}M\\Phi \\] が得られ, \\(M\\) は離散時間シルベスタ方程式の解であることが分かる. 定理 7.2 より, \\[ \\mathrm{vec}(M)=\\left[\\left(\\Phi^{\\top}\\otimes\\hat{J}_{u}\\right)-I\\right]^{-1}\\mathrm{vec}(D_{u}). \\] さらに計算を進めよう. \\[ \\begin{aligned} \\begin{bmatrix}x_{t+1}^{1}\\\\ x_{t+1}^{2} \\end{bmatrix} &amp; =\\begin{bmatrix}V_{1fs} &amp; V_{1u}\\\\ V_{2fs} &amp; V_{2u} \\end{bmatrix}\\begin{bmatrix}y_{t+1}^{fs}\\\\ y_{t+1}^{u} \\end{bmatrix}\\\\ &amp; =\\begin{bmatrix}V_{1fs} &amp; V_{1u}\\\\ V_{2fs} &amp; V_{2u} \\end{bmatrix}\\begin{bmatrix}J_{fs}y_{t}^{fs}+C_{fs}u_{t}\\\\ Mu_{t+1} \\end{bmatrix}\\\\ &amp; =\\begin{bmatrix}V_{1fs} &amp; V_{1u}\\\\ V_{2fs} &amp; V_{2u} \\end{bmatrix}\\begin{bmatrix}J_{fs}V_{1fs}^{-1}\\left(x_{t}^{1}-V_{1u}y_{t}^{u}\\right)+C_{fs}u_{t}\\\\ Mu_{t+1} \\end{bmatrix}\\\\ &amp; =\\begin{bmatrix}V_{1fs} &amp; V_{1u}\\\\ V_{2fs} &amp; V_{2u} \\end{bmatrix}\\begin{bmatrix}J_{fs}V_{1fs}^{-1}x_{t}^{1}+(C_{fs}-J_{fs}V_{1fs}^{-1}V_{1u}M)u_{t}\\\\ Mu_{t+1} \\end{bmatrix}. \\end{aligned} \\] \\(x_{t+1}^{2}\\) には\\(t\\) 期以前の情報が入ってほしくないので, 前節と同様に過去の情報を消去する. \\[ \\begin{aligned} V_{2fs}V_{1fs}^{-1}x_{t+1}^{1} &amp; =V_{2fs}J_{fs}V_{1fs}^{-1}x_{t}^{1}+V_{2fs}(C_{fs}-J_{fs}V_{1fs}^{-1}V_{1u}M)u_{t}+V_{2fs}V_{1fs}^{-1}V_{1u}Mu_{t+1}\\\\ x_{t+1}^{2} &amp; =V_{2fs}J_{fs}V_{1fs}^{-1}x_{t}^{1}+V_{2fs}(C_{fs}-J_{fs}V_{1fs}^{-1}V_{1u}M)u_{t}+V_{2u}Mu_{t+1}. \\end{aligned} \\] 上式から下式を引いて \\[ V_{2fs}V_{1fs}^{-1}x_{t+1}^{1}-x_{t+1}^{2}=\\left(V_{2fs}V_{1fs}^{-1}V_{1u}-V_{2u}\\right)Mu_{t+1} \\] を得る. これを \\(x_{t+1}^{2}\\) について解けばよい. したがって, \\[ \\begin{aligned} x_{t+1}^{1} &amp; =V_{1fs}J_{fs}V_{1fs}^{-1}x_{t}^{1}+\\left[V_{1fs}C_{fs}-V_{1fs}J_{fs}V_{1fs}^{-1}V_{1u}M+V_{1u}M\\Phi\\right]u_{t}\\\\ x_{t}^{2} &amp; =V_{2fs}V_{1fs}^{-1}x_{t}^{1}+\\left(V_{2u}-V_{2fs}V_{1fs}^{-1}V_{1u}\\right)Mu_{t}. \\end{aligned} \\] 随所に \\(u_{t+1}=\\Phi u_{t}\\) を用いている. 定理としてまとめよう. 定理 7.5 \\(n_{1}=n_{fs}\\) かつ \\(\\det V_{1fs}\\neq0\\) とする. このとき, (7.12) の解は一意に定まる. 外生変数 が \\(u_{t+1}=\\Phi u_{t}\\) に従うとすれば, \\(x_{t}\\), \\(t=0,1,\\dots\\) , は次式で与えられる. \\[ \\begin{aligned} x_{t+1}^{1} &amp; = V_{1fs}J_{fs}V_{1fs}^{-1}x_{t}^{1} + \\left[ V_{1fs}C_{fs}-V_{1fs}J_{fs}V_{1fs}^{-1}V_{1u}M + V_{1u}M\\Phi \\right] u_{t} \\\\ x_{t}^{2} &amp; = V_{2fs}V_{1fs}^{-1}x_{t}^{1} + \\left( V_{2u}-V_{2fs}V_{1fs}^{-1}V_{1u} \\right) Mu_{t}.\\\\ x_{0}^{1} &amp; :\\ \\text{given}. \\end{aligned} \\] ただし\\(M\\)は \\[ \\mathrm{vec}(M)=\\left[\\left(\\Phi^{\\top}\\otimes\\hat{J}_{u}\\right)-I\\right]^{-1}\\mathrm{vec}\\left(D_{u}\\right) \\] によって決まる行列. 定理 7.3 において \\(u_{t+1}=\\Phi u_{t}\\) を仮定したときの解の公式を導出せよ. 参考文献 "],
["schurqz.html", "第8章 Schur分解・QZ分解 8.1 直交性 8.2 シューア分解とその応用 8.3 QZ 分解とその応用 8.4 QZ解法の一般公式", " 第8章 Schur分解・QZ分解 今回はシステム軌道の数値計算を行う上で重要なSchur分解, QZ 分解について解説する. QZ分解は Klein (2000) や Sims (2001) で使われている実用的な方法である. Schur 分解は \\(E=I\\) のケースで, つまり Blanchard and Kahn (1980) の仮定の下で利用できる. 本稿では内積空間に関する基本事項を復習し, Schur 分解, Schur分解を用いたシステム分析, QZ分解, QZ分解を用いたシステム分析の順に解説する. 8.1 直交性 \\(X\\) を\\(\\mathbb{F}\\) 上の線形空間とする. 8.1.1 内積空間 定義 8.1 (内積空間の公理) \\(\\mathbb F\\) 上の線形空間 \\(X\\) が内積空間であるとは, 内積と呼ばれる写像 \\[ \\begin{aligned} (\\cdot,\\cdot):X\\times X &amp; \\to\\mathbb{F} \\end{aligned} \\] が定義されていて, 次の性質を満たすことをいう. 任意の \\(x\\in X\\) について \\((x,x)\\ge0\\), \\(x=0\\) のとき, またそのときに限り \\((x,x)=0\\); 任意の \\(x,y\\in X\\) について \\((y,x)=\\overline{(x,y)}\\); 任意の \\(x,y\\in X\\), \\(\\alpha\\in\\mathbb{F}\\) について \\((\\alpha x,y)=\\alpha(x,y)\\); 任意の \\(x,y,z\\in X\\) について \\((x+y,z)=(x,z)+(y,z)\\). 例 8.1 \\(X=\\mathbb{C}^{n}\\) とする. \\(X\\)に \\[ (x,y):=\\sum_{k=1}^{n}x_{k}\\bar{y}_{k}=y^{*}x \\] を導入した空間は内積空間の公理を満たす. 例 8.2 分散有限な\\(\\mathbb{R}^{n}\\)値確率変数全体からなる線形空間 \\(L^{2}(\\mathbb{R}^{n};P)\\) に \\[ (x,y):=\\mathbb{E}\\left[y^{\\top}x\\right]:=\\int_{\\mathbb{R}^{n}}y(\\omega)^{\\top}x(\\omega)P(d\\omega). \\] を導入すると, \\(L^{2}(\\mathbb{R}^{n};P)\\) は内積空間になる. 定義 8.2 \\(X\\) を内積空間とする. \\(x,y\\in X\\) が \\((x,y)=0\\) を満たすとき, \\(x\\) と \\(y\\) は直交しているといい, \\(x\\perp y\\) と書く. 定義 8.3 \\(X\\) を有限次元の内積空間とする. \\(X\\) の基底 \\(\\{v_{1},\\dots,v_{n}\\}\\) が \\[ \\begin{aligned} \\|v_{i}\\|=1, &amp; \\quad i=1,\\dots,n\\\\ v_{i}\\perp v_{j} &amp; \\quad i\\neq j \\end{aligned} \\] を満たすとき, \\(\\{v_{1},\\dots,v_{n}\\}\\) を正規直交基底であるという. 定義 8.4 部分空間 \\(M\\) の直交補空間 \\(M^{\\perp}\\)とは \\[ M^{\\perp}:=\\left\\{ x\\in X\\ \\mid\\ m\\perp x\\ \\forall m\\in M\\right\\} \\] のことである. あるいは, 全空間を明示して\\(X\\ominus M\\) と書く. 8.1.2 内積空間と基底 \\(X\\) を有限次元の内積空間とする. 内積は基底とは無関係に定義されるものであることに注意する. 内積空間 \\(X\\) に基底を導入し, 内積の座標表現を調べてみよう. \\(V = [v_1\\ \\cdots\\ v_n ]\\) をある正規直交基底とする. このとき, ベクトル \\(x, y \\in X\\) \\[ \\begin{aligned} x &amp;= x_1^V v_1 + \\cdots + x_n^V v_n, \\quad x^V = (x_1^V, \\dots, x_n^V)^\\top \\in \\mathbb F^n \\\\ y &amp;= y_1^V v_1 + \\cdots + y_n^V v_n, \\quad y^V = (y_1^V, \\dots, y_n^V)^\\top \\in \\mathbb F \\end{aligned} \\] について \\[ (x, y) = \\sum_{k = 1}^n x_k^V \\bar{y}_k^V = (y^V)^* (x^V) \\] が成り立つことに注意せよ。我々がよく知っている内積の定義は, 正規直交基底を導入することで自然に導かれるものである. ベクトルの組 \\(W = [w_1\\ \\cdots\\ w_n]\\) を正規直交基底とは限らない一般の基底, \\[ \\begin{aligned} x &amp;= x_1^W W_1 + \\cdots + x_n^W W_n, \\quad x^W = (x_1^W, \\dots, x_n^W)^\\top \\in \\mathbb F^n \\\\ y &amp;= y_1^W W_1 + \\cdots + y_n^W W_n, \\quad y^W = (y_1^W, \\dots, y_n^W)^\\top \\in \\mathbb F \\end{aligned} \\] \\[ (x, y) = \\sum_{i = 1}^n \\sum_{j = 1}^n x_i^W \\bar{y}_j^W (w_i, w_j) = (y^W)^* G^W (x^W) \\] ただし \\[ G^W = \\begin{bmatrix} (w_1, w_1) &amp; \\cdots &amp; (w_1, w_n) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ (w_n, w_1) &amp; \\cdots &amp; (w_n, w_n) \\end{bmatrix} \\] とした. \\(G^W\\) はHermite行列である. 基底の内積からなる \\(G^W\\) のような行列を Gram行列と呼ぶ. Gram行列は必ず正定値行列になる. 実際, \\[ (x^W)^*G^W x^W = (x, x) \\ge 0, \\] 等号が成立するのは \\(x = 0\\) のときのみである. 逆に, 正定置行列 \\(A\\) が与えられたとき, \\((\\xi, \\eta) \\mapsto \\eta^*A\\xi\\) は \\(\\mathbb F^n\\) の内積である. 次に, 転置行列（共役転置行列）を線形空間において特徴づけしよう. 定義 8.5 \\(X\\), \\(Y\\) を内積空間とする. 線形写像 \\(f:X\\to Y\\) の随伴写像 \\(f^{*}:Y\\to X\\) とは, 任意の \\(x\\in X\\) と \\(y\\in Y\\) について \\[ (fx,y)_{Y}=(x,f^{*}y)_{X} \\] が成り立つ線形写像のことである. \\(\\mathbb F^n\\) に通常の内積を導入したときに, 共役転置行列 \\(A^*\\) が上の性質を満たしていることを確認せよ. 8.1.3 内積空間のノルム 内積空間には「ベクトルの長さ」を表すノルム関数が自然に定義される. 定義 8.6 (内積によって導入されるノルム) \\(X\\) を内積空間とする. 写像 \\(\\|\\cdot\\|:X\\to\\mathbb{R}_{+}\\) \\[ \\begin{aligned} \\|\\cdot\\|:X &amp; \\to\\mathbb{R}_{+}\\\\ x &amp; \\mapsto\\sqrt{(x,x)} \\end{aligned} \\] は次の性質をもつ. \\(x=0\\) のとき, またそのときに限り \\(\\|x\\|=0\\); 任意の \\(x\\in X\\), \\(\\alpha\\in\\mathbb{F}\\) に対して, \\(\\|\\alpha x\\|=|\\alpha|\\|x\\|\\); 任意の \\(x,y\\in X\\) に対して, \\(\\|x+y\\|\\le\\|x\\|+\\|y\\|\\); このような性質を持つ関数をノルムと呼ぶ. 内積空間には自然なノルムが存在する. ノルム関数は内積とは無関係に定義できるものである. 内積空間とは限らない線形空間にノルムを導入した空間をノルム空間という. 内積から導入されたノルムが3つ目の性質（三角不等式と呼ばれる）を満たすことは証明が必要である. まず, 次の定理を紹介する. 定理 8.1 (コーシー・シュワルツ) \\(X\\) を内積空間とする. 任意の \\(x,y\\in X\\) について \\[\\begin{equation} \\left|(x,y)\\right|\\le\\|x\\|\\|y\\| (\\#eq:eq:uni-cs) \\end{equation}\\] が成り立つ. \\(x\\) と \\(y\\) が1次従属のとき, またそのときに限り等号が成立する. 証明. \\(x,y\\in X\\) を任意にとる. \\(y=0\\) のとき自明であるから, \\(y\\neq0\\) とする. さらに, \\(t\\in\\mathbb{F}\\) を固定する. \\[ \\begin{aligned} 0 &amp; \\le(x+ty,x+ty)\\\\ &amp; =(x,x)+(x,ty)+(ty,x)+(ty,ty)\\\\ &amp; =\\|x\\|^{2}+\\bar{t}(x,y)+t\\overline{(x,y)}+|t|^{2}\\|y\\|^{2} \\end{aligned} \\] ここで, \\(t=-(x,y)/\\|y\\|^{2}\\) を代入すると, \\[ 0\\le\\|x\\|^{2}-\\frac{\\overline{(x,y)}}{\\|y\\|^{2}}(x,y)-\\frac{(x,y)}{\\|y\\|^{2}}\\overline{(x,y)}+\\frac{(x,y)\\overline{(x,y)}}{\\|y\\|^{4}}\\|y\\|^{2}, \\] したがって \\[ \\left|(x,y)\\right|^{2}\\le\\|x\\|^{2}\\|y\\|^{2} \\] を得る. 等号で成立する場合には, \\((x+ty,x+ty)=0\\), すなわち \\(x+ty=0\\) なる \\(t\\) が存在するということである. \\(y\\neq0\\) を仮定しているので, これは1次独立性に外ならない. 逆に, 1次従属であるときに等号成立することは内積とノルムの定義により明らか. 命題 8.1 内積によって導入されたノルムは三角不等式を満たす. 証明. 定義に従って, \\[ \\begin{aligned} \\|x+y\\|^{2} &amp; =\\left|(x+y,x+y)\\right|\\\\ &amp; \\le(x,x)+\\left|(x,y)\\right|+\\left|(y,x)\\right|+(y,y)\\\\ &amp; =\\|x\\|^{2}+2\\left|(x,y)\\right|+\\|y\\|^{2}\\\\ &amp; \\le\\|x\\|^{2}+2\\|x\\|\\|y\\|+\\|y\\|^{2}\\\\ &amp; =\\left(\\|x\\|+\\|y\\|\\right)^{2}. \\end{aligned} \\] 4行目の不等式にコーシー・シュワルツの不等式を使った. ユニタリ行列とは \\[ U^{*}U=UU^{*}=I \\] を満たす行列のことであった. ユニタリ行列 \\(U\\) の列は \\(\\mathbb{F}^{n}\\) の正規直交基底を成す. ユニタリ変換は内積を保存する. なぜなら, \\[ (Ux,Uy)=(x,U^{*}Uy)=(x,y). \\] \\(U\\) をユニタリとすると, \\(|\\det U|=1\\). 8.1.4 グラム・シュミットの直交化法 \\(X\\) を内積空間, \\(v_{1},v_{2},\\dots,v_{k}\\in X\\) は1次独立であるとする. このとき, \\[ \\mathrm{span}\\{v_{1},\\dots,v_{k}\\}=\\mathrm{span}\\{u_{1},\\dots,u_{k}\\} \\] であって, \\[ u_{i}\\perp u_{j},\\quad i\\neq j \\] なる1次独立なベクトルの組 \\(u_{1},\\dots,u_{k}\\) で, \\(\\|u_{1}\\|=\\cdots=\\|u_{k}\\|=1\\) となるものが必ず存在する. 実際に構成してみよう. 考え方としては, \\[ u_{i+1}\\in\\mathrm{span}\\{v_{1},\\dots,v_{i+1}\\}\\ominus\\mathrm{span}\\{v_{1},\\dots,v_{i}\\},\\quad i=1,2,3,\\dots \\] が成り立つように, 順に \\(u_{1},u_{2},\\dots\\) を見つけていく. 最初のステップでは, \\[ \\bar{u}_{1}:=v_{1},\\qquad u_{1}:=\\frac{\\bar{u}_{1}}{\\|\\bar{u}_{1}\\|} \\] とする. 次に \\(\\bar{u}_{2}\\perp\\mathrm{span}\\{u_{1}\\}\\) なる \\(\\bar{u}_{2}\\in\\mathrm{span}\\{v_{1},v_{2}\\}=\\mathrm{span}\\{u_{1},v_{2}\\}\\) を見つける. \\[ \\bar{u}_{2}=\\alpha_{1}u_{1}+\\alpha_{2}v_{2} \\] と仮において, 直交性の条件から \\[ \\begin{aligned} (\\bar{u}_{2},u_{1}) &amp; =(\\alpha_{1}u_{1}+\\alpha_{2}v_{2},u_{1})\\\\ &amp; =\\alpha_{1}(u_{1},u_{1})+\\alpha_{2}(v_{2},u_{1})\\\\ &amp; =0 \\end{aligned} \\] が成り立つように \\(\\alpha_{1},\\alpha_{2}\\) を選べばよい. \\((u_{1},u_{1})=1\\) としたので, 例えば \\[ \\alpha_{1}=-(v_{2},u_{1}),\\quad\\alpha_{2}=1 \\] とすれば直交性が満たされる. したがって, \\[ \\bar{u}_{2}=v_{2}-(v_{2},u_{1})u_{1},\\qquad u_{2}=\\frac{\\bar{u}_{2}}{\\|u_{2}\\|}. \\] 同様の手続きを繰り返せばよいので, 詳細は省略する. 続きの証明を各自で完成させること. 結果的だけ述べると \\(i=2,3,\\dots,k\\) については, \\(u_{i}\\) は次の公式で与えられる. \\[ \\bar{u}_{i}=v_{i}-\\sum_{j=1}^{i-1}(v_{i},u_{j})u_{j},\\qquad u_{i}=\\frac{\\bar{u}_{i}}{\\|\\bar{u}_{i}\\|}. \\] この直交化のアルゴリズムをグラム・シュミットの直交化法という. \\(X\\) を有限次元内積空間, \\(M\\) をその部分空間とする. このとき, \\[ X=M\\oplus M^{\\perp}. \\] 証明. \\(x\\in M\\cap M^{\\perp}\\) とする. 定義から \\((x,x)=0\\) とならなければならないので, \\(x=0\\) が従う. シュミットの直交化により\\(M\\) の正規直交基底が存在する. さらに \\((\\dim X-\\dim M)\\) 本のベクトルを追加して \\(X\\) の基底を構成できる. シュミットの方法で直交化しておけば, 新しく追加したベクトルは \\(M^{\\perp}\\) に含まれている. 定理 8.2 任意の行列 \\(A\\in\\mathbb{F}^{m\\times n}\\) に対して, ユニタリ行列 \\(Q\\in\\mathbb{F}^{m\\times m}\\) と 上三角行列 \\(R\\in\\mathbb{F}^{m\\times n}\\) が存在して, \\(A=QR\\) が成り立つ. 証明. 証明の方針を述べる. \\(\\mathbb{F}^{n}\\) の標準基底を \\([e_{1}\\ \\cdots\\ e_{n}]\\) とする. \\(\\mathrm{rank}A=\\bar{m}&lt;\\min\\{m,n\\}\\) として \\([e_{1}\\ \\cdots\\ e_{\\bar{m}}]\\) が \\(\\mathrm{im}A\\) の基底になるように基底の順序を入れ替えておく. 順序交換はユニタリ変換である. 像 \\([Ae_{1}\\ \\cdots\\ Ae_{\\bar{m}}]=[A_{1}\\ \\cdots\\ A_{\\bar{m}}]\\subset\\mathbb{F}^{m}\\) にグラム・シュミットの方法を適用して互いに直交するノルム1のベクトルの組 \\([u_{1}\\ \\cdots\\ u_{\\bar{m}}]\\) を構成すると, 適当な \\(\\alpha_{ij}\\), \\(i\\ge j\\) に対して \\[ \\begin{aligned} Ae_{1} &amp; =\\alpha_{11}u_{1},\\\\ Ae_{2} &amp; =\\alpha_{21}u_{1}+\\alpha_{22}u_{2}\\\\ &amp; \\vdots\\\\ Ae_{\\bar{m}} &amp; =\\alpha_{n1}u_{1}+\\alpha_{n2}u_{2}+\\cdots+\\alpha_{\\bar{m}\\bar{m}}u_{\\bar{m}} \\end{aligned} \\] とできる. すなわち, \\[ A[e_{1}\\ \\cdots\\ e_{\\bar{m}}]=[u_{1}\\ \\cdots\\ u_{\\bar{m}}]\\left[\\begin{array}{cccc} \\alpha_{11} &amp; * &amp; * &amp; *\\\\ &amp; \\alpha_{22} &amp; * &amp; *\\\\ &amp; &amp; \\ddots &amp; *\\\\ &amp; &amp; &amp; \\alpha_{\\bar{m}\\bar{m}} \\end{array}\\right]=:QR. \\] \\(n=m=\\bar{m}\\) であれば, ここで分解 \\(A=QR\\) が完成している. \\(n=m&gt;\\bar{m}\\) であれば \\([u_{1}\\ \\cdots\\ u_{\\bar{m}}]\\) に適当なベクトルを付け加えて \\([u_{1}\\ \\cdots\\ u_{\\bar{m}}\\ u_{\\bar{m}+1}\\ \\cdots\\ u_{m}]\\) が正規直交基底になるようにできるので \\[ A[e_{1}\\ \\cdots\\ e_{\\bar{m}}\\ e_{\\bar{m}+1}\\ \\cdots\\ e_{n}]=[u_{1}\\ \\cdots\\ u_{\\bar{m}}\\ u_{\\bar{m}+1}\\ \\cdots\\ u_{m}]\\left[\\begin{array}{cccc|cc} \\alpha_{11} &amp; * &amp; * &amp; * &amp; * &amp; *\\\\ &amp; \\alpha_{22} &amp; * &amp; * &amp; * &amp; *\\\\ &amp; &amp; \\ddots &amp; * &amp; * &amp; *\\\\ &amp; &amp; &amp; \\alpha_{\\bar{m}\\bar{m}} &amp; * &amp; *\\\\ \\hline &amp; &amp; &amp; &amp; 0 &amp; 0\\\\ &amp; &amp; &amp; &amp; &amp; 0 \\end{array}\\right]=:QR. \\] \\(n\\neq m\\) であっても同様の考え方で証明できる. \\(n&lt;m\\) の場合にはゼロからなる行を最下部に付け加える. \\(n&gt;m\\) の場合は, 任意の数からなる列を最右部に付け加える. 8.2 シューア分解とその応用 行列の対角化やジョルダン標準化は, 行列表現が良い形になるような基底を探すことである. このような基底は一般には正規直交基底になるとは限らないが, すべての行列は適当な正規直交基底に関して三角行列に表現できることが知られている. この結果を使うと, 正規行列はユニタリ行列で対角化できることを証明できる. 例えば, エルミート行列はユニタリ行列で対角化できる. 8.2.1 シューア分解 定理 8.3 任意の正方行列 \\(A\\in\\mathbb{F}^{n\\times n}\\) に対して, あるユニタリ行列 \\(U\\in\\mathbb{F}^{n\\times n}\\) が存在して, \\(U^{-1}AU=U^{*}AU\\)を上三角行列にできる. \\(U^{-1}AU\\) の対角成分には\\(A\\)の固有値が並ぶ. 証明. \\(A\\) をジョルダン標準化する基底を \\([v_{1}\\ v_{2}\\ \\cdots\\ v_{n}]\\) とする. 基底の構成により, \\[ \\mathcal{V}_{k}:=\\mathrm{span}\\{v_{1},\\dots,v_{k}\\} \\] は不変部分空間の拡大列 \\[ \\{0\\}\\subsetneq\\mathcal{V}_{1}\\subsetneq\\mathcal{V}_{2}\\subsetneq\\cdots\\subsetneq\\mathcal{V}_{n-1}\\subsetneq\\mathcal{V}_{n}=\\mathbb{C}^{n} \\] を形成する. \\(A\\mathcal{V}_{k}\\subset\\mathcal{V}_{k}\\), \\(k=1,\\dots,n\\), だから \\(u_{k}\\in\\mathcal{V}_{k}\\setminus\\mathcal{V}_{k-1}\\) が成り立つように基底 \\([\\bar{u}_{1}\\ \\cdots\\ \\bar{u}_{n}]\\) を選べば, 上三角行列で表現される. グラム・シュミットの方法で \\([\\bar{u}_{1}\\ \\cdots\\ \\bar{u}_{n}]\\) に正規直交化を施せば \\(U^{-1}AU=T=[t_{ij}]\\) が上三角行列になるようなユニタリ行列 \\(U=[u_{1}\\ \\cdots\\ u_{n}]\\) が見つかる. \\(T\\) の対角成分に\\(A\\) の固有値が並ぶことは次のようにして分かる. 任意の \\(k\\) について, \\(u_{k}\\in\\mathcal{V}_{k}\\setminus\\mathcal{V}_{k-1}\\) は \\(u_{k}\\in\\alpha_{k}v_{k}+\\mathcal{V}_{k-1}\\), \\(\\alpha_{k}\\neq0\\), を満たすので, \\[ Au_{k}\\in\\alpha_{k}Av_{k}+A\\mathcal{V}_{k-1}\\subset\\alpha_{k}Av_{k}+\\mathcal{V}_{k-1}. \\] \\(v_{k}\\) が固有ベクトルであれば, \\(Av_{k}=\\lambda_{k}v_{k}\\), 高次の一般固有ベクトルであれば \\(Av_{k}=\\lambda_{k}v_{k}+v_{k-1}\\). いずれにせよ \\(Av_{k}\\in\\lambda_{k}v_{k}+\\mathcal{V}_{k-1}\\) であるから, 結局 \\[ Au_{k}\\in\\lambda_{k}\\alpha_{k}v_{k}+\\mathcal{V}_{k-1} \\] が成り立つ. したがって, \\(t_{kk}=\\lambda_{k}\\) にならなければならない. \\(A\\in\\mathbb{F}^{n\\times n}\\) が正規行列であるとは, \\(A^{*}A=AA^{*}\\) が成り立つことをいう. シューアの定理によって, 次の重要な結果を証明できる. 系 8.1 \\(A\\in\\mathbb{F}^{n\\times n}\\) を正規行列であることと, ユニタリ行列によって対角化可能であることは同値. 証明. \\(A\\) を正規行列とする. \\(U\\) をユニタリ行列, \\(U^{*}AU=T\\) が上三角行列であるとする. したがって, \\[ A=UTU^{*},\\quad A^{*}=UT^{*}U^{*}. \\] \\(A\\) は正規行列だから \\[ UTU^{*}\\cdot UT^{*}U^{*}=UT^{*}U^{*}\\cdot UTU^{*} \\] すなわち \\[ UTT^{*}U^{*}=UT^{*}TU^{*}. \\] \\(U\\), \\(U^{*}\\) は正則なので, \\(T\\) は正規行列である. 正規な上三角行列は対角行列以外にありえない(問題) ので, \\(A\\) はユニタリ行列で対角化できる. \\(A\\) がユニタリ行列で対角化可能であるとする. \\(UAU^{*}\\), \\(UA^{*}U^{*}\\) はどちらも対角行列なので \\(UAU^{*}\\cdot UA^{*}U^{*}=UA^{*}U^{*}\\cdot UAU^{*}\\) が成り立つ. \\(U\\), \\(U^{*}\\) は正則なので \\(AA^{*}=A^{*}A\\) を得る. 正規かつ上三角である行列は対角行列であることを示せ. エルミート行列はユニタリ行列で対角化できることを証明せよ. 8.2.2 システム分析への応用 無限期間のシステムが \\[\\begin{equation} x_{t+1}=Ax_{t}+Bu_{t},\\quad t=0,1,\\dots,\\tag{8.1} \\end{equation}\\] に従うとする. ただし, \\(x\\) を構成する変数の内, 一部にのみ初期条件が与えられているとする. 変数 \\(x\\) は \\(x^{1}\\in\\mathbb{R}^{n_{1}}\\) および \\(x^{2}\\in\\mathbb{R}^{n_{2}}\\) によって \\[\\begin{equation} x_{t}=\\begin{bmatrix}x_{t}^{1}\\\\ x_{t}^{2} \\end{bmatrix}\\tag{8.2} \\end{equation}\\] と分離されており, 初期条件が \\[ x_{0}^{1}=\\bar{x}_{0}^{1}, \\] と与えられているとしよう. 終端条件として経路の発散スピードが幾何数列のスピードを超えないことを仮定する. すなわち, 任意の \\(-1&lt;\\rho&lt;1\\) について \\[ \\lim_{\\tau\\to\\infty}\\rho^{\\tau}\\|x_{\\tau}^{2}\\|=0 \\] が成り立つとする. \\(A\\) のシューア分解 \\[ A=U\\begin{bmatrix}T_{ss} &amp; T_{su}\\\\ 0 &amp; T_{uu} \\end{bmatrix}U^{-1},\\qquad U=\\begin{bmatrix}U_{1s} &amp; U_{1u}\\\\ U_{2s} &amp; U_{2u} \\end{bmatrix},\\quad U^{-1}=\\begin{bmatrix}U_{1s}^{*} &amp; U_{2s}^{*}\\\\ U_{1u}^{*} &amp; U_{2u}^{*} \\end{bmatrix} \\] を利用して状態方程式を \\[ U^{-1}\\begin{bmatrix}x_{t+1}^{1}\\\\ x_{t+1}^{2} \\end{bmatrix}=\\begin{bmatrix}T_{ss} &amp; T_{su}\\\\ 0 &amp; T_{uu} \\end{bmatrix}U^{-1}\\begin{bmatrix}x_{t}^{1}\\\\ x_{t}^{2} \\end{bmatrix}+U^{-1}\\begin{bmatrix}B_{1}\\\\ B_{2} \\end{bmatrix}u_{t} \\] と変形できる. ただし, \\(T_{ss}\\), \\(T_{uu}\\) はともに上三角行列であり, \\(T_{ss}\\)には\\(A\\)の固有値の内, 絶対値が1以下のものが並んでいる. また, \\(T_{uu}\\)の対角成分には \\(A\\) の固有値の中でも絶対値が \\(1\\) より大きいものが並んでいるものとする. したがって, \\(T_{uu}\\) は正則である. \\(y:=U^{-1}x\\), と変数変換することで \\[\\begin{equation} \\begin{bmatrix}y_{t+1}^{s}\\\\ y_{t+1}^{u} \\end{bmatrix}=\\begin{bmatrix}T_{ss} &amp; T_{su}\\\\ 0 &amp; T_{uu} \\end{bmatrix}\\begin{bmatrix}y_{t}^{s}\\\\ y_{t}^{u} \\end{bmatrix}+\\begin{bmatrix}C_{s}\\\\ C_{u} \\end{bmatrix}u_{t}\\tag{8.3} \\end{equation}\\] を得る. ただし, \\[ y^{s}\\in\\mathbb{R}^{n_{s}},\\ y^{u}\\in\\mathbb{R}^{n_{u}},\\ y=\\begin{bmatrix}y^{s}\\\\ y^{u} \\end{bmatrix},\\ U^{-1}B=:\\begin{bmatrix}C_{s}\\\\ C_{u} \\end{bmatrix} \\] であり, 行列はすべて適合するサイズをもつものとする. 第7章 でやったように, 式 の第2ブロックについて最終期から後ろ向きに (forward-looking に) 解いてみよう. \\(T_{uu}\\) は正則であるから, \\[\\begin{align} y_{t}^{u} &amp; =T_{uu}^{-1}y_{t+1}^{u}-T_{uu}^{-1}C_{u}u_{t}\\nonumber \\\\ &amp; =T_{uu}^{-1}\\left(T_{uu}^{-1}y_{t+2}^{u}-T_{uu}^{-1}C_{u}u_{t+1}\\right)-T_{uu}^{-1}C_{u}u_{t}\\nonumber \\\\ &amp; =T_{uu}^{-2}y_{t+2}^{u}-T_{uu}^{-2}C_{u}u_{t+1}-T_{uu}^{-1}C_{u}u_{t}\\nonumber \\\\ &amp; =\\cdots\\nonumber \\\\ &amp; =T_{uu}^{-T}y_{t+T}^{u}-\\sum_{k=0}^{T-1}T_{uu}^{-k-1}C_{uu}u_{t+k}.\\tag{8.4} \\end{align}\\] \\(T_{uu}^{-1}\\) は安定行列であり, 解の発散スピードに関する仮定から \\[\\begin{equation} y_{t}^{u}=-\\sum_{k=0}^{\\infty}T_{uu}^{-k-1}C_{u}u_{t+k} \\tag{8.5} \\end{equation}\\] が成り立たなければならない. したがって, \\(y_{0}^{u}=\\bar{y}_{0}^{u}=-\\sum_{k=0}^{\\infty}T_{uu}^{-k-1}C_{u}u_{k}\\) を決定できる. \\(t=0\\) に関する変数を決定する方程式 \\[ \\begin{bmatrix}\\bar{x}_{0}^{1}\\\\ x_{0}^{2} \\end{bmatrix}=U\\begin{bmatrix}y_{0}^{s}\\\\ \\bar{y}_{0}^{u} \\end{bmatrix} \\] あるいは同値な方程式 \\[ \\begin{bmatrix}U_{1s} &amp; 0\\\\ U_{2s} &amp; -I \\end{bmatrix}\\begin{bmatrix}y_{0}^{s}\\\\ x_{0}^{2} \\end{bmatrix}=\\begin{bmatrix}I &amp; -U_{1u}\\\\ 0 &amp; -U_{2u} \\end{bmatrix}\\begin{bmatrix}\\bar{x}_{0}^{1}\\\\ \\bar{y}_{0}^{u} \\end{bmatrix} \\] は \\(U_{1s}\\) が正則であるときに解が一意に存在する. これを仮定して解くと, \\[ \\begin{bmatrix}y_{0}^{s}\\\\ x_{0}^{2} \\end{bmatrix}=\\begin{bmatrix}U_{1s}^{-1} &amp; -U_{1s}^{-1}U_{1u}\\\\ U_{2s}U_{1s}^{-1} &amp; U_{2u}-U_{2s}U_{1s}^{-1}U_{1u} \\end{bmatrix}\\begin{bmatrix}x_{0}^{1}\\\\ y_{0}^{u} \\end{bmatrix}. \\] 解の不在, 不決定性に関する議論はこれまでと同様であるので省略する. 再帰方程式を計算しよう. \\[ \\begin{aligned} \\begin{bmatrix}x_{t+1}^{1}\\\\ x_{t+1}^{2} \\end{bmatrix} &amp; =U\\begin{bmatrix}y_{t+1}^{s}\\\\ y_{t+1}^{u} \\end{bmatrix}\\\\ &amp; =U\\begin{bmatrix}T_{ss}y_{t}^{s}+T_{su}y_{t}^{u}+C_{s}u_{t}\\\\ y_{t+1}^{u} \\end{bmatrix}\\\\ &amp; =\\begin{bmatrix}U_{1s} &amp; U_{1u}\\\\ U_{2s} &amp; U_{2u} \\end{bmatrix}\\begin{bmatrix}T_{ss}U_{1s}^{-1}x_{t}^{1}+\\left(T_{su}-T_{ss}U_{1s}^{-1}U_{1u}\\right)y_{t}^{u}+C_{s}u_{t}\\\\ y_{t+1}^{u} \\end{bmatrix} \\end{aligned} \\] 両辺に \\([U_{2s}U_{1s}^{-1}\\ -I]\\)を掛けると \\[ \\begin{aligned} U_{2s}U_{1s}^{-1}x_{t+1}^{1}-x_{t+1}^{2} &amp; =\\begin{bmatrix}0 &amp; U_{2s}U_{1s}^{-1}U_{1u}-U_{2u}\\end{bmatrix}\\begin{bmatrix}T_{ss}U_{1s}^{-1}x_{t}^{1}+\\left(T_{su}-T_{ss}U_{1s}^{-1}U_{1u}\\right)y_{t}^{u}+C_{s}u_{t}\\\\ y_{t+1}^{u} \\end{bmatrix}\\\\ &amp; =\\left(U_{2s}U_{1s}^{-1}U_{1u}-U_{2u}\\right)y_{t+1}^{u}. \\end{aligned} \\] したがって, \\[\\begin{equation} x_{t}^{2}=U_{2s}U_{1s}^{-1}x_{t}^{1}+\\left(U_{2u}-U_{2s}U_{1s}^{-1}U_{1u}\\right)y_{t}^{u},\\qquad t=0,1,\\dots\\tag{8.6} \\end{equation}\\] 次に \\(x^{1}\\) の再帰方程式を求める. まず \\[ \\begin{aligned} y_{t+1}^{u} &amp; =-\\sum_{k=0}^{\\infty}T_{uu}^{-k-1}C_{u}u_{t+1+k}\\\\ &amp; =-T_{uu}^{-1}C_{u}u_{t+1}-T_{uu}^{-2}C_{u}u_{t+2}-T_{uu}^{-3}C_{u}u_{t+3}-\\cdots\\\\ &amp; =C_{u}u_{t}+T_{uu}\\left(-T_{uu}^{-1}C_{u}u_{t}-T_{uu}^{-2}C_{u}u_{t+1}-T_{uu}^{-3}C_{u}u_{t+2}-\\cdots\\right)\\\\ &amp; =C_{u}u_{t}+T_{uu}y_{t}^{u} \\end{aligned} \\] に注意する. \\[\\begin{align} x_{t+1}^{1} &amp; =U_{1s}T_{ss}U_{1s}^{-1}x_{t}^{1}+U_{1s}\\left(T_{su}-T_{ss}U_{1s}^{-1}U_{1u}\\right)y_{t}^{u}+U_{1s}C_{s}u_{t}+U_{1u}y_{t+1}^{u}\\nonumber \\\\ &amp; =U_{1s}T_{ss}U_{1s}^{-1}x_{t}^{1}+\\left[U_{1s}\\left(T_{su}-T_{ss}U_{1s}^{-1}U_{1u}\\right)+U_{1u}T_{uu}\\right]y_{t}^{u}+\\left(U_{1s}C_{s}+U_{1u}C_{u}\\right)u_{t}\\nonumber \\\\ &amp; =U_{1s}T_{ss}U_{1s}^{-1}x_{t}^{1}+\\left[U_{1s}\\left(T_{su}-T_{ss}U_{1s}^{-1}U_{1u}\\right)+U_{1u}T_{uu}\\right]y_{t}^{u}+B_{1}u_{t}.\\tag{8.7} \\end{align}\\] これで再帰的な解が得られたことになる. 8.2.3 \\(u_{t+1}=\\Phi u_{t}\\) \\(u_{t}\\) を特定化しよう. (8.5) で \\[ M:=-\\sum_{k=0}^{\\infty}T_{uu}^{-k-1}C_{u}\\Phi^{k} \\] とおけば, \\(M\\) は離散時間のシルベスタ方程式 \\[ \\begin{aligned} M &amp; =-T_{uu}^{-1}C_{u}+T_{uu}^{-1}M\\Phi \\end{aligned} \\] を満たす. あるいは, 連続時間のシルベスタ方程式 \\[ T_{uu}M-M\\Phi=-C_{u} \\] を満たす. \\[ \\mathrm{vec}(T_{uu}M)-\\mathrm{vec}(M\\Phi)=-\\mathrm{vec}(C_{u}). \\] に 補題 7.1 を使うと \\[ \\left(I^{\\top}\\otimes T_{uu}\\right)\\mathrm{vec}(M)-\\left(\\Phi^{\\top}\\otimes I\\right)\\mathrm{vec}(M)=-\\mathrm{vec}(C_{u}), \\] \\[ \\mathrm{vec}(M)=\\left[\\left(\\Phi^{\\top}\\otimes I\\right)-\\left(I^{\\top}\\otimes T_{uu}\\right)\\right]^{-1}\\mathrm{vec}(C_{u}) \\] を得る. \\(M\\) がこのように決まるものとしたとき, 式 (8.7) より \\[\\begin{equation} x_{t+1}^{1}=U_{1s}T_{ss}U_{1s}^{-1}x_{t}^{1}+\\left[U_{1s}T_{su}M-U_{1s}T_{ss}U_{1s}^{-1}U_{1u}M+U_{1u}T_{uu}M+B_{1}\\right]u_{t}\\tag{8.8} \\end{equation}\\] を得る. 式 (8.6) より, \\[ x_{t}^{2}=U_{2s}U_{1s}^{-1}x_{t}^{1}+\\left(U_{2u}-U_{2s}U_{1s}^{-1}U_{1u}\\right)Mu_{t}. \\] 8.2.4 実Schur分解 上の議論では \\(T\\) が上三角行列であるとしたが, この事実は完全には利用されていない. 再帰式を計算する上では \\(U^{-1}AU\\) がブロック上三角行列 \\[ \\begin{bmatrix}T_{ss} &amp; T_{su}\\\\ 0 &amp; T_{uu} \\end{bmatrix} \\] でありさえすればよい. さらに, \\(T_{ss}\\), \\(T_{uu}\\) が対角成分に\\(A\\) の固有値 \\(\\lambda=a\\pm bj\\) に対応する\\(2\\times2\\) 実行列 \\[ \\begin{bmatrix}a &amp; -b\\\\ b &amp; a \\end{bmatrix} \\] が並ぶようなブロック上三角行列であれば, 収束性に関する議論も上と同様に成り立つ. 実は, 任意の実行列\\(A\\) に対して, 適当な直交行列 \\(U\\) を選んで \\[ U^{\\top}AU=\\left[\\begin{array}{cc|cc|cc|cc} a_{1} &amp; -b_{1} &amp; * &amp; * &amp; * &amp; * &amp; * &amp; *\\\\ b_{1} &amp; a_{1} &amp; * &amp; * &amp; * &amp; * &amp; * &amp; *\\\\ \\hline &amp; &amp; a_{2} &amp; -b &amp; * &amp; * &amp; * &amp; *\\\\ &amp; &amp; b_{2} &amp; a_{2} &amp; * &amp; * &amp; * &amp; *\\\\ \\hline &amp; &amp; &amp; &amp; \\ddots &amp; * &amp; * &amp; *\\\\ &amp; &amp; &amp; &amp; &amp; \\ddots &amp; * &amp; *\\\\ \\hline &amp; &amp; &amp; &amp; &amp; &amp; a_{r} &amp; -b_{r}\\\\ &amp; &amp; &amp; &amp; &amp; &amp; b_{r} &amp; a_{r} \\end{array}\\right] \\] となるようにできる. この分解を 実Schur分解という. 上で行なった議論は実Schur分解の範囲で考えれば十分である. 8.3 QZ 分解とその応用 行列のペア \\((E,A)\\) が与えられたとき, 正則行列 \\(V\\), \\(W\\) により \\[ W^{-1}AV=\\begin{bmatrix}J\\\\ &amp; I \\end{bmatrix},\\qquad W^{-1}EV=\\begin{bmatrix}I\\\\ &amp; N \\end{bmatrix} \\] が成り立つようにできるのであった. \\(J\\), \\(N\\) はいずれもジョルダン標準形行列になっているので, \\(W^{-1}AV\\), \\(W^{-1}EV\\) はいずれも上三角行列になっていることに注意しよう. 行列ペアに対してSchur 分解を拡張することで, 一般化Schur分解あるいはQZ分解と呼ばれる標準化の方法が得られる. ユニタリ変換による標準化は数値的に望ましい性質を持つので, 実用的にはQZ分解を利用したシステム解析法がもっとも使いやすい. 8.3.1 QZ分解 Golub and Van Loan (2013) にしたがって証明する.30 定理 8.4 任意の正方行列のペア \\(E,A\\in\\mathbb{F}^{n\\times n}\\) に対して, 適当なユニタリ行列 \\(Q\\), \\(Z\\) を選べば \\[ Q^{*}AZ=T,\\qquad Q^{*}EZ=S \\] が上三角行列になるようにできる. ペンシル \\((E,A)\\) がレギュラーであれば, \\(T\\) と\\(S\\) の対角成分の比が \\((E,A)\\) の有限固有値の集合と一致する. すなわち \\[ \\mathrm{sp}(E,A)=\\left\\{ \\frac{t_{ii}}{s_{ii}}\\ \\mid\\ s_{ii}\\neq0,\\ i=1,\\dots,n\\right\\} . \\] 証明. \\(\\{E_{k}\\}\\) を \\(E\\) に収束する正則行列の列とする. \\(AE_{k}^{-1}\\) をシューア分解するユニタリ行列を\\(Q_{k}\\) とすれば, \\[\\begin{align} Q_{k}^{*}\\left(AE_{k}^{-1}\\right)Q_{k}=R_{k} \\tag{8.9} \\end{align}\\] は上三角行列である. QR分解を \\(E_{k}^{-1}Q_{k}\\) にほどこして, \\[\\begin{equation} E_{k}^{-1}Q_{k}=Z_{k}S_{k}^{-1}\\tag{8.10} \\end{equation}\\] とする. ただし, \\(S_{k}^{-1}\\) は上三角行列で, \\(Z_{k}\\) はユニタリ行列. (8.9) に (8.10) を代入すると, \\[ Q_{k}^{*}AZ_{k}=R_{k}S_{k} \\] を得る. 正則な上三角行列の逆行列もまた上三角行列なので, \\(S_{k}\\) は上三角行列である. したがって, \\(R_{k}S_{k}\\) は上三角行列である. \\(T_{k}:=R_{k}S_{k}\\) とする. 式 (8.10) に右から \\(S_{k}\\), 左から\\(E_{k}\\)を掛けて \\[ Q_{k}S_{k}=E_{k}Z_{k}, \\] さらに左から \\(Q_{k}^{-k}=Q_{k}^{*}\\) を掛けて \\[ S_{k}=Q_{k}^{*}E_{k}Z_{k} \\] を得る. \\(|\\det Q_{k}|=|\\det Z_{k}|\\) が成り立つので, \\(Q_{k}\\), \\(Z_{k}\\) は \\(\\mathbb{F}^{n\\times n}\\) の有界な点列である. ボルツァーノ・ワイエルシュトラスの定理により, \\(\\{(Q_{k},Z_{k})\\}\\) は収束する部分列 \\(\\{(Q_{k_{i}},Z_{k_{i}})\\}_{i}\\) を持つ. 極限を \\((Q,Z)\\) とすれば \\[ \\begin{aligned} \\lim_{i\\to\\infty}Q_{k_{i}}^{*}A_{k_{i}}Z_{k_{i}} &amp; =Q^{*}AZ=T=\\lim_{i\\to\\infty}T_{k_{i}}\\\\ \\lim_{i\\to\\infty}Q_{k_{i}}^{*}E_{k_{i}}Z_{k_{i}} &amp; =Q^{*}EZ=S=\\lim_{i\\to\\infty}S_{k_{i}} \\end{aligned} \\] が成り立つ. \\(T\\), \\(S\\) が上三角行列であることは明らか. \\(Q\\), \\(Z\\) がユニタリであることは, \\[ I=Q_{k_{i}}^{*}Q_{k_{i}}\\to Q^{*}Q,\\quad I=Z_{k_{i}}^{*}Z_{k_{i}}\\to Z^{*}Z \\] から分かる. 有限固有値に関する性質は, \\[ \\begin{aligned} |\\det(\\lambda E-A)| &amp; =|\\det(\\lambda QSZ^{*}-QTZ^{*})|\\\\ &amp; =|\\det Q\\cdot\\det(\\lambda S-T)\\cdot\\det Z^{*}|\\\\ &amp; =|\\det(\\lambda S-T)|\\\\ &amp; =\\prod|\\lambda s_{ii}-t_{ii}| \\end{aligned} \\] から従う. 8.3.2 システム分析への応用 \\((E,A)\\) をレギュラーとする. ここではシステム \\[\\begin{equation} Ex_{t+1}=Ax_{t}+Bu_{t},\\quad x\\in\\mathbb{R}^{n},\\quad u\\in\\mathbb{R}^{m}\\tag{8.11} \\end{equation}\\] に対する解公式を導出しよう. QZ分解を用いた分析といってもこれまでの分析と大きくは変わらない. もうかなり慣れてきたと思うので, 簡単に述べるにとどめる. これまでと同様, \\(x_{0}=(x_{0}^{1},x_{0}^{2})\\in\\mathbb{R}^{n_{1}}\\times\\mathbb{R}^{n_{2}}\\) には初期条件 \\(x_{0}^{1}=\\bar{x}_{0}^{1}\\) が与えられている. 幾何数列の発散スピードを超えない \\((x_{t})_{t\\ge0}\\) を実行可能な解とする. \\((E,A)\\) の QZ分解 \\[ \\begin{aligned} Q^{*}EZ &amp; =S=\\begin{bmatrix}S_{ss} &amp; S_{su}\\\\ 0 &amp; S_{uu} \\end{bmatrix},\\quad S_{ss}\\in\\mathbb{C}^{n_{s}\\times n_{s}},\\ S_{su}\\in\\mathbb{C}^{n_{s}\\times n_{u}},\\ S_{uu}\\in\\mathbb{C}^{n_{u}\\times n_{u}}\\\\ Q^{*}AZ &amp; =T=\\begin{bmatrix}T_{ss} &amp; T_{su}\\\\ 0 &amp; T_{uu} \\end{bmatrix}\\quad T_{ss}\\in\\mathbb{C}^{n_{s}\\times n_{s}},\\ T_{su}\\in\\mathbb{C}^{n_{s}\\times n_{u}},\\ T_{uu}\\in\\mathbb{C}^{n_{u}\\times n_{u}} \\end{aligned} \\] は \\(\\{\\lambda_{i}=(T_{ss})_{ii}/(S_{ss})_{ii}\\ \\mid\\ i=1,\\dots,n_{s}\\}\\) が \\(|\\lambda_{i}|\\le1\\) を満たす有限固有値とする. \\(S_{ss}\\) は正則である. \\((S_{uu},T_{uu})\\) について, \\((S_{uu})_{ii}\\neq0\\) のとき \\((T_{uu})_{ii}/(S_{uu})_{ii}&gt;1\\) であり \\((E,A)\\) の有限不安定固有値に対応し, \\((S_{uu})_{ii}=0\\) は無限大固有値に対応している. \\((E,A)\\) がレギュラーなので \\((S_{uu})_{ii}=0\\) のときは \\((T_{uu})_{ii}\\neq0\\) である. したがって, \\(T_{uu}\\) は必ず正則になる. いつものように \\(\\left[\\begin{smallmatrix}y_{t}^{s}\\\\ y_{t}^{u} \\end{smallmatrix}\\right]=Z^{*}\\left[\\begin{smallmatrix}x_{t}^{1}\\\\ x_{t}^{2} \\end{smallmatrix}\\right]\\in\\mathbb{R}^{n_{s}}\\times\\mathbb{R}^{n_{u}}\\)と変数変換すると, \\[\\begin{equation} \\begin{bmatrix}S_{ss} &amp; S_{su}\\\\ 0 &amp; S_{uu} \\end{bmatrix}\\begin{bmatrix}y_{t+1}^{s}\\\\ y_{t+1}^{u} \\end{bmatrix}=\\begin{bmatrix}T_{ss} &amp; T_{su}\\\\ 0 &amp; T_{uu} \\end{bmatrix}\\begin{bmatrix}y_{t}^{s}\\\\ y_{t}^{u} \\end{bmatrix}+Q^{*}Bu_{t}.\\tag{8.12} \\end{equation}\\] ただし, \\(Z=\\left[\\begin{smallmatrix}Z_{1s} &amp; Z_{1u}\\\\ Z_{2s} &amp; Z_{2u} \\end{smallmatrix}\\right]\\), \\(Q^{*}B=\\left[\\begin{smallmatrix}C_{s}\\\\ C_{u} \\end{smallmatrix}\\right]\\) としておこう. \\(T_{uu}\\), \\(S_{uu}\\) は上三角行列なので, \\(T_{uu}^{-1}\\) および \\(T_{uu}^{-1}S_{uu}\\) も上三角行列になる. \\(S_{uu}\\) , \\(T_{uu}\\) の定義から, \\(T_{uu}^{-1}S_{uu}\\) は絶対値が1以下の固有値のみをもつ行列である. バックワードサブシステム \\[\\begin{equation} y_{t}^{u}=(T_{uu}^{-1}S_{uu})y_{t+1}^{u}-T_{uu}^{-1}C_{u}u_{t}\\tag{8.13} \\end{equation}\\] が \\(y_{t}^{u}\\) を決定する. 発散スピードに対する仮定から \\[\\begin{equation} y_{t}^{u}=-\\sum_{k=0}^{\\infty}\\left(T_{uu}^{-1}S_{uu}\\right)^{k}T_{uu}^{-1}C_{u}u_{t+k}\\tag{8.14} \\end{equation}\\] となる. 未知数の決定は線形方程式 \\[\\begin{equation} \\begin{bmatrix}Z_{1s} &amp; 0\\\\ Z_{2s} &amp; -I \\end{bmatrix}\\begin{bmatrix}y_{0}^{s}\\\\ x_{0}^{2} \\end{bmatrix}=\\begin{bmatrix}I &amp; -Z_{1u}\\\\ 0 &amp; -Z_{2u} \\end{bmatrix}\\begin{bmatrix}\\bar{x}_{0}^{1}\\\\ \\bar{y}_{0}^{u} \\end{bmatrix}\\tag{8.15} \\end{equation}\\] によることもこれまでと同様である. \\(Z_{1s}\\) が正則な正方行列であるときに \\(t=0\\) 時点の変数を一意に定めることができる. 再帰方程式を導出するにあたって, \\(u_{t+1}=\\Phi u_{t}\\) を仮定する. 式(8.14) より\\(y_{t}^{u}=Mu_{t}\\). ただし, \\(M\\) はシルベスタ方程式 \\[ \\begin{aligned} M &amp; -T_{uu}^{-1}S_{uu}M\\Phi=-T_{uu}^{-1}C_{u} \\end{aligned} \\] の解である. (8.12) の第1行目の式から \\[\\begin{align} y_{t+1}^{s} &amp; =S_{ss}^{-1}T_{ss}y_{t}^{s}-S_{ss}^{-1}S_{su}y_{t+1}^{u}+S_{ss}^{-1}T_{su}y_{t}^{u}+S_{ss}^{-1}C_{s}u_{t}(\\#eq:uni-ys_gen)\\\\ &amp; =S_{ss}^{-1}T_{ss}y_{t}^{s}-\\left(S_{ss}^{-1}S_{su}M\\Phi-S_{ss}^{-1}T_{su}M-S_{ss}^{-1}C_{s}\\right)u_{t}.\\nonumber \\end{align}\\] \\(y_{t}^{s}=Z_{1s}^{-1}\\left(x_{t}^{1}-Z_{1u}y_{t}^{u}\\right)\\) を使ってさらに計算を進めると, \\[\\begin{align} x_{t+1}^{1} &amp; =Z_{1s}S_{ss}^{-1}T_{ss}Z_{1s}^{-1}x_{t}^{1}+Lu_{t},\\tag{8.16} \\end{align}\\] を得る. ただし, \\[ L=-Z_{1s}S_{ss}^{-1}T_{ss}Z_{1s}^{-1}Z_{1u}M+Z_{1s}S_{ss}^{-1}\\left(T_{su}M-S_{su}M\\Phi+C_{s}\\right)+Z_{1u}M\\Phi. \\] 続いて, \\(x_{t}^{2}\\) に関する方程式を導出しよう. 変数変換の定義より \\[ Z_{2s}y_{t}^{s}-x_{t}^{2}=-Z_{2u}y_{t}^{u},\\qquad y_{t}^{s}=Z_{1s}^{-1}\\left(x_{t}^{1}-Z_{1u}y_{t}^{u}\\right) \\] が成り立つので, \\[ \\begin{aligned} x_{t}^{2} &amp; =Z_{2s}y_{t}^{s}+Z_{2u}y_{t}^{u}\\\\ &amp; =Z_{2s}Z_{1s}^{-1}\\left(x_{t}^{1}-Z_{1u}y_{t}^{u}\\right)+Z_{2u}y_{t}^{u}\\\\ &amp; =Z_{2s}Z_{1s}^{-1}x_{t}^{1}+(Z_{2u}-Z_{2s}Z_{1s}^{-1}Z_{1u})Mu_{t}. \\end{aligned} \\] したがって, 次の公式を得た. 定理 8.5 (Klein (2000)) システム をQZ分解を用いて解くと, 解は \\[ \\begin{aligned} x_{t+1}^{1} &amp; =Z_{1s}S_{ss}^{-1}T_{ss}Z_{1s}^{-1}x_{t}^{1}+Lu_{t},\\\\ x_{t}^{2} &amp; =Z_{2s}Z_{1s}^{-1}x_{t}^{1}+(Z_{2u}-Z_{2s}Z_{1s}^{-1}Z_{1u})Mu_{t} \\end{aligned} \\] によって表される. ただし,\\(x_{0}^{1}\\) は所与, \\(S\\), \\(T\\), \\(Z\\), \\(M\\), \\(C\\) は上で与えたものであり, \\[ \\begin{aligned} L &amp; =-Z_{1s}S_{ss}^{-1}T_{ss}Z_{1s}^{-1}Z_{1u}M+Z_{1s}S_{ss}^{-1}\\left(T_{su}M-S_{su}M\\Phi+C_{s}\\right)+Z_{1u}M\\Phi,\\\\ M &amp; =\\mathrm{vec}^{-1}\\left(\\left(\\Phi^{\\top}\\otimes S_{uu}-I\\otimes T_{uu}\\right)^{-1}\\mathrm{vec}(C_{u})\\right). \\end{aligned} \\] 8.4 QZ解法の一般公式 一般の \\(u\\) に対する公式は次のように表現できる. \\(t=0,1,\\dots\\) に対して, \\[ \\begin{aligned} x_{t+1}^{1} &amp; =\\Omega_{x}x_{t}^{1}+\\Omega_{u}u_{t}+\\Omega_{y}y_{t+1}^{u}\\\\ x_{t}^{2} &amp; =\\Psi_{x}x_{t}^{1}+\\Psi_{y}y_{t}^{u} \\end{aligned} \\] ただし, \\(x_{0}^{1}\\) は所与であり, \\(y_{t}^{u}\\) は式 (8.14) から定まる. 行列, \\(\\Omega_{x}\\), \\(\\Omega_{u}\\), \\(\\Omega_{y}\\), \\(\\Psi_{x}\\), \\(\\Psi_{y}\\) を導出しよう. \\[ \\begin{aligned} x_{t+1}^{1}-Z_{1u}y_{t+1}^{u} &amp; =Z_{1s}y_{t+1}^{s}\\\\ &amp; =Z_{1s}\\left[S_{ss}^{-1}T_{ss}y_{t}^{s}-S_{ss}^{-1}S_{su}y_{t+1}^{u}+S_{ss}^{-1}T_{su}y_{t}^{u}+S_{ss}^{-1}C_{s}u_{t}\\right]\\\\ &amp; =Z_{1s}S_{ss}^{-1}T_{ss}Z_{1s}^{-1}\\left(x_{t}^{1}-Z_{1u}y_{t}^{u}\\right)\\\\ &amp; \\qquad-Z_{1s}S_{ss}^{-1}S_{su}y_{t+1}^{u}\\\\ &amp; \\qquad+Z_{1s}S_{ss}^{-1}T_{su}y_{t}^{u}\\\\ &amp; \\qquad+Z_{1s}S_{ss}^{-1}C_{s}u_{t} \\end{aligned} \\] から \\[ \\begin{aligned} x_{t+1}^{1} &amp; =Z_{1s}S_{ss}^{-1}T_{ss}Z_{1s}^{-1}x_{t}^{1}\\\\ &amp; \\qquad+(Z_{1u}-Z_{1s}S_{ss}^{-1}S_{su})y_{t+1}^{u}\\\\ &amp; \\qquad+Z_{1s}S_{ss}^{-1}(T_{su}-T_{ss}Z_{1s}^{-1}Z_{1u})y_{t}^{u}\\\\ &amp; \\qquad+Z_{1s}S_{ss}^{-1}C_{s}u_{t}. \\end{aligned} \\] 式(8.13) を用いて整理すると, \\[ \\begin{aligned} x_{t+1}^{1} &amp; =Z_{1s}S_{ss}^{-1}T_{ss}Z_{1s}^{-1}x_{t}^{1}\\\\ &amp; \\qquad+(Z_{1u}-Z_{1s}S_{ss}^{-1}S_{su})y_{t+1}^{u}\\\\ &amp; \\qquad+Z_{1s}S_{ss}^{-1}(T_{su}-T_{ss}Z_{1s}^{-1}Z_{1u})T_{uu}^{-1}(S_{uu}y_{t+1}^{u}-C_{u}u_{t})\\\\ &amp; \\qquad+Z_{1s}S_{ss}^{-1}C_{s}u_{t}\\\\ &amp; =Z_{1s}S_{ss}^{-1}T_{ss}Z_{1s}^{-1}x_{t}^{1}\\\\ &amp; \\qquad+Z_{1s}S_{ss}^{-1}\\left[C_{s}-(T_{su}-T_{ss}Z_{1s}^{-1}Z_{1u})T_{uu}^{-1}C_{u}\\right]u_{t}\\\\ &amp; \\qquad+\\left[Z_{1u}-Z_{1s}S_{ss}^{-1}S_{su}+Z_{1s}S_{ss}^{-1}(T_{su}-T_{ss}Z_{1s}^{-1}Z_{1u})T_{uu}^{-1}S_{uu}\\right]y_{t+1}^{u} \\end{aligned} \\] を得る. したがって, \\[ \\begin{aligned} \\Omega_{x} &amp; =Z_{1s}S_{ss}^{-1}T_{ss}Z_{1s}^{-1}\\\\ \\Omega_{u} &amp; =Z_{1s}S_{ss}^{-1}\\left[(T_{ss}Z_{1s}^{-1}Z_{1u}-T_{su})T_{uu}^{-1}C_{u}+C_{s}\\right]\\\\ \\Omega_{y} &amp; =Z_{1u}-Z_{1s}S_{ss}^{-1}S_{su}+Z_{1s}S_{ss}^{-1}(T_{su}-T_{ss}Z_{1s}^{-1}Z_{1u})T_{uu}^{-1}S_{uu}. \\end{aligned} \\] 非先決変数については, \\[ \\begin{aligned} x_{t}^{2} &amp; =Z_{2s}y_{t}^{s}+Z_{2u}y_{t}^{u}\\\\ &amp; =Z_{2s}Z_{1s}^{-1}(x_{t}^{1}-Z_{1u}y_{t}^{u})+Z_{2u}y_{t}^{u}\\\\ &amp; =Z_{2s}Z_{1s}^{-1}x_{t}^{1}+(Z_{2u}-Z_{2s}Z_{1s}^{-1}Z_{1u})y_{t}^{u} \\end{aligned} \\] より, \\[ \\begin{aligned} \\Psi_{x} &amp; =Z_{2s}Z_{1s}^{-1}\\\\ \\Psi_{y} &amp; =Z_{2u}-Z_{2s}Z_{1s}^{-1}Z_{1u} \\end{aligned} \\] となる. 参考文献 "],
["section-9.html", "第9章 線形確率システム 9.1 Klein (2000) の方法 9.2 確率過程について", " 第9章 線形確率システム 9.1 Klein (2000) の方法 本稿ではこれまで扱った線形システムに確率的な要因を導入しよう. 本稿は確率過程に関する初級の知識を前提にしているが, Klein (2000) の手法を理解するのに確率論の高度な知識は必要ではない. 読み進めながら定義などを確認していけばよいだろう. 次節に簡単な補論を設けて, 未定義語を補うようにしている. \\((\\Omega,P,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t\\ge0})\\) をフィルター付き確率空間, 入力項 \\((u_{t})\\) を\\(\\{\\mathcal{F}_{t}\\}_{t\\ge0}\\) に適合した確率過程とする. 適合性の定義は省略しているが, 実用上は \\(\\mathbb{E}_{t}u_{t}=u_{t}\\), \\(t=0,1,\\dots\\) が成り立つことを認識していればよい. システム変数に関する (期待の) 時間発展が方程式 (9.1) で表されているものとする. \\[\\begin{equation} E\\mathbb{E}_{t}x_{t+1}=Ax_{t}+Bu_{t},\\qquad t=0,1,\\dots,\\tag{9.1} \\end{equation}\\] ただし \\(\\mathbb{E}_{t}x_{t+1}=\\mathbb{E}\\left[x_{t+1}\\mid\\mathcal{F}_{t}\\right]\\) は \\(\\mathcal{F}_{t}\\) に関する\\(x_{t+1}\\)の条件付き期待値である. \\(\\mathbb{E}_{t}x_{t}=x_{t}\\), \\(\\mathbb{E}_{t}u_{t}=u_{t}\\) が成り立つことに注意しておこう. したがって, (9.1) は \\[ E\\mathbb{E}_{t}x_{t+1}=A\\mathbb{E}_{t}x_{t}+B\\mathbb{E}_{t}u_{t} \\] と書けば, 決定論との関係がより明確になるだろう. ただし, 条件付き期待値はそれ自身確率変数なので対象の複雑性が大幅に増していることは注意しておこう. それでもなお決定論の解法が必要な仕事をしてくれる. 以下では, 決定論的な方法をなぞりながら仮定すべき条件を探していこう. まずは \\((E,A)\\) のQZ分解を行う. したがって, 次の条件を仮定する. \\((E,A)\\) はレギュラーである. \\(Q\\), \\(Z\\) をユニタリ行列, \\(Q^{*}EZ=S\\) および\\(Q^{*}AZ=T\\) を上三角行列であるとする. 変数変換 \\(x=Zy\\) の部分行列分解を \\[ \\begin{bmatrix}x_{t}^{1}\\\\ x_{t}^{2} \\end{bmatrix}=\\begin{bmatrix}Z_{1s} &amp; Z_{1u}\\\\ Z_{2s} &amp; Z_{2u} \\end{bmatrix}\\begin{bmatrix}y_{t}^{s}\\\\ y_{t}^{u} \\end{bmatrix} \\] とする. \\(x^{1}\\in\\mathbb{R}^{n_{1}}\\), \\(x^{2}\\in\\mathbb{R}^{n_{2}}\\), \\(y^{s}\\in\\mathbb{R}^{n_{s}}\\), \\(y^{u}\\in\\mathbb{R}^{n_{u}}\\) とする. 次の条件を仮定しよう. \\(n_{u}\\neq 0\\). 方程式の適当な並び替えによって \\(x^{1}\\) は先決変数, \\(x^{2}\\) は非先決変数であるように選んでいる. ただし, Klein (2000) の意味で先決変数とは \\[ x_{t+1}^{1}=\\mathbb{E}_{t}x_{t+1}^{1}+\\xi_{t+1} \\] なる, 外生的な\\((\\mathcal{F}_{t})\\)-適合確率過程 \\((\\xi_{t})\\) が存在する変数のこと. 定義より \\(\\mathbb{E}_{t}\\xi_{t+1}=0\\) であることに注意せよ. 先決変数は \\(t\\) 期に獲得できる情報によって \\((t+1)\\) 期の値を予測不可能な誤差 (\\(\\xi_{t+1}\\)) を除いて決定できる変数である. \\(0\\) 期時点の先決変数の期待値は \\((-1)\\) 期には既に決定されているはずなので, \\(x_{0}^{1}\\) は初期値として与えられている値に外生的な撹乱を加えたものと考えてよい. これは決定論の場合に仮定したことと同等である.31 繰り返し現れている次の仮定の役割は今や明らかであろう. \\(n_{1}=n_{s}\\), \\(\\det Z_{1s}\\neq0\\). さて, 決定論の場合と同様に QZ分解の一般化固有値の並び替えによって, ブロック行列表現 \\[\\begin{equation} \\begin{bmatrix}S_{ss} &amp; S_{su}\\\\ 0 &amp; S_{uu} \\end{bmatrix}\\mathbb{E}_{t}\\begin{bmatrix}y_{t+1}^{s}\\\\ y_{t+1}^{u} \\end{bmatrix}=\\begin{bmatrix}T_{ss} &amp; T_{su}\\\\ 0 &amp; T_{uu} \\end{bmatrix}\\begin{bmatrix}y_{t}^{s}\\\\ y_{t}^{u} \\end{bmatrix}+\\begin{bmatrix}C_{s}\\\\ C_{u} \\end{bmatrix}u_{t}\\tag{9.2} \\end{equation}\\] が次のような性質をもつとする. すなわち, \\((S_{ss},T_{ss})\\) は安定 (絶対値が1 のものを含む) な一般化固有値に対応するブロックであり, \\((S_{uu},T_{uu})\\) は不安定 (無限大固有値を含む) 一般化固有値に対応するブロックである. レギュラー性の仮定により \\(T_{uu}\\) は正則行列であるから, バックワードサブシステム \\[\\begin{equation} y_{t}^{u}=T_{uu}^{-1}S_{uu}\\mathbb{E}_{t}y_{t+1}^{u}-T_{uu}^{-1}C_{u}u_{t}\\tag{9.3} \\end{equation}\\] が定義できる. これをforward-looking に解いて \\(y_{t}^{u}\\) を決定する. \\[\\begin{align} y_{t}^{u} &amp; =T_{uu}^{-1}S_{uu}\\mathbb{E}_{t}y_{t+1}^{u}-T_{uu}^{-1}C_{u}u_{t}\\nonumber \\\\ &amp; =T_{uu}^{-1}S_{uu}\\mathbb{E}_{t}\\left[T_{uu}^{-1}S_{uu}\\mathbb{E}_{t+1}y_{t+2}^{u}-T_{uu}^{-1}C_{u}u_{t+1}\\right]-T_{uu}^{-1}C_{u}u_{t}\\nonumber \\\\ &amp; =\\left(T_{uu}^{-1}S_{uu}\\right)^{2}\\mathbb{E}_{t}y_{t+2}^{u}-\\left(T_{uu}^{-1}S_{uu}\\right)T_{uu}^{-1}C_{u}\\mathbb{E}_{t}u_{t+1}-T_{uu}^{-1}C_{u}u_{t}\\nonumber \\\\ &amp; =\\cdots\\nonumber \\\\ &amp; =\\left(T_{uu}^{-1}S_{uu}\\right)^{\\tau}\\mathbb{E}_{t}y_{t+\\tau}^{u}-\\sum_{k=0}^{\\tau-1}\\left(T_{uu}^{-1}S_{uu}\\right)^{k}T_{uu}^{-1}C_{u}\\mathbb{E}_{t}u_{t+k}.\\tag{9.4} \\end{align}\\] \\(T_{uu}^{-1}S_{uu}\\) は安定行列であることに注意しよう. 式 (9.4) の第2項が収束性するために次を仮定しよう. 一番簡単な十分条件は \\(u\\) が有界なサポートを持つことである. 確率過程 \\((u_{t})\\) に対する期待は安定である. すなわち, \\[ \\sup_{t}\\|\\mathbb{E}u_{t}\\|&lt;\\infty. \\] さらに, (9.4) の第1項が収束することを保証するために, 解空間を制限する. このためには, \\[ \\lim_{\\tau\\to\\infty}\\left(T_{uu}^{-1}S_{uu}\\right)^{\\tau}=0 \\] の収束スピードが指数的であることに注意して, 指数的に発散する解を排除してやればよい. 以上より, \\[\\begin{equation} y_{t}^{u}=-\\sum_{k=0}^{\\infty}\\left(T_{uu}^{-1}S_{uu}\\right)^{k} T_{uu}^{-1}C_{u}\\mathbb{E}_{t}u_{t+k}\\tag{9.5} \\end{equation}\\] を得る. 非先決変数を決定するための線形方程式 \\[ \\begin{bmatrix}Z_{1s} &amp; 0\\\\ Z_{2s} &amp; -I \\end{bmatrix}\\begin{bmatrix}y_{t}^{s}\\\\ x_{t}^{2} \\end{bmatrix}=\\begin{bmatrix}I &amp; -Z_{1u}\\\\ 0 &amp; -Z_{2u} \\end{bmatrix}\\begin{bmatrix}x_{t}^{1}\\\\ y_{t}^{u} \\end{bmatrix} \\] から, \\[ y_{t}^{s}=Z_{1s}^{-1}\\left(x_{t}^{1}-Z_{1u}y_{t}^{u}\\right). \\] さらに, (9.2) の上段の式から, \\[ S_{ss}\\mathbb{E}_{t}y_{t+1}^{s}+S_{su}\\mathbb{E}_{t}y_{t+1}^{u}=T_{ss}y_{t}^{s}+T_{su}y_{t}^{u}+C_{s}u_{t} \\] \\[ \\begin{aligned} S_{ss}\\mathbb{E}_{t}Z_{1s}^{-1}\\left(x_{t+1}^{1}-Z_{1u}y_{t+1}^{u}\\right)+S_{su}\\mathbb{E}_{t}y_{t+1}^{u} &amp; =T_{ss}Z_{1s}^{-1}\\left(x_{t}^{1}-Z_{1u}y_{t}^{u}\\right)+T_{su}y_{t}^{u}+C_{s}u_{t}\\\\ S_{ss}Z_{1s}^{-1}\\mathbb{E}_{t}x_{t+1}^{1}+\\left(S_{su}-S_{ss}Z_{1s}^{-1}Z_{1u}\\right)\\mathbb{E}_{t}y_{t+1}^{u} &amp; =T_{ss}Z_{1s}^{-1}x_{t}^{1}+\\left(T_{su}-T_{ss}Z_{1s}^{-1}Z_{1u}\\right)y_{t}^{u}+C_{s}u_{t} \\end{aligned} \\] \\[ \\begin{aligned} S_{ss}Z_{1s}^{-1}\\mathbb{E}_{t}x_{t+1}^{1} &amp; =T_{ss}Z_{1s}^{-1}x_{t}^{1}+\\left(S_{ss}Z_{1s}^{-1}Z_{1u}-S_{su}\\right)\\mathbb{E}_{t}y_{t+1}^{u}\\\\ &amp; \\qquad+(T_{su}-T_{ss}Z_{1s}^{-1}Z_{1u})y_{t}^{u}+C_{s}u_{t}\\\\ \\mathbb{E}_{t}x_{t+1}^{1} &amp; =Z_{1s}S_{ss}^{-1}T_{ss}Z_{1s}^{-1}x_{t}^{1}+\\left(Z_{1u}-Z_{1s}S_{ss}^{-1}S_{su}\\right)\\mathbb{E}_{t}y_{t+1}^{u}\\\\ &amp; \\qquad+Z_{1s}S_{ss}^{-1}(T_{su}-T_{ss}Z_{1s}^{-1}Z_{1u})y_{t}^{u}+Z_{1s}S_{ss}^{-1}C_{s}u_{t}. \\end{aligned} \\] 式 (9.3) より, \\[ Z_{1s}S_{ss}^{-1}(T_{su}-T_{ss}Z_{1s}^{-1}Z_{1u})y_{t}^{u}=Z_{1s}S_{ss}^{-1}(T_{su}-T_{ss}Z_{1s}^{-1}Z_{1u})\\left(T_{uu}^{-1}S_{uu}\\mathbb{E}_{t}y_{t+1}^{u}-T_{uu}^{-1}C_{u}u_{t}\\right) \\] だから, \\[ \\mathbb{E}_{t}x_{t+1}^{1}=\\Omega_{x}x_{t}^{1}+\\Omega_{u}u_{t}+\\Omega_{y}\\mathbb{E}_{t}y_{t+1}^{u} \\] \\[ \\begin{aligned} \\Omega_{x} &amp; =Z_{1s}S_{ss}^{-1}T_{ss}Z_{1s}^{-1}\\\\ \\Omega_{u} &amp; =Z_{1s}S_{ss}^{-1}\\left[(T_{ss}Z_{1s}^{-1}Z_{1u}-T_{su})T_{uu}^{-1}C_{u}+C_{s}\\right]\\\\ \\Omega_{y} &amp; =Z_{1u}-Z_{1s}S_{ss}^{-1}S_{su}+Z_{1s}S_{ss}^{-1}(T_{su}-T_{ss}Z_{1s}^{-1}Z_{1u})T_{uu}^{-1}S_{uu}. \\end{aligned} \\] ここで, 先決変数の定義を使うと, \\[\\begin{equation} x_{t+1}^{1}=\\Omega_{x}x_{t}^{1}+\\Omega_{u}u_{t}+\\Omega_{y}\\mathbb{E}_{t}y_{t+1}^{u}+\\xi_{t+1}\\tag{9.6} \\end{equation}\\] を得る. 非先決変数については, \\[ \\begin{aligned} x_{t}^{2} &amp; =Z_{2s}y_{t}^{s}+Z_{2u}y_{t}^{u}\\\\ &amp; =Z_{2s}Z_{1s}^{-1}(x_{t}^{1}-Z_{1u}y_{t}^{u})+Z_{2u}y_{t}^{u}\\\\ &amp; =Z_{2s}Z_{1s}^{-1}x_{t}^{1}+(Z_{2u}-Z_{2s}Z_{1s}^{-1}Z_{1u})y_{t}^{u} \\end{aligned} \\] より, \\[\\begin{equation} x_{t}^{2}=\\Psi_{x}x_{t}^{1}+\\Psi_{y}y_{t}^{u},\\tag{9.7} \\end{equation}\\] \\[ \\begin{aligned} \\Psi_{x} &amp; =Z_{2s}Z_{1s}^{-1}\\\\ \\Psi_{y} &amp; =Z_{2u}-Z_{2s}Z_{1s}^{-1}Z_{1u} \\end{aligned} \\] とできる. ARショック ここで, \\(u\\) がAR過程 \\[ u_{t+1}=\\Phi u_{t}+\\varepsilon_{t+1} \\] であると仮定しよう. ただし, \\((\\varepsilon_{t})\\) は独立同分布をもつ確率過程であり, \\(\\mathbb{E}_{t}\\varepsilon_{t+1}=0\\), \\(\\mathbb{E}_{t}\\varepsilon_{t+1}\\varepsilon_{t+1}^{\\top}=\\Sigma\\). 行列 \\(\\Phi\\) の固有値はすべて絶対値が1より小さいとする. 式 (9.5) と \\(\\mathbb{E}_{t}u_{t+1}=\\Phi u_{t}\\) より, \\[ y_{t}^{u}=-\\sum_{k=0}^{\\tau-1}\\left(T_{uu}^{-1}S_{uu}\\right)^{k}T_{uu}^{-1}C_{u}\\Phi^{k}u_{t}. \\] \\(M\\) をシルベスタ方程式 \\[ M-T_{uu}^{-1}S_{uu}M\\Phi=-T_{uu}^{-1}C_{u} \\] の解とすれば \\[ y_{t}^{u}=Mu_{t} \\] と書ける. (9.6) と(9.7) から \\[ \\begin{aligned} x_{t+1}^{1} &amp; =\\Omega_{x}x_{t}^{1}+\\Omega_{u}u_{t}+\\Omega_{y}\\mathbb{E}_{t}y_{t+1}^{u}+\\xi_{t+1}\\\\ &amp; =\\Omega_{x}x_{t}^{1}+(\\Omega_{u}+\\Omega_{y}M\\Phi)u_{t}+\\xi_{t+1} \\end{aligned} \\] および \\[ x_{t}^{2}=\\Psi_{x}x_{t}^{1}+\\Psi_{y}Mu_{t} \\] を得る. これで解の特徴づけが完了した. 9.2 確率過程について ランダムネスというのは, ごく荒く言って, 発生する現象に複数の可能性があるということである. 明日は雨が降るかもしれないし, 雨は降らないかもしれない. 天気予報は 「明日雨になる確率」を教えてくれる. ランダムな現象の素朴な数学的表現は, と呼ばれる2つの可能な値を持つ「変数」(ここでは \\(X\\) を名付けよう) のそれぞれの値に, 足して1になる数字\\(p_{1}\\) , \\(p_{2}\\) を割り当てて, \\[ P(X=\\text{fine})=p_{1},\\qquad P(X=\\text{rainy})=p_{2} \\] のように書くことである. \\(\\{p_{1},p_{2}\\}\\) をという. では, \\(X\\) を関数として定式化して, 定義域 \\(\\{\\omega_{1},\\omega_{2}\\}\\) の方に確率を割り振る. もちろん, まったく同じことであるがこうすることによって関数解析の枠組みで確率論を扱えるので大変都合が良い. この補論では, マクロ経済学で頻出するフィルトレーションとか条件付き期待値といった用語がなんとなくイメージできるようになる程度に確率論の簡単な紹介を行なってみたいと思う. 9.2.1 確率変数 ランダムな現象を表現するために関数を使うということは, 関数 \\(X:\\omega\\mapsto X(\\omega)\\) の取りうる値 (値域) の方に「生起するかもしれないと認識している値」を対応させるということは想像がつくだろう. 例えば株価のように非負の値を取る確率変数であれば \\[ X:\\bullet\\to\\mathbb{R}_{+} \\] という関数を考えるのである. 定義域の方は何が入るだろうか？実は取りうる値を区別するのに十分な数の要素を持っていれば何でもよい. 慣例に従って \\(\\Omega\\) という記号を使うと \\[ X:\\Omega\\to\\mathbb{R}_{+} \\] と書ける. \\(\\Omega\\) を状態空間, \\(\\Omega\\) の元を根元事象, \\(\\Omega\\) の部分集合を事象という.. \\(\\Omega\\) がどのように分割されるか, \\(\\Omega\\) の分割のそれぞれにどのように確率を割り振るかが大事なのである. 例えば, \\(X\\) が 2値関数であれば, \\(\\Omega_{1}=\\{\\omega_{1},\\omega_{2}\\}\\) という2点集合を取って, \\(\\Omega_{1}=\\{\\omega_{1}\\}\\cup\\{\\omega_{2}\\}\\) と分割し, \\[ X_{1}(\\omega)=\\begin{cases} 0, &amp; \\omega\\in\\{\\omega_{1}\\}\\\\ 1, &amp; \\omega\\in\\{\\omega_{2}\\} \\end{cases} \\] としてもいいし, \\(\\Omega_{2}=[0,1]\\) として, \\(\\Omega_{2}=[0,1/2)\\cup[1/2,1]\\) と分割し, \\[ X_{2}(\\omega)=\\begin{cases} 0, &amp; \\omega\\in[0,1/2)\\\\ 1, &amp; \\omega\\in[1/2,1] \\end{cases} \\] とするのでもよい. さらに, この分割にさえ自由度がある. 例えば, \\(\\Omega_{3}=[0,1]\\) として, \\(\\Omega_{3}=[0,1/3)\\cup[1/3,1]\\) と分割し, \\[ X_{3}(\\omega)=\\begin{cases} 0, &amp; \\omega\\in[0,1/3)\\\\ 1, &amp; \\omega\\in[1/3,1] \\end{cases} \\] としても問題は起こらない. 3つの例において, 関数 \\(X_{1}\\) , \\(X_{2}\\), \\(X_{3}\\) はそれぞれ異なる関数であるにも関わらず, 同じ確率的現象の表現と解釈するためには, 各事象に対して確率を割り振るやり方を同じにしてやる必要がある. つまり, \\[ X_{1}^{-1}(\\{0\\})=\\{\\omega_{1}\\},\\quad X_{1}^{-1}(\\{1\\})=\\{\\omega_{2}\\} \\] \\[ X_{2}^{-1}(\\{0\\})=[0,1/2),\\quad X_{2}^{-1}(\\{1\\})=[1/2,1] \\] \\[ X_{3}^{-1}(\\{0\\})=[0,1/3),\\quad X_{3}^{-1}(\\{1\\})=[1/3,1] \\] に注意して, \\[ P_{1}(\\{\\omega_{1}\\})=P_{2}([0,1/2))=P_{3}([0,1/3))=p_{1} \\] \\[ P_{1}(\\{\\omega_{2}\\})=P_{2}([1/2,1])=P_{3}([1/3,1])=p_{2} \\] と「確率」\\(P_{1}\\), \\(P_{2}\\), \\(P_{3}\\)を定義すれば , これらが同じ現象を表現していると言える. ただし, \\(p_{1}+p_{2}=1\\), \\(p_{1},p_{2}\\ge0\\). 連続値を取るような場合には, 事象としてどのようなものを考えればよいだろうか. 例を挙げて考えてみよう. 「株価が5000円以上6000円未満である」という事象は \\[ \\{\\omega\\in\\Omega\\mid5000\\le X(\\omega)&lt;6000\\} \\] と書ける. あるいは同じことを \\[ X^{-1}\\left([5000,6000)\\right)=X^{-1}\\left([0,5000)\\right)^{c}\\cap X^{-1}\\left([0,6000)\\right) \\] とも書ける. \\(X^{-1}(A)\\) は\\(A\\) の\\(X\\)による逆像, \\(A^{c}\\) は\\(A\\) の補集合である. 非負の実数値を取るような確率現象を扱う上で, 「事象全体の集合」として考えるべきは, \\(\\{\\omega\\in\\Omega\\mid X(\\omega)\\le a\\}\\), \\(\\{\\omega\\in\\Omega\\mid X(\\omega)&lt;a\\}\\), \\(a\\in\\mathbb{R}_{+}\\cup\\{+\\infty\\}\\) やその補集合, 和集合, 積集合などを含むものとするのがよい. さらに技術的な理由から, そのような部分集合の可算無限個の和集合も含んでいるとするのである. 以上の要請を汲んだ上で, 確率論の教科書を確認してほしい. そこには次のような定義が書いていると思う: \\(\\Omega\\) の部分集合族 \\(\\mathcal{F}\\) が であるとは, (i) \\(\\Omega\\in\\mathcal{F}\\), (ii) \\(A\\in\\mathcal{F}\\Rightarrow A^{c}\\in\\mathcal{F}\\), (iii) \\(A_{1},A_{2},\\dots,A_{n},\\dots\\in\\mathcal{F}\\Rightarrow\\cup_{n=1}^{\\infty}A_{n}\\in\\mathcal{F}\\) の3つが成り立つことをいう. \\(\\Omega\\) とその部分集合族 \\(\\mathcal{F}\\) — \\((\\Omega,\\mathcal{F})\\) のペアをという — の出来上がりである. さらに, \\(\\{\\omega\\mid X(\\omega)&lt;a\\}\\) の形式の部分集合やそれらの補集合, 可算無限個の和集合, 積集合が \\(\\mathcal{F}\\) に含まれているとき, \\(X\\) はという. 確率はと呼ばれる写像 \\(P:\\mathcal{F}\\to[0,1]\\) によって定式化される. 確率測度は次の性質を満たす. (i) \\(P(\\Omega)=1\\), (ii) \\(P(A^{c})=1-P(A)\\), (iii) \\(A_{1},A_{2},\\dots\\) が \\(A_{i}\\cap A_{j}=\\emptyset\\), \\(i\\neq j\\) ならば \\(P(\\cup_{i}A_{i})=\\sum_{i}P(A_{i})\\). 3つ組 \\((\\Omega,\\mathcal{F},P)\\) を確率空間という. 以上でランダムネスの構成要素が全部出揃った. 適当に状態空間 \\(\\Omega\\) を設定し, 状態空間を事象を表す部分集合に分割し (\\(\\mathcal{F}\\) はその一般化・抽象化である), さらに事象ごとの生起確率を割り振っていく(\\(P\\) がその役割を果たしてくれる). 確率変数 \\(X\\) は, \\(\\Omega\\) を定義域にもつ関数であって, \\[ \\{\\omega\\in\\Omega\\mid X(\\omega)&lt;a\\}\\in\\mathcal{F} \\] が成り立つものである. \\(\\mathcal{F}\\) の元になるということは,「\\(X&lt;a\\) である確率」がキチンと定義されるように作っているということである. ところで, 可測性は\\(\\mathcal{F}\\)の選び方に依存するので, 明示的に \\(\\mathcal{F}\\)-可測と言ったりもする. \\(\\Omega\\) が有限個の部分集合の族に分割される場合を考えると可測関数の直感的な理解が得られやすい. \\(\\Omega=A_{1}\\cup A_{2}\\cup\\cdots\\cup A_{n}\\), \\(A_{i}\\cap A_{j}=\\emptyset\\), \\(i\\neq j\\) としよう. この場合, 分割 \\(\\Delta=\\{A_{1},A_{2},\\dots,A_{n}\\}\\) が事象の集合族を定める. すなわち, \\[ \\mathcal{F}_{\\Delta}=\\left\\{ \\cup_{k=1}^{r}A_{n_{k}}\\mid\\{n_{1},\\dots,n_{r}\\}\\subset\\{1,\\dots,n\\},\\ r=0,1,\\dots,n\\right\\} \\] とすれば, \\(\\mathcal{F}_{\\Delta}\\) は \\(\\sigma\\)-集合族となる. \\(\\mathcal{F}_{\\Delta}\\) は \\(\\Delta\\) の元をすべて含む最小の \\(\\sigma\\)-集合族であって \\(\\mathcal{F}\\supset\\mathcal{F}_{\\Delta}\\) なる \\(\\sigma\\)-集合族であれば何を考えても以下の議論は上手くいく. 2値関数の場合に可測関数と非可測関数を図示してみよう. \\(\\Omega_{3}=[0,1]\\), \\(\\Delta_{3}=\\{[0,1/3),[1/3,1]\\}\\) とすれば, \\(\\mathcal{F}_{\\Delta_{3}}=\\{\\emptyset,[0,1/3),[1/3,1],\\Omega_{3}\\}\\) である. 図 の \\(X_{3}\\) は分割 \\(\\Delta_{3}\\) の各集合上で定数値を取る関数である. 一方で, \\(f\\) は \\([1/3,1]\\) 上で定数関数でない. \\(f\\) が可測性の定義 \\(\\{\\omega\\in\\Omega\\mid f(\\omega)&lt;a\\}\\in\\mathcal{F}_{\\Delta_{3}}\\) を満たさないことを確認してほしい. 有限個の事象のみを扱うケースでは, 可測関数すなわち確率変数は, 分割を構成する各集合上で定数値を取る関数に外ならない. 2値関数, 2事象への分割を考えたが, より一般のケースでも同様のことを確認できる. なお, \\(\\Omega\\) 上で定数値をとる関数はもちろん可測関数である. 不確実性がない現象の表現に使う. 9.2.2 期待値 期待値は積分で定義できる. \\((\\Omega,\\mathcal{F},P)\\) を確率空間, \\(X\\) を\\(\\mathbb{R}\\)-値確率変数とする. \\(X\\) の期待値を \\[\\begin{equation} \\mathbb{E}X=\\int_{\\Omega}X(\\omega)dP(\\omega)\\tag{9.8} \\end{equation}\\] と定義する. ここでの積分はいわゆるルベーグ積分である. ルベーグ積分論を未修であれば, 次のように理解すればよいだろう. 先ほど述べた可測関数の定義より, \\[ \\left\\{ \\omega\\in\\Omega\\mid x\\le X(\\omega)&lt;x+dx\\right\\} \\in\\mathcal{F} \\] が成り立つ. \\(\\mathcal{F}\\) は\\(P\\) の定義域なので, \\[ P\\left(\\left\\{ \\omega\\in\\Omega\\mid x\\le X(\\omega)&lt;x+dx\\right\\} \\right) \\] という確率が対応している. これを \\(dP(\\omega)\\) と思えば, \\[ \\mathbb{E}X=\\int_{x\\in\\mathbb{R}}xP\\left(\\left\\{ \\omega\\in\\Omega\\mid x\\le X(\\omega)&lt;x+dx\\right\\} \\right) \\] と書けて, 馴染みのある定式化とかなり近い形式になる. 累積分布関数を \\(F\\) とすれば, \\[ P\\left(\\left\\{ \\omega\\in\\Omega\\mid x\\le X(\\omega)&lt;x+dx\\right\\} \\right)=F(x+dx)-F(x) \\] だから, \\[ \\mathbb{E}X=\\int_{x\\in\\mathbb{R}}x\\left[F(x+dx)-F(x)\\right] \\] はスティルチェス積分による表現であるし, 確率密度関数が存在する場合には, \\[ P\\left(\\left\\{ \\omega\\in\\Omega\\mid x\\le X(\\omega)&lt;x+dx\\right\\} \\right)=p(x)dx \\] と書き換えることができるので, \\[ \\mathbb{E}X=\\int_{x\\in\\mathbb{R}}xp(x)dx \\] と, よりいっそう馴染み深い表現になる. この操作で一つ見えてくることは, \\((\\Omega,\\mathcal{F})\\) に備えた測度 \\(P\\) が\\(\\mathbb{R}\\)上の測度を定めるということである. \\(A\\subset\\mathbb{R}\\) が普通の (例えば区間 \\((a,b)\\) など) 部分集合であれば \\[ X^{-1}(A)\\in\\mathcal{F} \\] が成り立つので, \\(P(X^{-1}(A))\\) が定義されている. これを \\(\\mathbb{R}\\) の部分集合 \\(A\\) に対して確率\\(P(X^{-1}(A))\\) を対応させる集合関数 \\(P\\circ X^{-1}\\) と見れば, \\(P\\circ X^{-1}\\) は確率測度の性質をもっている. \\(X\\) から誘導された確率測度を\\(X\\)のという. 簡単な例を見てみよう. \\[ \\begin{aligned} \\mathbb{E}X_{3} &amp; =0\\cdot P([0,1/3))+1\\cdot P([1/3,1])\\\\ &amp; =0\\cdot p_{1}+1\\cdot p_{2}\\\\ &amp; =p_{2} \\end{aligned} \\] である. 分布は \\[ \\begin{aligned} P\\circ X^{-1}(\\{0\\}) &amp; =p_{1}\\\\ P\\circ X^{-1}(\\{1\\}) &amp; =p_{2} \\end{aligned} \\] によって定まる. 9.2.3 条件付き期待値 \\(\\sigma\\)-集合族, あるいはそれを生成する分割が不確実な現象に対する情報精度の上限という意味を持っていることを確認しておこう. \\((\\Omega,\\mathcal{F},P)\\) を確率空間, \\(X\\), \\(Y\\) を実数値確率変数とする. 説明の都合上, \\(X\\) が先に判明して, \\(Y\\) は後で判明するという時間差があるとしておく. 根元事象 \\(\\omega\\in\\Omega\\) を特定できれば, \\(X(\\omega)\\) にも \\(Y(\\omega)\\) にも不確実性は存在しない. したがって, 不確実性は どの \\(\\omega\\) に対応する値がでてくるか分からないということに起因している. また, 先に明らかになる確率変数 \\(X\\) の観測によって得られる情報は, あくまで \\(X(\\omega)=a\\) とか, \\(a&lt;X(\\omega)&lt;b\\) とか, \\(X(\\omega)\\in A\\) という生起した値に関する情報だけであるから \\(\\omega\\in X^{-1}(\\{a\\})\\) や \\(\\omega\\in X^{-1}((a,b))\\), \\(\\omega\\in X^{-1}(A)\\)といった情報を得ることができるが, \\(\\omega\\) の完全な特定はできない. もし, \\(Y\\) が \\(X^{-1}(A)\\) の上で定数でなければ, \\(Y\\) の値を正確に予見することはできない. ただし, \\(\\{Y(\\omega)\\mid\\omega\\in X^{-1}(A)\\}\\) のいずれかの値が生起するということだけは分かる. 2つの重要なメッセージがある. 第一に, \\(\\mathcal{F}\\) が細かく細分化されていればいるほど, \\(\\mathcal{F}\\)-可測関数の値から得られる情報は多くなる. すなわち観測値を用いて, より小さな集合の中に\\(\\omega\\) を特定できる. 第二に, 時間を通じて \\(\\omega\\) に関する情報が徐々に明らかになっていくということである. 具体的な例を通して見ていこう. \\(\\Omega=[0,1]\\) が $={ [0,),[,),[,),[,1]} $ と分割されているとする. 簡単のため分割を構成する各集合の確率は等しく \\(\\tfrac{1}{4}\\) であるとしよう. 確率変数を \\[ X(\\omega)=\\begin{cases} 0 &amp; \\text{if }\\omega\\in[0,\\tfrac{1}{2})\\\\ 1 &amp; \\text{if }\\omega\\in[\\tfrac{1}{2},1] \\end{cases} \\] \\[ Y(\\omega)=\\begin{cases} 0 &amp; \\text{if }\\omega\\in[0,\\tfrac{1}{4})\\\\ 1 &amp; \\text{if }\\omega\\in[\\tfrac{1}{4},\\tfrac{1}{2})\\\\ 2 &amp; \\text{if }\\omega\\in[\\tfrac{1}{2},\\tfrac{3}{4})\\\\ 3 &amp; \\text{if }\\omega\\in[\\tfrac{3}{4},1] \\end{cases} \\] と特定化する. \\(X\\), \\(Y\\) が\\(\\mathcal{F}_{\\Delta}\\)-可測関数であることを確認してほしい. \\(X\\) が実現する前には, \\(\\omega\\in\\Omega\\) という情報しかないので, \\(\\Delta_{0}=\\{\\Omega\\}\\) で表される情報集合を持っているといってよいだろう. \\(X\\) が実現した後には, \\(\\omega\\in[0,\\tfrac{1}{2})\\) あるいは \\(\\omega\\in[\\tfrac{1}{2},1]\\) のいずれかであるので, \\(X\\) によって \\(\\Delta_{1}=\\{[0,\\tfrac{1}{2}),[\\tfrac{1}{2},1]\\}\\) という\\(\\Delta_{0}\\) よりは細かいが \\(\\Delta\\) よりは粗い分割で表される情報を獲得できる. さらに, \\(Y\\) が明らかになると, \\(\\Delta_{2}=\\Delta\\) を獲得できる. それでは, \\(X\\) が授けてくれた情報を使って, \\(Y\\) について何が言えるようになるか, を考えてほしい. まずは初級の確率論の議論をなぞってみよう. \\(X\\) の実現値で条件付けると, \\(Y\\) がぞれぞれの値をとる確率が変わる. 例えば, \\(X=0\\) という条件の下で, \\(Y=0\\) となる確率は \\[ P(Y=0|X=0)=\\frac{P(Y=0)}{P(Y=0)+P(Y=1)}=\\frac{1}{2}. \\] また, \\[ P(Y=2|X=0)=0 \\] などが分かる. 次に, \\(X\\) の実現値で条件付けると, \\(Y\\) に関する期待値も変化する. 例えば, \\[ \\begin{aligned} \\mathbb{E}\\left[Y\\mid X=0\\right] &amp; =0\\cdot P(Y=0|X=0)+1\\cdot P(Y=1|X=0)+2\\cdot P(Y=2|X=0)+3\\cdot P(Y=3|X=0)\\\\ &amp; =\\frac{1}{2}. \\end{aligned} \\] また, \\[ \\mathbb{E}[Y\\mid X=1]=\\frac{5}{2}. \\] したがって, 条件付き期待値は実現値 \\(x\\) の関数として \\[ \\mathbb{E}[Y\\mid X=x]=\\begin{cases} \\tfrac{1}{2} &amp; \\text{if }x=0\\\\ \\tfrac{5}{2} &amp; \\text{if }x=1 \\end{cases} \\] などと書かれるのである. \\(\\mathbb{E}[Y|X=x]\\) と書いてはいるが, 実現値\\(x\\)に依存していないことに違和感を感じないだろうか. もちろん, \\(x\\) に依存するような条件付き期待値を作ろうと思えば作れるし, そのような例が重要であるのは事実だが, \\(x\\) の関数として条件付き期待値を特徴付けると本質を見誤ることになる. では何に依存していると認識するべきか？それは, 特定の実現値を生成する \\(\\omega\\) のなす集合である. すなわち条件付き期待値は \\(\\omega\\) に依存していて, \\(\\Delta_{1}\\) のどの元に \\(\\omega\\) が属するかによって値が変わるものと捉えるべきである. したがって, 次のように書く方がより本質に近い. \\[ \\mathbb{E}[Y|\\Delta_{1}](\\omega)=\\begin{cases} \\tfrac{1}{2} &amp; \\text{if }\\omega\\in[0,\\tfrac{1}{2})\\\\ \\tfrac{5}{2} &amp; \\text{if }\\omega\\in[\\tfrac{1}{2},1]. \\end{cases} \\] この定式化は, \\(\\mathbb{E}[Y|\\Delta_{1}]\\) は確率変数であり, \\(\\mathcal{F}_{\\Delta_{1}}\\)-可測であるということを示唆している. 有限分割が得られないケースでは, \\(\\mathbb{E}[Y|\\mathcal{F}_{\\Delta_{1}}]\\) と書く. ここで, \\[ \\mathcal{F}_{\\Delta_{1}}\\subset\\mathcal{F}_{\\Delta} \\] に注意をしてほしい. 粗い分割で得られた情報で「予測」をしているので (\\(Y\\) は \\(\\mathcal{F}_{\\Delta_{1}}\\)-可測ではない), \\(\\omega\\) への依存性が残ってしまうのである. 条件付き期待値の性質 一般に, 条件付き期待値は次の性質をもつ: \\((\\Omega,\\mathcal{F},P)\\) を確率空間 \\(Z\\) を確率変数, \\(\\mathcal{G}\\) を \\(\\mathcal{G}\\subset\\mathcal{F}\\) なる \\(\\sigma\\)-集合族とすると\\(\\mathbb{E}[Z|\\mathcal{G}]\\) は \\(\\mathcal{G}\\)-可測な確率変数になる. さらに, \\(\\mathcal{H\\subset\\mathcal{G}\\subset F}\\) なる \\(\\sigma\\)-集合族を取れば, \\[ \\mathbb{E}[\\mathbb{E}[Z|\\mathcal{G}]|\\mathcal{H}]=\\mathbb{E}[Z|\\mathcal{H}] \\] が成り立つ (正式には「ほとんど確実に」というのを付けなければならない). これを分割の言葉で確認しておこう. 前半の部分はすでに確認できているので, 後半を確認する. 分割 \\(\\Delta_{1}\\), \\(\\Delta_{2}\\) に対して, \\(\\Delta_{2}\\) が\\(\\Delta_{1}\\) より細分化されているとは, 任意の \\(A\\in\\Delta_{2}\\) についてある \\(B\\in\\Delta_{1}\\) があって \\(A\\subset B\\) が成り立つこととし, \\(\\Delta_{1}|\\Delta_{2}\\) と書こう. \\(\\Delta_{1}|\\Delta_{2}\\) ならば\\(\\mathcal{F}_{\\Delta_{1}}\\subset\\mathcal{F}_{\\Delta_{2}}\\) が成り立つ. したがって, \\[ \\mathbb{E}[\\mathbb{E}[Z|\\Delta_{2}]|\\Delta_{1}]=\\mathbb{E}[Z|\\Delta_{1}] \\] を確かめよう. \\(\\Delta\\) が確率空間を構成する分割（最も細かい分割）であるとすれば, \\(\\Delta_{2}|\\Delta\\) がなりたつ. 任意の \\(A\\in\\Delta_{2}\\) について, \\(A=\\cup_{i}A_{i}\\), \\(A_{i}\\in\\Delta\\), なる互いに素な分割がある. 上の計算を一般化すると, 条件付き期待値の定義は \\(P(A)\\neq0\\), \\(\\omega\\in A\\in\\Delta_{2}\\) のとき, \\[ \\mathbb{E}[Z|\\Delta_{2}](\\omega)=\\sum_{i}\\frac{Z(A_{i})P(A_{i})}{P(A)} \\] とできる. \\(P(A)=0\\) のときは, \\(\\mathbb{E}[Z|\\Delta_{2}](\\omega)=0\\) である. ただし, \\(Z(A_{i})\\) は \\(\\omega\\in A_{i}\\) のときの \\(Z(\\omega)\\) の値, 以下では \\(\\mathbb{E}[Z|\\Delta_{2}](A)\\) も同様の意味で使う. 可測関数は区分的な定数関数であったことに注意しよう. \\(B\\in\\Delta_{1}\\) に対して, \\(\\Delta_{2}\\) の元による互いに素な分割 \\(B=\\cup_{j}B_{j}\\) が存在する. 各 \\(B_{j}\\in\\Delta_{2}\\) には, \\(\\Delta\\) の元による互いに素な分割 \\(B_{i}=\\cup_{i}B_{ij}\\) が存在するので, \\(\\omega\\in B\\) に対して \\[ \\begin{aligned} \\mathbb{E}[\\mathbb{E}[Z|\\Delta_{2}]|\\Delta_{1}](B) &amp; =\\sum_{j}\\frac{\\mathbb{E}[Z|\\Delta_{2}](B_{j})P(B_{j})}{P(B)}\\\\ &amp; =\\sum_{j}\\frac{\\sum_{i}\\left[\\frac{Z(B_{ij})P(B_{ij})}{P(B_{j})}\\right]P(B_{j})}{P(B)}\\\\ &amp; =\\sum_{j}\\sum_{i}\\frac{Z(B_{ij})P(B_{ij})}{P(B)}. \\end{aligned} \\] \\(B=\\cup_{i,j}B_{ij}\\) は, \\(B\\in\\Delta_{1}\\) の \\(\\Delta\\) の元による互いに素な分割になっているので, \\[ \\mathbb{E}[\\mathbb{E}[Z|\\Delta_{2}]|\\Delta_{1}](B)=\\mathbb{E}[Z|\\Delta_{1}](B) \\] が成り立つ. 9.2.4 確率過程 前節では確率変数が情報を明らかにしていく様子を具体例をもって描写してみた. どの程度成功しているかは分からないが, 我々はもっと先に進まなければならない. ここまでの話を逆転させてみよう. すなわち, しだいに細分化されていく分割の列 \\[ \\{\\Omega\\}=\\Delta_{0}|\\Delta_{1}|\\Delta_{2}|\\cdots \\] あるいは, \\(\\sigma\\)-集合族の増大列 \\[ \\{\\emptyset,\\Omega\\}=\\mathcal{F}_{0}\\subset\\mathcal{F}_{1}\\subset\\mathcal{F}_{2}\\cdots \\] が予め与えられていて, 確率変数の列 \\(X_{1}\\), \\(X_{2}\\), … がこれらの分割, \\(\\sigma\\)-集合族に関して可測になっているという状況を考えるのである. \\(\\sigma\\)-集合族の列 \\(\\mathbb{F}=(\\mathcal{F}_{n})\\) をフィルトレーションといい, \\(X_{n}\\) が\\(\\mathcal{F}_{n}\\)-可測であるような確率変数の列を \\(\\mathbb{F}\\)-適合確率過程という. 具体的なイメージとしては, コイントスの繰り返しを考えるとよい. ひとまず繰り返しの回数を3回としてみよう. \\(\\Omega=[0,1)\\) の分割列を \\[ \\begin{aligned} \\Delta_{0} &amp; =\\{\\Omega\\}\\\\ \\Delta_{1} &amp; =\\{[0,\\tfrac{1}{2}),[\\tfrac{1}{2},1)\\}\\\\ \\Delta_{2} &amp; =\\{[0,\\tfrac{1}{4}),[\\tfrac{1}{4},\\tfrac{1}{2}),[\\tfrac{1}{2},\\tfrac{3}{4}),[\\tfrac{3}{4},1)\\}\\\\ \\Delta_{3} &amp; =\\{[0,\\tfrac{1}{8}),[\\tfrac{1}{8},\\tfrac{1}{4}),\\dots,[\\tfrac{3}{4},\\tfrac{7}{8}),[\\tfrac{7}{8},1)\\} \\end{aligned} \\] と構成する. 1回目が表であるという事象 (\\(x_{1}=0\\)) を \\(\\omega\\in[\\tfrac{0}{2},\\tfrac{1}{2})\\), 裏であるという事象 (\\(x_{1}=1\\)) を \\(\\omega\\in[\\tfrac{1}{2},1)\\) に対応させる. さらに1回目が \\(x_{1}\\) で2回目が表 (\\(x_{2}=0\\)) あるいは裏 (\\(x_{2}=1)\\) であるという事象を, \\(\\omega\\in[\\tfrac{x_{1}}{2}+\\tfrac{x_{2}}{2^{2}},\\tfrac{x_{1}}{2}+\\frac{x_{2}+1}{2^{2}})\\) に対応させる. 1回目が\\(x_{1}\\) で, 2回目が \\(x_{2}\\) で3回目が\\(x_{3}\\) である (\\(x_{3}=0\\) or \\(1\\)) という事象を \\(\\omega\\in[\\tfrac{x_{1}}{2}+\\tfrac{x_{2}}{2^{2}}+\\tfrac{x_{3}}{2^{3}},\\tfrac{x_{1}}{2}+\\frac{x_{2}}{2^{2}}+\\tfrac{x_{3}+1}{2^{3}})\\) に対応させると, コイントスモデルの分割ができあがる. この分割列に適合した確率過程を作ることは難しくない. 各ステップの分割にあわせて区分的定数関数を作ればよい. 適合過程を考察する理由は明白であろう. 確率過程 \\(X_{n}(\\omega)\\) が \\(n\\) 回目のコイントス後の所持金を表している場合を考えてみよ. 参考文献 "],
["e58f82e88083e69687e78cae.html", "参考文献", " 参考文献 "]
]
